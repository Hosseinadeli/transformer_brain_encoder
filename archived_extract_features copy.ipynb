{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08067d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/transformer_brain_encoder\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/')\n",
    "!pwd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#data_dir = './algonauts_2023_challenge_data'\n",
    "\n",
    "device = 'cuda:0' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "device = torch.device(device)\n",
    "\n",
    "nsd_stimuli_dir = '/engram/nklab/datasets/natural_scene_dataset/nsddata_stimuli/stimuli/nsd/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# filename = nsd_stimuli_dir + 'nsd_stimuli.hdf5'\n",
    "\n",
    "# with h5py.File(filename, \"r\") as f:\n",
    "#     # Print all root level object names (aka keys) \n",
    "#     # these can be group or dataset names \n",
    "#     print(\"Keys: %s\" % f.keys())\n",
    "#     # get first object name/key; may or may NOT be a group\n",
    "#     a_group_key = list(f.keys())[0]\n",
    "\n",
    "#     # get the object type for a_group_key: usually group or dataset\n",
    "#     print(type(f[a_group_key])) \n",
    "\n",
    "#     # If a_group_key is a group name, \n",
    "#     # this gets the object names in the group and returns as a list\n",
    "#     data = list(f[a_group_key])\n",
    "\n",
    "#     # If a_group_key is a dataset name, \n",
    "#     # this gets the dataset values and returns as a list\n",
    "#     data = list(f[a_group_key])\n",
    "#     # preferred methods to get dataset values:\n",
    "#     ds_obj = f[a_group_key]      # returns as a h5py dataset object\n",
    "#     ds_arr = f[a_group_key][()]  # returns as a numpy array\n",
    "    \n",
    "    \n",
    "# print(ds_arr.shape)\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((425,425)), # resize the images to 224x24 pixels\n",
    "#     transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19623518",
   "metadata": {},
   "source": [
    "## DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96f1b441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "arch = 'dinov2_vitb14'\n",
    "model = torch.hub.load('facebookresearch/dinov2', arch).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d77aeb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "patch_size = 14\n",
    "\n",
    "feat_out = {}\n",
    "def hook_fn_forward_qkv(module, input, output):\n",
    "    feat_out[\"qkv\"] = output\n",
    "\n",
    "# #for i in range(1,13):\n",
    "model._modules[\"blocks\"][-1]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "\n",
    "def aff_features(img):\n",
    "\n",
    "    size_im = (\n",
    "        img.shape[0],\n",
    "        img.shape[1],\n",
    "        int(np.ceil(img.shape[2] / patch_size) * patch_size),\n",
    "        int(np.ceil(img.shape[3] / patch_size) * patch_size),\n",
    "    )\n",
    "    paded = torch.zeros(size_im).to(device)\n",
    "    paded[:,:, : img.shape[2], : img.shape[3]] = img\n",
    "    img = paded\n",
    "\n",
    "    # Size for transformers\n",
    "    h_featmap = img.shape[-2] // patch_size\n",
    "    w_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "\n",
    "    model._modules[\"blocks\"][-10]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "\n",
    "    which_features = 'q'\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass in the model\n",
    "        outputs = model.get_intermediate_layers(img)\n",
    "\n",
    "        # Scaling factor\n",
    "        scales = [patch_size, patch_size]\n",
    "\n",
    "        # Dimensions\n",
    "        nb_im = img.shape[0] #Batch size\n",
    "        nh = 12 #Number of heads\n",
    "        nb_tokens = h_featmap*w_featmap + 1\n",
    "\n",
    "        # Extract the qkv features of the last attention layer\n",
    "        qkv = feat_out[\"qkv\"].reshape(nb_im, nb_tokens, 3, nh, -1 // nh).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k = k.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
    "        q = q.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
    "        v = v.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
    "\n",
    "        # Modality selection\n",
    "        if which_features == \"k\":\n",
    "            feats = k\n",
    "        elif which_features == \"q\":\n",
    "            feats = q\n",
    "        elif which_features == \"v\":\n",
    "            feats = v\n",
    "\n",
    "        cls_token = feats[0,0:1,:].cpu().numpy() \n",
    "        \n",
    "    #print(feats.flatten(1).dtype)\n",
    "    return feats.flatten(1).cpu().numpy() \n",
    "\n",
    "    #return cls_token[0]\n",
    "\n",
    "def extract_dino_features(dataloader):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = aff_features(d.to(device))\n",
    "        # Flatten the features\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00846cc1",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4c9567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ha2366/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "arch = 'alexnet'\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', arch)\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it\n",
    "\n",
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)\n",
    "\n",
    "#feature_type =  [\"features.2\"] # \"features.2\" #\"layer2.0.conv1\" # #@param [\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\", \"classifier.2\", \"classifier.5\", \"classifier.6\"] {allow-input: true}\n",
    "#'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', \n",
    "feature_type =  ['features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n",
    "#feature_type =  ['features.10', 'features.11', 'features.12', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n",
    "\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=feature_type)\n",
    "\n",
    "def extract_alexnet_features(dataloader):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d.to(device))\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Flatten the features\n",
    "        features.append(ft.detach().cpu().numpy())\n",
    "    return np.vstack(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76ebaa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80ecc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=768, batch_size=1536)\n",
    "\n",
    "    # Fit PCA to batch\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca\n",
    "\n",
    "\n",
    "def extract_features(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d)\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7a545",
   "metadata": {},
   "source": [
    "## extract and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5652efb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [09:32<00:00,  7.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9841, 725992)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Training images: 9082\n",
      "Test images: 293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [03:41<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9082, 725992)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "7\n",
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [03:44<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9841, 725992)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:11<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "8\n",
      "Training images: 8779\n",
      "Test images: 395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [05:53<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8779, 725992)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:22<00:00,  5.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr\n",
    "\n",
    "\n",
    "data_dir = '/engram/nklab/algonauts/algonauts_2023_challenge_data'\n",
    "\n",
    "device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "device = torch.device(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((425,425)), # resize the images to 224x24 pixels\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "])\n",
    "\n",
    "feature_name = 'alexnet_all'\n",
    "\n",
    "for subj in range(5,9): #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n",
    "\n",
    "    print(subj)\n",
    "    class argObj:\n",
    "        def __init__(self, data_dir, subj):\n",
    "\n",
    "            self.subj = format(subj, '02')\n",
    "            self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "\n",
    "    args = argObj(data_dir, subj)\n",
    "\n",
    "    feature_dir = './saved_image_features/'\n",
    "\n",
    "    subject_feature_dir =  os.path.join(feature_dir, feature_name,format(subj, '02'))\n",
    "\n",
    "    if not os.path.isdir(subject_feature_dir):\n",
    "        os.makedirs(subject_feature_dir)\n",
    "\n",
    "\n",
    "    train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "    test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "    #test_img_dir = os.path.join(args.data_dir, '../nsdsynthetic_stimuli/')\n",
    "\n",
    "    # Create lists will all training and test image file names, sorted\n",
    "    train_img_list = os.listdir(train_img_dir)\n",
    "    train_img_list = [f for f in train_img_list if f.endswith('.png')]\n",
    "    train_img_list.sort()\n",
    "\n",
    "    train_imgs_paths = list(Path(train_img_dir).iterdir())\n",
    "    train_imgs_paths = [f for f in train_imgs_paths if str(f).endswith('.png')]\n",
    "    train_imgs_paths = sorted(train_imgs_paths)\n",
    "\n",
    "    test_img_list = os.listdir(test_img_dir)\n",
    "    test_img_list = [f for f in test_img_list if f.endswith('.png')]\n",
    "    test_img_list.sort()\n",
    "\n",
    "    test_imgs_paths = list(Path(test_img_dir).iterdir())\n",
    "    test_imgs_paths = [f for f in test_imgs_paths if str(f).endswith('.png')]\n",
    "    test_imgs_paths = sorted(test_imgs_paths)\n",
    "\n",
    "    # Create lists with all training and test image file names, sorted\n",
    "    # train_img_list = os.listdir(train_img_dir)\n",
    "    # train_img_list.sort()\n",
    "    # test_img_list = os.listdir(test_img_dir)\n",
    "    # test_img_list.sort()\n",
    "    print('Training images: ' + str(len(train_img_list)))\n",
    "    print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "    idxs_train = np.arange(len(train_img_list))\n",
    "    idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "\n",
    "    class ImageDataset(Dataset):\n",
    "        def __init__(self, imgs_paths, idxs, transform):\n",
    "            self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.imgs_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Load the image\n",
    "            img_path = self.imgs_paths[idx]\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "            if self.transform:\n",
    "                img = self.transform(img).to(device)\n",
    "            return img\n",
    "\n",
    "\n",
    "    batch_size = 128 #@param\n",
    "    # Get the paths of all image files\n",
    "    # train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "    # test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "    # The DataLoaders contain the ImageDataset class\n",
    "    train_imgs_dataloader = DataLoader(\n",
    "        ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_imgs_dataloader = DataLoader(\n",
    "        ImageDataset(test_imgs_paths, idxs_test, transform), \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "\n",
    "    features_train = extract_alexnet_features(train_imgs_dataloader)\n",
    "    print(features_train.shape)\n",
    "    np.save(subject_feature_dir + '/train.npy', features_train)\n",
    "    \n",
    "    features_test = extract_alexnet_features(test_imgs_dataloader)\n",
    "    np.save(subject_feature_dir + '/test.npy', features_test)\n",
    "\n",
    "    for run in range(1,6):\n",
    "        print(run)\n",
    "        save_dir = subject_feature_dir + '/pca_run' + str(run)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        # pca = fit_pca(feature_extractor, train_imgs_dataloader)\n",
    "        # features_train = extract_features(feature_extractor, train_imgs_dataloader, pca)\n",
    "\n",
    "        num_train = int(np.round(len(features_train) / 100 * 90))\n",
    "        # Shuffle all training stimulus images\n",
    "        idxs = np.arange(len(features_train))\n",
    "\n",
    "        np.random.shuffle(idxs)\n",
    "        np.save(save_dir+ '/idxs.npy', idxs)\n",
    "        \n",
    "        # Assign 90% of the shuffled stimulus images to the training partition,\n",
    "        # and 10% to the test partition\n",
    "        idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "\n",
    "        features_train_run = features_train[idxs_train]\n",
    "        features_val_run = features_train[idxs_val]\n",
    "\n",
    "        pca = PCA(n_components=768)\n",
    "        pca.fit(features_train_run)\n",
    "        features_train_pca = pca.transform(features_train_run)\n",
    "        features_val_pca = pca.transform(features_val_run)\n",
    "        features_test_pca = pca.transform(features_test)\n",
    "\n",
    "        np.save(save_dir + '/train.npy', features_train_pca)\n",
    "        np.save(save_dir + '/val.npy', features_val_pca)\n",
    "        np.save(save_dir + '/test.npy', features_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5469bd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159, 738816)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ad45d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "7\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "8\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for subj in range(2,9):\n",
    "    print(subj )\n",
    "    feature_name = 'dinov2_q_last'\n",
    "\n",
    "    feature_dir = './saved_image_features/'\n",
    "    subject_feature_dir =  os.path.join(feature_dir, feature_name,format(subj, '02'))\n",
    "\n",
    "    features_train = np.load(subject_feature_dir+'/train.npy')\n",
    "    features_test = np.load(subject_feature_dir+'/test.npy')\n",
    "\n",
    "    for run in range(1,6):\n",
    "        print(run)\n",
    "        save_dir = subject_feature_dir + '/pca_run' + str(run)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        num_train = int(np.round(len(features_train) / 100 * 90))\n",
    "        # Shuffle all training stimulus images\n",
    "        idxs = np.arange(len(features_train))\n",
    "\n",
    "        np.random.shuffle(idxs)\n",
    "        np.save(save_dir+ '/idxs.npy', idxs)\n",
    "        \n",
    "        # Assign 90% of the shuffled stimulus images to the training partition,\n",
    "        # and 10% to the test partition\n",
    "        idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "\n",
    "        features_train_run = features_train[idxs_train]\n",
    "        features_val_run = features_train[idxs_val]\n",
    "\n",
    "        pca = PCA(n_components=768)\n",
    "        pca.fit(features_train_run)\n",
    "        features_train_pca = pca.transform(features_train_run)\n",
    "        features_val_pca = pca.transform(features_val_run)\n",
    "        features_test_pca = pca.transform(features_test)\n",
    "\n",
    "        np.save(save_dir + '/train.npy', features_train_pca)\n",
    "        np.save(save_dir + '/val.npy', features_val_pca)\n",
    "        np.save(save_dir + '/test.npy', features_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b759b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=768)\n",
    "pca.fit(features_test)\n",
    "features_train_pca = pca.transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bc44a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d08d2150",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = nsd_feats_dir + 'dinov2_vitb14_q_features_last1_300pc.npy'\n",
    "np.save(file_name, features_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d937a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "nsd_feats_dir = '../../../share/nklab/projects/nsd_features/'\n",
    "file_name = nsd_feats_dir + 'dinov2_vitb14_q_features_last1_100pc.npy'\n",
    "features_pca = np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a00f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4212c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dir = './algonauts_image_features/dinov2_q_last/' + '01'\n",
    "fts_subj_train = np.load(feat_dir + '/train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e507800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9841, 738816)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_subj_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9820fc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.20000e+01, 1.24000e+02, 1.29300e+03, 1.09440e+04, 9.91260e+04,\n",
       "        4.64691e+05, 1.47989e+05, 1.27560e+04, 1.80100e+03, 4.00000e+01]),\n",
       " array([-2.20427322, -1.80853426, -1.41279531, -1.01705623, -0.62131727,\n",
       "        -0.22557831,  0.17016068,  0.56589967,  0.96163863,  1.35737765,\n",
       "         1.75311661]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(fts_subj_train[0]/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f2330",
   "metadata": {},
   "source": [
    "## CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71629ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[index]\n",
    "\n",
    "#         if self.transform:\n",
    "#             x = self.transform(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors.shape[0]\n",
    "\n",
    "train_dataset = CustomTensorDataset(ds_arr, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c465b391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "# probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "968958b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clip_features(d):\n",
    "    \n",
    "    inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=d, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    for key, value in inputs.items():\n",
    "        inputs[key] = inputs[key].to(device)\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    return outputs['image_embeds'].detach().cpu().numpy()\n",
    "\n",
    "def extract_features(dataloader):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        \n",
    "        print(d.shape)\n",
    "        # Extract features\n",
    "        ft = clip_features(d)\n",
    "        # Flatten the features\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9744c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/1141 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[162, 177, 172],\n",
      "          [161, 175, 176],\n",
      "          [159, 169, 179],\n",
      "          ...,\n",
      "          [151, 168, 176],\n",
      "          [144, 164, 171],\n",
      "          [151, 165, 166]],\n",
      "\n",
      "         [[154, 173, 171],\n",
      "          [169, 181, 181],\n",
      "          [161, 175, 185],\n",
      "          ...,\n",
      "          [153, 164, 176],\n",
      "          [143, 165, 174],\n",
      "          [155, 174, 178]],\n",
      "\n",
      "         [[161, 178, 168],\n",
      "          [160, 176, 170],\n",
      "          [154, 172, 178],\n",
      "          ...,\n",
      "          [151, 165, 175],\n",
      "          [150, 165, 179],\n",
      "          [156, 166, 175]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[191, 214, 222],\n",
      "          [200, 218, 232],\n",
      "          [201, 216, 232],\n",
      "          ...,\n",
      "          [225, 235, 243],\n",
      "          [215, 234, 249],\n",
      "          [223, 246, 252]],\n",
      "\n",
      "         [[188, 215, 224],\n",
      "          [192, 222, 233],\n",
      "          [197, 226, 238],\n",
      "          ...,\n",
      "          [199, 230, 235],\n",
      "          [212, 233, 238],\n",
      "          [224, 244, 251]],\n",
      "\n",
      "         [[191, 220, 228],\n",
      "          [191, 223, 234],\n",
      "          [189, 221, 232],\n",
      "          ...,\n",
      "          [200, 229, 237],\n",
      "          [212, 233, 238],\n",
      "          [216, 239, 245]]],\n",
      "\n",
      "\n",
      "        [[[212, 212, 204],\n",
      "          [205, 205, 197],\n",
      "          [203, 203, 195],\n",
      "          ...,\n",
      "          [138, 142, 125],\n",
      "          [144, 144, 128],\n",
      "          [109, 103,  91]],\n",
      "\n",
      "         [[205, 205, 197],\n",
      "          [200, 200, 192],\n",
      "          [200, 200, 192],\n",
      "          ...,\n",
      "          [138, 144, 133],\n",
      "          [147, 154, 145],\n",
      "          [ 99, 107, 102]],\n",
      "\n",
      "         [[204, 204, 196],\n",
      "          [200, 200, 192],\n",
      "          [201, 201, 193],\n",
      "          ...,\n",
      "          [139, 148, 138],\n",
      "          [105, 118, 114],\n",
      "          [160, 174, 180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[113,  93,  85],\n",
      "          [140, 109,  82],\n",
      "          [172, 126,  71],\n",
      "          ...,\n",
      "          [156, 157, 181],\n",
      "          [174, 177, 184],\n",
      "          [190, 177, 164]],\n",
      "\n",
      "         [[117,  98,  90],\n",
      "          [140, 109,  84],\n",
      "          [171, 125,  78],\n",
      "          ...,\n",
      "          [144, 146, 164],\n",
      "          [180, 182, 185],\n",
      "          [199, 184, 168]],\n",
      "\n",
      "         [[120, 101,  90],\n",
      "          [139, 108,  84],\n",
      "          [174, 128,  82],\n",
      "          ...,\n",
      "          [149, 154, 163],\n",
      "          [177, 179, 181],\n",
      "          [204, 187, 178]]],\n",
      "\n",
      "\n",
      "        [[[151, 111,  42],\n",
      "          [154, 116,  46],\n",
      "          [154, 115,  41],\n",
      "          ...,\n",
      "          [ 96,  85,  65],\n",
      "          [ 86,  80,  61],\n",
      "          [ 85,  84,  65]],\n",
      "\n",
      "         [[157, 118,  53],\n",
      "          [152, 114,  46],\n",
      "          [148, 109,  39],\n",
      "          ...,\n",
      "          [ 96,  84,  64],\n",
      "          [ 83,  76,  60],\n",
      "          [ 82,  79,  64]],\n",
      "\n",
      "         [[160, 121,  57],\n",
      "          [156, 118,  53],\n",
      "          [151, 111,  45],\n",
      "          ...,\n",
      "          [105,  92,  72],\n",
      "          [ 85,  77,  61],\n",
      "          [ 82,  79,  63]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[157, 122,  73],\n",
      "          [152, 118,  70],\n",
      "          [147, 113,  65],\n",
      "          ...,\n",
      "          [126,  94,  53],\n",
      "          [131, 100,  57],\n",
      "          [140, 108,  61]],\n",
      "\n",
      "         [[150, 115,  66],\n",
      "          [146, 112,  64],\n",
      "          [145, 111,  63],\n",
      "          ...,\n",
      "          [124,  92,  51],\n",
      "          [126,  94,  53],\n",
      "          [131,  99,  54]],\n",
      "\n",
      "         [[154, 118,  67],\n",
      "          [145, 110,  60],\n",
      "          [145, 110,  61],\n",
      "          ...,\n",
      "          [118,  85,  45],\n",
      "          [119,  86,  44],\n",
      "          [124,  91,  44]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[255, 255, 255],\n",
      "          [255, 255, 255],\n",
      "          [255, 255, 255],\n",
      "          ...,\n",
      "          [ 97,  79,  69],\n",
      "          [ 99,  80,  72],\n",
      "          [102,  81,  74]],\n",
      "\n",
      "         [[255, 255, 255],\n",
      "          [247, 251, 252],\n",
      "          [254, 255, 255],\n",
      "          ...,\n",
      "          [ 98,  78,  77],\n",
      "          [ 95,  75,  74],\n",
      "          [ 99,  79,  75]],\n",
      "\n",
      "         [[255, 255, 255],\n",
      "          [250, 252, 253],\n",
      "          [253, 252, 253],\n",
      "          ...,\n",
      "          [ 93,  75,  71],\n",
      "          [ 88,  70,  63],\n",
      "          [ 95,  76,  69]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[209, 198, 134],\n",
      "          [202, 191, 130],\n",
      "          [226, 215, 153],\n",
      "          ...,\n",
      "          [191, 173, 107],\n",
      "          [207, 188, 125],\n",
      "          [220, 200, 133]],\n",
      "\n",
      "         [[204, 193, 128],\n",
      "          [217, 206, 142],\n",
      "          [226, 215, 151],\n",
      "          ...,\n",
      "          [229, 211, 152],\n",
      "          [206, 186, 124],\n",
      "          [216, 196, 132]],\n",
      "\n",
      "         [[215, 203, 134],\n",
      "          [239, 228, 162],\n",
      "          [227, 216, 151],\n",
      "          ...,\n",
      "          [224, 206, 139],\n",
      "          [185, 165, 103],\n",
      "          [216, 195, 133]]],\n",
      "\n",
      "\n",
      "        [[[210, 255, 255],\n",
      "          [196, 255, 242],\n",
      "          [195, 255, 241],\n",
      "          ...,\n",
      "          [205, 255, 244],\n",
      "          [205, 255, 244],\n",
      "          [220, 255, 255]],\n",
      "\n",
      "         [[195, 255, 241],\n",
      "          [182, 240, 225],\n",
      "          [182, 240, 225],\n",
      "          ...,\n",
      "          [192, 243, 228],\n",
      "          [192, 243, 228],\n",
      "          [205, 255, 244]],\n",
      "\n",
      "         [[195, 255, 241],\n",
      "          [182, 240, 225],\n",
      "          [183, 241, 226],\n",
      "          ...,\n",
      "          [192, 242, 228],\n",
      "          [192, 242, 228],\n",
      "          [206, 255, 244]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[224, 107,  38],\n",
      "          [215, 106,  41],\n",
      "          [220, 110,  47],\n",
      "          ...,\n",
      "          [204,  96,  34],\n",
      "          [209, 103,  41],\n",
      "          [216, 103,  36]],\n",
      "\n",
      "         [[226, 109,  40],\n",
      "          [207,  98,  33],\n",
      "          [204,  95,  31],\n",
      "          ...,\n",
      "          [209, 101,  39],\n",
      "          [202,  96,  34],\n",
      "          [215, 102,  35]],\n",
      "\n",
      "         [[243, 118,  42],\n",
      "          [228, 111,  39],\n",
      "          [220, 104,  35],\n",
      "          ...,\n",
      "          [225, 108,  45],\n",
      "          [214, 100,  36],\n",
      "          [231, 110,  37]]],\n",
      "\n",
      "\n",
      "        [[[  7,  13,   0],\n",
      "          [  7,  13,   0],\n",
      "          [ 14,  21,   5],\n",
      "          ...,\n",
      "          [ 49,  54,  20],\n",
      "          [ 44,  45,  12],\n",
      "          [ 33,  34,   0]],\n",
      "\n",
      "         [[ 11,  17,   2],\n",
      "          [  9,  15,   3],\n",
      "          [ 12,  18,   4],\n",
      "          ...,\n",
      "          [ 45,  50,  15],\n",
      "          [ 45,  46,  12],\n",
      "          [ 45,  46,  11]],\n",
      "\n",
      "         [[ 10,  13,   0],\n",
      "          [ 24,  27,  16],\n",
      "          [ 26,  29,  18],\n",
      "          ...,\n",
      "          [ 44,  48,  14],\n",
      "          [ 45,  49,  14],\n",
      "          [ 45,  47,  11]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[166, 197, 231],\n",
      "          [151, 185, 208],\n",
      "          [161, 210, 227],\n",
      "          ...,\n",
      "          [117,  96, 121],\n",
      "          [128, 114, 136],\n",
      "          [149, 130, 157]],\n",
      "\n",
      "         [[175, 227, 253],\n",
      "          [153, 180, 206],\n",
      "          [148, 153, 181],\n",
      "          ...,\n",
      "          [135, 129, 158],\n",
      "          [148, 138, 166],\n",
      "          [135, 106, 139]],\n",
      "\n",
      "         [[176, 229, 253],\n",
      "          [181, 226, 247],\n",
      "          [157, 165, 193],\n",
      "          ...,\n",
      "          [162, 196, 219],\n",
      "          [161, 184, 211],\n",
      "          [156, 164, 195]]]], dtype=torch.uint8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1141 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features_train \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(d)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m ft \u001b[38;5;241m=\u001b[39m \u001b[43mclip_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Flatten the features\u001b[39;00m\n\u001b[1;32m     20\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(ft)\n",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m, in \u001b[0;36mclip_features\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_features\u001b[39m(d):\n\u001b[0;32m----> 3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma photo of a cat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma photo of a dog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m         inputs[key] \u001b[38;5;241m=\u001b[39m inputs[key]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/clip/processing_clip.py:102\u001b[0m, in \u001b[0;36mCLIPProcessor.__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/image_processing_utils.py:464\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:323\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 323\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    326\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39mcrop_size) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:323\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    320\u001b[0m images \u001b[38;5;241m=\u001b[39m [to_numpy_array(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[0;32m--> 323\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[1;32m    326\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39mcrop_size) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:150\u001b[0m, in \u001b[0;36mCLIPImageProcessor.resize\u001b[0;34m(self, image, size, resample, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `size` parameter must contain the key `shortest_edge`. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m output_size \u001b[38;5;241m=\u001b[39m get_resize_output_image_size(image, size\u001b[38;5;241m=\u001b[39msize[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortest_edge\u001b[39m\u001b[38;5;124m\"\u001b[39m], default_to_square\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/image_transforms.py:291\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize\u001b[39m(\n\u001b[1;32m    262\u001b[0m     image,\n\u001b[1;32m    263\u001b[0m     size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     return_numpy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    Resizes `image` to `(height, width)` specified by `size` using the PIL library.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m        `np.ndarray`: The resized image.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     resample \u001b[38;5;241m=\u001b[39m resample \u001b[38;5;28;01mif\u001b[39;00m resample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m PILImageResampling\u001b[38;5;241m.\u001b[39mBILINEAR\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/utils/import_utils.py:1012\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1011\u001b[0m checks \u001b[38;5;241m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[38;5;28;01mfor\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m backends)\n\u001b[0;32m-> 1012\u001b[0m failed \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mavailable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchecks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mavailable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/utils/import_utils.py:1012\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1011\u001b[0m checks \u001b[38;5;241m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[38;5;28;01mfor\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m backends)\n\u001b[0;32m-> 1012\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mavailable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/utils/import_utils.py:541\u001b[0m, in \u001b[0;36mis_vision_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pil_available:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m         package_version \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPillow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m importlib_metadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:1008\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:981\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    976\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \n\u001b[1;32m    978\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:563\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA distribution name is required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:915\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find metadata directories in paths heuristically.\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m prepared \u001b[38;5;241m=\u001b[39m Prepared(name)\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m--> 915\u001b[0m     \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(FastPath, paths)\n\u001b[1;32m    916\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:813\u001b[0m, in \u001b[0;36mFastPath.search\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtime\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msearch(name)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:823\u001b[0m, in \u001b[0;36mFastPath.lookup\u001b[0;34m(self, mtime)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;129m@method_cache\u001b[39m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlookup\u001b[39m(\u001b[38;5;28mself\u001b[39m, mtime):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLookup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:833\u001b[0m, in \u001b[0;36mLookup.__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfos \u001b[38;5;241m=\u001b[39m FreezableDefaultDict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meggs \u001b[38;5;241m=\u001b[39m FreezableDefaultDict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m--> 833\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    834\u001b[0m     low \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m low\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.dist-info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.egg-info\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;66;03m# rpartition is faster than splitext and suitable for this purpose.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:800\u001b[0m, in \u001b[0;36mFastPath.children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 800\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip_children()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_train = extract_features(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7374aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = nsd_feats_dir + 'openai-clip-vit-base-patch32_512.npy'\n",
    "features_train = np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3008f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63888c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd_feats_dir = '../../../share/nklab/projects/nsd_features/'\n",
    "\n",
    "#'detr_encoder_last_1000pc.npy'\n",
    "#'detr_encoder_last_500pc.npy' \n",
    "# 'dinov2_vitb14_patch_features_last1_1000pc.npy'\n",
    "#'dinov2_vitb14_q_features_last1_100pc.npy'\n",
    "\n",
    "file_name = nsd_feats_dir + 'CLIP_ViT_L_14_336_features.npy'\n",
    "features_train = np.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "905c20e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=50)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=50)\n",
    "pca.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aa0c66e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_pca = pca.transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3ae62f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = nsd_feats_dir + 'openai-clip-vit-base-patch32_50pc.npy'\n",
    "np.save(file_name, features_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3858b6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6f8e9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.13369147e-02,  4.46945392e-02,  1.53597696e-02, -1.49051193e-02,\n",
       "        3.67578417e-02,  4.20614779e-02,  9.07251053e-03, -3.41922827e-02,\n",
       "        4.21581045e-02,  7.98099581e-03, -1.36037543e-03, -4.07831371e-02,\n",
       "        1.87325440e-02,  3.83571163e-02, -3.12291905e-02,  4.48935898e-03,\n",
       "       -1.57416239e-02, -1.07952589e-02,  4.28928919e-02, -3.77885140e-02,\n",
       "       -9.35867876e-02, -4.67771851e-02, -1.44485934e-02,  2.68457104e-02,\n",
       "        2.87249479e-02,  6.21722192e-02,  8.40742048e-03,  5.07963598e-02,\n",
       "       -1.23138418e-02,  1.89871714e-03, -4.41632718e-02,  2.80229356e-02,\n",
       "        3.62683237e-02,  1.38974069e-02, -1.55962538e-03, -3.61796492e-03,\n",
       "        2.82525085e-02, -3.09100021e-02,  4.25203936e-03,  1.15564503e-01,\n",
       "        1.59260016e-02,  2.24354621e-02, -1.53426016e-02, -1.11231357e-02,\n",
       "        1.21209603e-02, -9.26479250e-02,  4.64015752e-02, -8.44668411e-03,\n",
       "       -3.40510309e-02,  2.24342961e-02, -4.51825075e-02, -2.66059823e-02,\n",
       "       -2.36431491e-02, -5.15010282e-02, -2.49090623e-02,  4.37921733e-02,\n",
       "        1.38027435e-02,  5.99228498e-03,  1.59874819e-02,  1.25917364e-02,\n",
       "       -4.06080224e-02,  8.47580284e-03,  3.40505019e-02, -9.61414631e-03,\n",
       "        1.20891035e-02,  5.68772666e-03, -9.30321403e-03,  1.26951084e-01,\n",
       "       -4.23756465e-02, -1.43088717e-02,  2.13151723e-02,  2.25605257e-02,\n",
       "       -1.78747233e-02,  3.48232803e-03, -1.21734971e-02,  5.30224741e-02,\n",
       "        3.75735797e-02, -1.55535322e-02, -2.08274573e-02, -4.04102076e-03,\n",
       "        1.42221283e-02, -8.46835785e-03, -1.15297196e-04,  9.35302582e-03,\n",
       "       -3.35288905e-02,  2.13542841e-02, -1.27616432e-02,  5.39744180e-03,\n",
       "       -7.51980301e-03,  4.20003869e-02,  3.36344875e-02, -4.54374664e-02,\n",
       "       -5.67704320e-01, -6.00837506e-02,  2.42999718e-02, -3.14052068e-02,\n",
       "        3.25012133e-02, -1.01983128e-02, -4.34612446e-02, -1.77844182e-01,\n",
       "        4.45742197e-02,  1.27603691e-02, -8.70936438e-02,  3.67498919e-02,\n",
       "        6.15999009e-03, -2.52398830e-02, -7.50472620e-02, -2.56120320e-02,\n",
       "       -2.04942748e-02,  3.41178849e-02,  9.01192240e-03, -9.08017755e-02,\n",
       "        2.87299771e-02,  1.06396731e-02,  7.90082850e-03, -5.07130921e-02,\n",
       "       -1.58739537e-02, -1.95889417e-02,  1.44146131e-02, -2.65657920e-02,\n",
       "        7.63649773e-03, -8.81755352e-03,  7.39859510e-03, -2.04098206e-02,\n",
       "       -3.35595272e-02,  5.55462874e-02,  2.39789542e-02, -8.15112237e-03,\n",
       "       -2.75863130e-02, -1.55022356e-03,  6.45137578e-02, -9.43435822e-03,\n",
       "        4.44180658e-03,  7.87159130e-02, -1.34248678e-02, -1.65993907e-02,\n",
       "        1.01683997e-02, -5.38156405e-02,  4.27888008e-03, -1.60351377e-02,\n",
       "        1.54473232e-02, -3.12787690e-03,  4.49282743e-05, -3.62559557e-02,\n",
       "        3.95853296e-02, -7.08456337e-03,  1.14349844e-02, -2.42095608e-02,\n",
       "       -5.81703223e-02, -7.48671242e-04,  5.46730757e-02, -6.90324325e-03,\n",
       "        5.77920955e-03,  8.71000905e-03, -3.13324062e-03, -6.13748319e-02,\n",
       "       -6.56434707e-03,  4.03283685e-02, -3.96880647e-03, -2.11545844e-02,\n",
       "        1.73807505e-03, -6.70449659e-02,  5.04559018e-02,  3.36803608e-02,\n",
       "        3.61095965e-02, -7.24157551e-03,  3.65887135e-02,  5.80903515e-02,\n",
       "        9.76806134e-03, -3.50029655e-02,  1.25925988e-02,  2.57783085e-02,\n",
       "        2.52293013e-02,  1.92765915e-03, -2.45513115e-02, -2.84045842e-02,\n",
       "       -4.50205766e-02, -5.06337620e-02, -9.05520990e-02,  4.00760993e-02,\n",
       "        6.46090731e-02,  2.58685127e-02,  1.56873912e-02,  3.07733770e-02,\n",
       "       -3.32371593e-02,  7.06238148e-04,  2.34845970e-02, -7.67658371e-03,\n",
       "        3.98068726e-02, -1.26021644e-02,  4.61687334e-03, -1.86170137e-03,\n",
       "       -4.73719202e-02,  4.38577905e-02, -3.53971049e-02, -4.29131612e-02,\n",
       "       -2.55994741e-02,  6.75321817e-02,  8.02568719e-02,  4.10859846e-02,\n",
       "       -1.91140398e-02, -2.56839022e-02, -2.79249605e-02,  1.47541389e-02,\n",
       "       -1.17435437e-02, -4.12017889e-02, -4.99196053e-02, -8.22089519e-03,\n",
       "        3.75342765e-03,  5.67628769e-03, -1.94204487e-02, -1.33324452e-02,\n",
       "       -1.04168095e-02,  1.49329528e-02,  1.68079603e-02,  4.77546602e-02,\n",
       "        2.15331707e-02,  5.95336370e-02, -2.22237390e-02, -2.68784408e-02,\n",
       "        3.65919434e-02, -4.34164144e-02, -3.00608575e-02,  5.15389293e-02,\n",
       "        4.24782485e-02,  1.42901042e-03, -1.00571825e-03, -2.42376216e-02,\n",
       "        1.40103986e-02, -2.19936930e-02, -5.83547130e-02, -1.95362139e-02,\n",
       "       -7.25912536e-03, -1.65076870e-02, -3.14502884e-03, -1.77726805e-01,\n",
       "       -4.46077287e-02, -1.42815728e-02, -2.32968517e-02, -2.69739199e-02,\n",
       "       -6.03090152e-02, -3.56170237e-02, -3.04733030e-02,  9.49893892e-03,\n",
       "        3.58510436e-03, -3.31724696e-02,  1.93476118e-02,  2.51456983e-02,\n",
       "       -5.07292198e-03,  2.77490262e-02, -1.29191745e-02,  1.58999569e-03,\n",
       "       -1.11586191e-02,  1.20712267e-02, -8.18173140e-02,  2.12374516e-02,\n",
       "        3.30835357e-02,  2.79563442e-02,  8.44397768e-02,  1.06750196e-02,\n",
       "       -2.05957741e-02,  4.54248823e-02, -3.64285568e-03, -3.21924244e-03,\n",
       "        1.43636586e-02, -1.56634487e-02,  3.49418297e-02,  8.43295362e-03,\n",
       "       -4.40317243e-02,  3.28441374e-02,  1.97603852e-02,  1.31540559e-02,\n",
       "        2.50971392e-02, -4.31817509e-02,  2.65775267e-02, -1.58857387e-02,\n",
       "        8.65464844e-03, -2.01069750e-02,  2.09889491e-03, -7.18968222e-03,\n",
       "        1.59198493e-02,  7.49219209e-03, -4.08300944e-02,  2.93936245e-02,\n",
       "       -5.75107988e-03,  4.77625467e-02,  1.77998364e-01, -3.41934934e-02,\n",
       "        1.51975751e-02,  6.41941139e-03,  2.70788334e-02, -3.28776836e-02,\n",
       "        2.33611036e-02,  4.57749031e-02, -4.90217889e-03,  6.24449961e-02,\n",
       "        9.81290359e-03,  1.31613994e-02, -8.99839494e-03,  7.05238581e-02,\n",
       "        4.37068492e-02, -4.17929888e-02,  1.72158773e-03, -2.17850562e-02,\n",
       "       -1.51349073e-02, -6.27506226e-02, -9.71132424e-03,  1.84910260e-02,\n",
       "        3.47110666e-02, -5.97395375e-03, -6.14345493e-03,  9.09077656e-03,\n",
       "        7.84339979e-02,  2.05520485e-02,  8.88102036e-03,  3.65463272e-02,\n",
       "        1.78501324e-03,  4.81629334e-02, -1.04178907e-02, -3.98659147e-02,\n",
       "        6.96093515e-02,  3.59688746e-03, -1.80625338e-02, -5.19730970e-02,\n",
       "        1.68363508e-02,  2.80489083e-02, -2.67801508e-02, -6.88222572e-02,\n",
       "        7.61643201e-02, -5.59231602e-02,  1.47435172e-02, -4.69642729e-02,\n",
       "        3.83518240e-03, -2.39352006e-02, -1.91892106e-02,  1.58685576e-02,\n",
       "       -9.38123651e-03,  1.14095295e-02, -2.53792666e-02, -1.07410522e-02,\n",
       "        3.90704572e-02, -1.83500648e-02, -1.63155347e-02, -2.17704363e-02,\n",
       "        1.91632081e-02,  5.26346313e-03, -5.81375957e-02, -1.98554853e-03,\n",
       "       -2.11687647e-02,  2.16357261e-02,  3.44169401e-02, -1.57537125e-02,\n",
       "       -1.63787864e-02, -5.51613804e-04, -2.30341237e-02,  1.70111880e-02,\n",
       "       -4.26238365e-02, -2.17995439e-02, -3.04130442e-03, -3.62178311e-02,\n",
       "        4.36881408e-02, -1.37789464e-02, -1.17000146e-02, -1.58936083e-02,\n",
       "        3.76010537e-02,  5.64112794e-03, -1.35228550e-02, -1.10502057e-01,\n",
       "        1.60771757e-02, -4.52215318e-03, -1.69922523e-02,  3.48144248e-02,\n",
       "        9.25062783e-03, -4.87137772e-02, -1.05230035e-02, -3.20390984e-02,\n",
       "        9.58354920e-02,  5.50566614e-02,  2.01625619e-02,  1.34265712e-02,\n",
       "       -7.27326982e-03,  1.87598150e-02, -1.69328935e-02, -4.40831203e-03,\n",
       "       -4.03840169e-02,  6.69042096e-02,  3.21066268e-02, -5.07422769e-03,\n",
       "       -4.33966182e-02, -1.54335778e-02, -2.28027049e-02, -6.36328245e-04,\n",
       "        1.47311781e-02,  4.83160466e-02,  8.74294376e-04,  3.57971038e-03,\n",
       "       -2.13274546e-02,  5.58378883e-02, -5.38369827e-02, -1.07122539e-02,\n",
       "        5.03389090e-02, -2.37306673e-02,  3.26979831e-02, -1.97232552e-02,\n",
       "       -1.70254037e-02, -1.28583992e-02,  3.14349160e-02, -4.10696119e-02,\n",
       "        5.84666319e-02,  1.66287627e-02,  5.91954701e-02,  2.14037932e-02,\n",
       "       -6.63826661e-03, -3.45637314e-02, -3.20151597e-02, -1.37097873e-02,\n",
       "       -4.66905795e-02, -1.41176907e-02,  2.21203789e-02, -1.69602893e-02,\n",
       "        3.13936695e-02,  1.11226626e-02,  2.85148956e-02, -5.70936874e-02,\n",
       "       -3.19910794e-02, -3.63025703e-02, -2.07859613e-02, -4.77553019e-03,\n",
       "        4.43397984e-02, -4.29784805e-02, -2.48405077e-02,  3.85343097e-02,\n",
       "       -6.03020377e-02, -3.77249904e-02, -2.48488840e-02, -6.82629063e-04,\n",
       "       -5.19058518e-02,  3.80637422e-02, -4.62203212e-02,  9.31905583e-03,\n",
       "        1.28197633e-02, -2.62140967e-02, -1.98701285e-02,  2.51750574e-02,\n",
       "        2.31887493e-02, -3.33088525e-02, -1.73869934e-02,  1.54963732e-02,\n",
       "       -1.33342315e-02, -2.72533502e-02,  1.64741073e-02, -4.58700582e-02,\n",
       "       -7.02769533e-02,  8.57073907e-03,  1.64940339e-02,  2.49065850e-02,\n",
       "        4.90726624e-03, -2.50847470e-02,  8.97496380e-03, -3.26197781e-02,\n",
       "       -1.07667958e-02,  5.75086474e-03, -9.95477475e-03, -1.12545928e-02,\n",
       "       -5.87905832e-02, -4.62219194e-02,  4.11119945e-02, -5.48988804e-02,\n",
       "       -6.69949874e-02, -1.90415978e-02, -4.33672220e-02,  2.80340612e-02,\n",
       "        1.94853265e-02, -2.39579994e-02,  2.19245702e-02,  2.31228280e-03,\n",
       "        1.25939269e-02, -5.60789090e-03, -1.41425664e-02, -3.51112895e-02,\n",
       "       -4.55925846e-03, -2.32179016e-02,  3.37339286e-03, -3.86221819e-02,\n",
       "       -3.93311456e-02,  8.25053677e-02,  1.26070017e-02, -3.85384113e-02,\n",
       "        3.06391753e-02, -2.84199081e-02, -4.50355336e-02,  1.86449047e-02,\n",
       "        2.22928077e-02, -7.66122490e-02, -2.28962791e-03, -5.77199506e-03,\n",
       "       -1.06449537e-02,  5.30060902e-02, -1.67424921e-02, -1.26369353e-02,\n",
       "       -2.47769039e-02, -1.85813401e-02, -2.25116983e-02,  5.35878800e-02,\n",
       "        3.18070198e-03, -2.72621289e-02, -9.33454279e-03, -3.14110294e-02,\n",
       "       -6.12721592e-03,  5.51926605e-02,  4.91992431e-03, -3.41873392e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_pca[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4603904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73000, 512)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = nsd_feats_dir + 'openai-clip-vit-base-patch32_512.npy'\n",
    "features_pca = np.load(file_name)\n",
    "features_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "084a8b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subj in range(1,9): #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n",
    "\n",
    "    class argObj:\n",
    "        def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "\n",
    "            self.subj = format(subj, '02')\n",
    "            \n",
    "            self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "            self.parent_submission_dir = parent_submission_dir\n",
    "            self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "                'subj'+self.subj)\n",
    "\n",
    "            # Create the submission directory if not existing\n",
    "            if not os.path.isdir(self.subject_submission_dir):\n",
    "                os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "    args = argObj(data_dir, parent_submission_dir, subj)\n",
    "\n",
    "    train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "    #test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "    \n",
    "    test_img_dir = os.path.join(args.data_dir, '../nsdsynthetic_stimuli/')\n",
    "\n",
    "    # Create lists will all training and test image file names, sorted\n",
    "    train_img_list = os.listdir(train_img_dir)\n",
    "    train_img_list.sort()\n",
    "    test_img_list = os.listdir(test_img_dir)\n",
    "    test_img_list.sort()\n",
    "#     print('Training images: ' + str(len(train_img_list)))\n",
    "#     print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "    train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "    test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "    idxs_train = np.arange(len(train_img_list))\n",
    "    idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "    fts_subj_train = []\n",
    "    for i in range(len(train_imgs_paths)):\n",
    "\n",
    "        ind = int(str(train_imgs_paths[i]).split('-')[-1].split('.')[0])\n",
    "        fts_subj_train.append(features_pca[ind])\n",
    "\n",
    "    fts_subj_train = np.vstack(fts_subj_train)\n",
    "\n",
    "    fts_subj_test = []\n",
    "    for i in range(len(test_imgs_paths)):\n",
    "\n",
    "        ind = int(str(test_imgs_paths[i]).split('-')[-1].split('.')[0])\n",
    "        fts_subj_test.append(features_pca[ind])\n",
    "\n",
    "    features_test = np.vstack(fts_subj_test)\n",
    "    \n",
    "    feat_dir = './algonauts_image_features/clip_vit_512/' +  args.subj \n",
    "    \n",
    "    if not os.path.isdir(feat_dir):\n",
    "        os.makedirs(feat_dir)\n",
    "        \n",
    "    \n",
    "    #np.save(feat_dir + '/train.npy', fts_subj_train)\n",
    "    np.save(feat_dir + '/synt.npy', features_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239416c",
   "metadata": {},
   "source": [
    "### for image dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8fef033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 9841\n",
      "Test images: 220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 714, 714, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 109\u001b[0m\n\u001b[1;32m     98\u001b[0m train_imgs_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     99\u001b[0m     ImageDataset(train_imgs_paths, idxs_train, transform), \n\u001b[1;32m    100\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m test_imgs_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m    104\u001b[0m     ImageDataset(test_imgs_paths, idxs_test, transform), \n\u001b[1;32m    105\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[1;32m    106\u001b[0m )\n\u001b[0;32m--> 109\u001b[0m features_test \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_imgs_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m#np.save(subject_feature_dir + '/synt.npy', features_test)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 18\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(d\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m ft \u001b[38;5;241m=\u001b[39m \u001b[43mclip_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Flatten the features\u001b[39;00m\n\u001b[1;32m     20\u001b[0m features\u001b[38;5;241m.\u001b[39mappend(ft)\n",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m, in \u001b[0;36mclip_features\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip_features\u001b[39m(d):\n\u001b[0;32m----> 3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma photo of a cat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma photo of a dog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      6\u001b[0m         inputs[key] \u001b[38;5;241m=\u001b[39m inputs[key]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/clip/processing_clip.py:102\u001b[0m, in \u001b[0;36mCLIPProcessor.__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m     encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/image_processing_utils.py:464\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m    463\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:320\u001b[0m, in \u001b[0;36mCLIPImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     images \u001b[38;5;241m=\u001b[39m [convert_to_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mto_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    323\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, resample\u001b[38;5;241m=\u001b[39mresample) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/clip/image_processing_clip.py:320\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    317\u001b[0m     images \u001b[38;5;241m=\u001b[39m [convert_to_rgb(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# All transformations expect numpy arrays.\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m images \u001b[38;5;241m=\u001b[39m [\u001b[43mto_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    323\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, resample\u001b[38;5;241m=\u001b[39mresample) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/image_utils.py:139\u001b[0m, in \u001b[0;36mto_numpy_array\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_numpy_array\u001b[39m(img) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_valid_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_vision_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/image_utils.py:73\u001b[0m, in \u001b[0;36mis_valid_image\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_image\u001b[39m(img):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 73\u001b[0m         (\u001b[43mis_vision_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage))\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_torch_tensor(img)\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_tf_tensor(img)\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_jax_tensor(img)\n\u001b[1;32m     78\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/site-packages/transformers/utils/import_utils.py:541\u001b[0m, in \u001b[0;36mis_vision_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pil_available:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 541\u001b[0m         package_version \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPillow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m importlib_metadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:1008\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \n\u001b[1;32m   1004\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:981\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    976\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \n\u001b[1;32m    978\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:563\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA distribution name is required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mdiscover(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:915\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find metadata directories in paths heuristically.\"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m prepared \u001b[38;5;241m=\u001b[39m Prepared(name)\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m--> 915\u001b[0m     \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(FastPath, paths)\n\u001b[1;32m    916\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:813\u001b[0m, in \u001b[0;36mFastPath.search\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtime\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msearch(name)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:823\u001b[0m, in \u001b[0;36mFastPath.lookup\u001b[0;34m(self, mtime)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;129m@method_cache\u001b[39m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlookup\u001b[39m(\u001b[38;5;28mself\u001b[39m, mtime):\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLookup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:833\u001b[0m, in \u001b[0;36mLookup.__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfos \u001b[38;5;241m=\u001b[39m FreezableDefaultDict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meggs \u001b[38;5;241m=\u001b[39m FreezableDefaultDict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m--> 833\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    834\u001b[0m     low \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m low\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.dist-info\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.egg-info\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    836\u001b[0m         \u001b[38;5;66;03m# rpartition is faster than splitext and suitable for this purpose.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.11/importlib/metadata/__init__.py:800\u001b[0m, in \u001b[0;36mFastPath.children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m--> 800\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip_children()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr\n",
    "\n",
    "\n",
    "data_dir = './algonauts_2023_challenge_data'\n",
    "parent_submission_dir = './algonauts_2023_challenge_submission'\n",
    "\n",
    "device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "device = torch.device(device)\n",
    "\n",
    "for subj in range(1,9): #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n",
    "\n",
    "    class argObj:\n",
    "        def __init__(self, data_dir, parent_submission_dir, subj):\n",
    "\n",
    "            self.subj = format(subj, '02')\n",
    "            self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "            self.parent_submission_dir = parent_submission_dir\n",
    "            self.subject_submission_dir = os.path.join(self.parent_submission_dir,\n",
    "                'subj'+self.subj)\n",
    "\n",
    "            # Create the submission directory if not existing\n",
    "            if not os.path.isdir(self.subject_submission_dir):\n",
    "                os.makedirs(self.subject_submission_dir)\n",
    "\n",
    "\n",
    "\n",
    "    args = argObj(data_dir, parent_submission_dir, subj)\n",
    "\n",
    "    feature_dir = './algonauts_image_features'\n",
    "\n",
    "    subject_feature_dir =  os.path.join(feature_dir, 'clip_vit_512',format(subj, '02'))\n",
    "\n",
    "    if not os.path.isdir(subject_feature_dir):\n",
    "        os.makedirs(subject_feature_dir)\n",
    "\n",
    "\n",
    "    train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "    #test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "\n",
    "    test_img_dir = os.path.join(args.data_dir, '../nsdsynthetic_stimuli/')\n",
    "\n",
    "    # Create lists will all training and test image file names, sorted\n",
    "    train_img_list = os.listdir(train_img_dir)\n",
    "    train_img_list.sort()\n",
    "    test_img_list = os.listdir(test_img_dir)\n",
    "    test_img_list.sort()\n",
    "    print('Training images: ' + str(len(train_img_list)))\n",
    "    print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "    idxs_train = np.arange(len(train_img_list))\n",
    "    idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.Resize((224,224)), # resize the images to 224x24 pixels\n",
    "        transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "    ])\n",
    "\n",
    "    class ImageDataset(Dataset):\n",
    "        def __init__(self, imgs_paths, idxs, transform):\n",
    "            self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.imgs_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Load the image\n",
    "            img_path = self.imgs_paths[idx]\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            #img = img.resize()\n",
    "            # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "    #         if self.transform:\n",
    "    #             img = self.transform(img).to(device)\n",
    "            return img\n",
    "\n",
    "\n",
    "    batch_size = 200 #@param\n",
    "    # Get the paths of all image files\n",
    "    train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "    test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "    # The DataLoaders contain the ImageDataset class\n",
    "    train_imgs_dataloader = DataLoader(\n",
    "        ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_imgs_dataloader = DataLoader(\n",
    "        ImageDataset(test_imgs_paths, idxs_test, transform), \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    \n",
    "    features_test = extract_features(test_imgs_dataloader)\n",
    "    \n",
    "    #np.save(subject_feature_dir + '/synt.npy', features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f251e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:25<00:00, 12.96s/it]\n"
     ]
    }
   ],
   "source": [
    "features_test = extract_features(test_imgs_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1cfff94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mmax(\u001b[43mimg\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "np.max(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "831df05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./algonauts_image_features/clip_vit_512/subj04/synt.npy'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_feature_dir + '/synt.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e60d3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c2cc21e",
   "metadata": {},
   "source": [
    "## Sentence encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85b221f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd_stim_info =  '../../../share/nklab/projects/natural_scenes_dataset/nsddata/experiments/nsd/nsd_stim_info_merged.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa536f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cocoId</th>\n",
       "      <th>cocoSplit</th>\n",
       "      <th>cropBox</th>\n",
       "      <th>loss</th>\n",
       "      <th>nsdId</th>\n",
       "      <th>flagged</th>\n",
       "      <th>BOLD5000</th>\n",
       "      <th>shared1000</th>\n",
       "      <th>subject1</th>\n",
       "      <th>...</th>\n",
       "      <th>subject5_rep2</th>\n",
       "      <th>subject6_rep0</th>\n",
       "      <th>subject6_rep1</th>\n",
       "      <th>subject6_rep2</th>\n",
       "      <th>subject7_rep0</th>\n",
       "      <th>subject7_rep1</th>\n",
       "      <th>subject7_rep2</th>\n",
       "      <th>subject8_rep0</th>\n",
       "      <th>subject8_rep1</th>\n",
       "      <th>subject8_rep2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>532481</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.1671875, 0.1671875)</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>245764</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13985</td>\n",
       "      <td>14176</td>\n",
       "      <td>28603</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>385029</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>311303</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.16640625, 0.16640625)</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>393226</td>\n",
       "      <td>val2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>13720</td>\n",
       "      <td>22861</td>\n",
       "      <td>23023</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72995</th>\n",
       "      <td>72995</td>\n",
       "      <td>518071</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72995</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6083</td>\n",
       "      <td>11650</td>\n",
       "      <td>26531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72996</th>\n",
       "      <td>72996</td>\n",
       "      <td>255930</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0, 0, 0.125, 0.125)</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>72996</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10402</td>\n",
       "      <td>10434</td>\n",
       "      <td>10625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72997</th>\n",
       "      <td>72997</td>\n",
       "      <td>255934</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0, 0, 0.1, 0.1)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72997</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72998</th>\n",
       "      <td>72998</td>\n",
       "      <td>518080</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0.125, 0.125, 0, 0)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72998</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5585</td>\n",
       "      <td>11846</td>\n",
       "      <td>14495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72999</th>\n",
       "      <td>72999</td>\n",
       "      <td>518083</td>\n",
       "      <td>train2017</td>\n",
       "      <td>(0, 0, 0.16640625, 0.16640625)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72999</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73000 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  cocoId  cocoSplit                         cropBox  \\\n",
       "0               0  532481    val2017    (0, 0, 0.1671875, 0.1671875)   \n",
       "1               1  245764    val2017            (0, 0, 0.125, 0.125)   \n",
       "2               2  385029    val2017            (0, 0, 0.125, 0.125)   \n",
       "3               3  311303    val2017  (0, 0, 0.16640625, 0.16640625)   \n",
       "4               4  393226    val2017            (0, 0, 0.125, 0.125)   \n",
       "...           ...     ...        ...                             ...   \n",
       "72995       72995  518071  train2017            (0, 0, 0.125, 0.125)   \n",
       "72996       72996  255930  train2017            (0, 0, 0.125, 0.125)   \n",
       "72997       72997  255934  train2017                (0, 0, 0.1, 0.1)   \n",
       "72998       72998  518080  train2017            (0.125, 0.125, 0, 0)   \n",
       "72999       72999  518083  train2017  (0, 0, 0.16640625, 0.16640625)   \n",
       "\n",
       "           loss  nsdId  flagged  BOLD5000  shared1000  subject1  ...  \\\n",
       "0      0.100000      0    False     False       False         0  ...   \n",
       "1      0.000000      1    False     False       False         0  ...   \n",
       "2      0.000000      2    False     False       False         0  ...   \n",
       "3      0.125000      3    False     False       False         0  ...   \n",
       "4      0.133333      4    False     False       False         0  ...   \n",
       "...         ...    ...      ...       ...         ...       ...  ...   \n",
       "72995  0.000000  72995    False     False       False         0  ...   \n",
       "72996  0.125000  72996    False     False       False         0  ...   \n",
       "72997  0.000000  72997    False     False       False         0  ...   \n",
       "72998  0.000000  72998    False     False       False         0  ...   \n",
       "72999  0.000000  72999    False     False       False         1  ...   \n",
       "\n",
       "       subject5_rep2  subject6_rep0  subject6_rep1  subject6_rep2  \\\n",
       "0                  0              0              0              0   \n",
       "1                  0              0              0              0   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0          13720          22861          23023   \n",
       "...              ...            ...            ...            ...   \n",
       "72995              0              0              0              0   \n",
       "72996              0              0              0              0   \n",
       "72997              0              0              0              0   \n",
       "72998              0              0              0              0   \n",
       "72999              0              0              0              0   \n",
       "\n",
       "       subject7_rep0  subject7_rep1  subject7_rep2  subject8_rep0  \\\n",
       "0                  0              0              0              0   \n",
       "1              13985          14176          28603              0   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0              0              0              0   \n",
       "...              ...            ...            ...            ...   \n",
       "72995              0              0              0           6083   \n",
       "72996              0              0              0          10402   \n",
       "72997              0              0              0              0   \n",
       "72998           5585          11846          14495              0   \n",
       "72999              0              0              0              0   \n",
       "\n",
       "       subject8_rep1  subject8_rep2  \n",
       "0                  0              0  \n",
       "1                  0              0  \n",
       "2                  0              0  \n",
       "3                  0              0  \n",
       "4                  0              0  \n",
       "...              ...            ...  \n",
       "72995          11650          26531  \n",
       "72996          10434          10625  \n",
       "72997              0              0  \n",
       "72998              0              0  \n",
       "72999              0              0  \n",
       "\n",
       "[73000 rows x 41 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(nsd_stim_info)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2436a86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 768)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bceed992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.66s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.12s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coco2017_path = '../coco'\n",
    "\n",
    "annFile_train='{}/annotations/captions_{}.json'.format(coco2017_path,'train2017')\n",
    "coco_train=COCO(annFile_train)\n",
    "\n",
    "annFile_val='{}/annotations/captions_{}.json'.format(coco2017_path,'val2017')\n",
    "coco_val=COCO(annFile_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2e4584d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73000/73000 [13:21<00:00, 91.11it/s] \n"
     ]
    }
   ],
   "source": [
    "sent_encs = []\n",
    "for i in tqdm(range(len(df))):\n",
    "\n",
    "    imgId = df['cocoId'].iloc[i]\n",
    "    dataType = df['cocoSplit'].iloc[i]\n",
    "\n",
    "    if dataType == 'train2017':\n",
    "        annIds = coco_train.getAnnIds(imgIds=imgId)  #catIds=catIds, ,  iscrowd=None\n",
    "        anns = coco_train.loadAnns(annIds)\n",
    "\n",
    "    else:\n",
    "        annIds = coco_val.getAnnIds(imgIds=imgId)  #catIds=catIds, ,  iscrowd=None\n",
    "        anns = coco_val.loadAnns(annIds)\n",
    "\n",
    "    capts = []\n",
    "    for c in range(len(anns)):\n",
    "        capts.append(anns[c]['caption'])\n",
    "        \n",
    "    encs = model.encode(capts)\n",
    "        \n",
    "    sent_encs.append(np.mean(encs, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8fe88510",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.vstack(sent_encs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dbd82560",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = nsd_feats_dir + 'all-mpnet-base-v2_768.npy'\n",
    "np.save(file_name, features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf08d11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=100)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=100)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=100)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=100)\n",
    "pca.fit(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50dc722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_pca = pca.transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dfcbfa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = nsd_feats_dir + 'all-mpnet-base-v2_100pc.npy'\n",
    "np.save(file_name, features_train_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84a6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pca(feature_extractor, dataloader):\n",
    "\n",
    "    # Define PCA parameters\n",
    "    pca = IncrementalPCA(n_components=500, batch_size=batch_size)\n",
    "\n",
    "    # Fit PCA to batch\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d.to(device))\n",
    "        \n",
    "        \n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Fit PCA to batch\n",
    "        pca.partial_fit(ft.detach().cpu().numpy())\n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11ecab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_pca(feature_extractor, dataloader, pca):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d.to(device))\n",
    "        # Flatten the features\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Apply PCA transform\n",
    "        ft = pca.transform(ft.cpu().detach().numpy())\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py39]",
   "language": "python",
   "name": "conda-env-.conda-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
