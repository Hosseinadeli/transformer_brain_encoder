{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0f03b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/transformer_brain_encoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function datasets.nsd_utils.roi_maps(data_dir)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/')\n",
    "!pwd\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from models.activations import get_transformer_activations\n",
    "from collections import OrderedDict\n",
    "\n",
    "from datasets.nsd_utils import roi_maps\n",
    "from datasets.nsd import fetch_dataloaders\n",
    "\n",
    "roi_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a grid on the image to show different patches\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "image_path = train_img_dir + '/train-9039_nsd-66847.png'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "#img = Image.fromarray(img_with_dots)\n",
    "# new_size = (608, 912)\n",
    "# image = img.resize(new_size)\n",
    "image = np.array(image)\n",
    "\n",
    "print(image.shape)\n",
    "init_image_size = image.shape\n",
    "patch_size = 14\n",
    "\n",
    "\n",
    "# Padding the image with zeros to fit multiple of patch-size\n",
    "size_im = (\n",
    "    int(np.ceil(image.shape[0] / patch_size) * patch_size),\n",
    "    int(np.ceil(image.shape[1] / patch_size) * patch_size),\n",
    "    image.shape[2]\n",
    ")\n",
    "print(size_im)\n",
    "paded = np.zeros(size_im)\n",
    "paded[: image.shape[0], : image.shape[1], :] = image\n",
    "print(np.max(paded))\n",
    "image = Image.fromarray(np.uint8(paded))\n",
    "    \n",
    "\n",
    "# Draw some lines\n",
    "draw = ImageDraw.Draw(image)\n",
    "y_start = 0\n",
    "y_end = image.size[1]\n",
    "\n",
    "#h_featmap,w_featmap  (57, 38)\n",
    "\n",
    "step_count_h, step_count_w = 31, 31\n",
    "\n",
    "step_size_w = int(image.size[0] / step_count_w)\n",
    "step_size_h = int(image.size[1] / step_count_h)\n",
    "\n",
    "for x in range(0, image.size[0], step_size_w):\n",
    "    line = ((x, y_start), (x, y_end))\n",
    "    draw.line(line, fill=(251, 250,245))\n",
    "\n",
    "x_start = 0\n",
    "x_end = image.size[0]\n",
    "\n",
    "for y in range(0, image.size[1], step_size_h):\n",
    "    line = ((x_start, y), (x_end, y))\n",
    "    draw.line(line, fill=(251, 250,245))\n",
    "\n",
    "\n",
    "image.save('./results/image_with_grid_orig.png', dpi=(300, 300))\n",
    "plt.imshow(np.array(image))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py39]",
   "language": "python",
   "name": "conda-env-.conda-py39-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
