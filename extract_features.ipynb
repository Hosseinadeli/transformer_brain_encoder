{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08067d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/transformer_brain_encoder\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/')\n",
    "!pwd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "#data_dir = './algonauts_2023_challenge_data'\n",
    "\n",
    "device = 'cuda:1' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "device = torch.device(device)\n",
    "\n",
    "nsd_stimuli_dir = '/engram/nklab/datasets/natural_scene_dataset/nsddata_stimuli/stimuli/nsd/'\n",
    "\n",
    "\n",
    "# filename = nsd_stimuli_dir + 'nsd_stimuli.hdf5'\n",
    "\n",
    "# with h5py.File(filename, \"r\") as f:\n",
    "#     # Print all root level object names (aka keys) \n",
    "#     # these can be group or dataset names \n",
    "#     print(\"Keys: %s\" % f.keys())\n",
    "#     # get first object name/key; may or may NOT be a group\n",
    "#     a_group_key = list(f.keys())[0]\n",
    "\n",
    "#     # get the object type for a_group_key: usually group or dataset\n",
    "#     print(type(f[a_group_key])) \n",
    "\n",
    "#     # If a_group_key is a group name, \n",
    "#     # this gets the object names in the group and returns as a list\n",
    "#     data = list(f[a_group_key])\n",
    "\n",
    "#     # If a_group_key is a dataset name, \n",
    "#     # this gets the dataset values and returns as a list\n",
    "#     data = list(f[a_group_key])\n",
    "#     # preferred methods to get dataset values:\n",
    "#     ds_obj = f[a_group_key]      # returns as a h5py dataset object\n",
    "#     ds_arr = f[a_group_key][()]  # returns as a numpy array\n",
    "    \n",
    "    \n",
    "# print(ds_arr.shape)\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     transforms.Resize((425,425)), # resize the images to 224x24 pixels\n",
    "#     transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19623518",
   "metadata": {},
   "source": [
    "## DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f1b441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "arch = 'dinov2_vitb14'\n",
    "model = torch.hub.load('facebookresearch/dinov2', arch).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d77aeb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "patch_size = 14\n",
    "\n",
    "feat_out = {}\n",
    "def hook_fn_forward_qkv(module, input, output):\n",
    "    feat_out[\"qkv\"] = output\n",
    "\n",
    "# #for i in range(1,13):\n",
    "model._modules[\"blocks\"][-1]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "\n",
    "def aff_features(img):\n",
    "\n",
    "    size_im = (\n",
    "        img.shape[0],\n",
    "        img.shape[1],\n",
    "        int(np.ceil(img.shape[2] / patch_size) * patch_size),\n",
    "        int(np.ceil(img.shape[3] / patch_size) * patch_size),\n",
    "    )\n",
    "    paded = torch.zeros(size_im).to(device)\n",
    "    paded[:,:, : img.shape[2], : img.shape[3]] = img\n",
    "    img = paded\n",
    "\n",
    "    # Size for transformers\n",
    "    h_featmap = img.shape[-2] // patch_size\n",
    "    w_featmap = img.shape[-1] // patch_size\n",
    "\n",
    "\n",
    "    model._modules[\"blocks\"][-10]._modules[\"attn\"]._modules[\"qkv\"].register_forward_hook(hook_fn_forward_qkv)\n",
    "\n",
    "    which_features = 'q'\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Forward pass in the model\n",
    "        outputs = model.get_intermediate_layers(img)\n",
    "\n",
    "        # Scaling factor\n",
    "        scales = [patch_size, patch_size]\n",
    "\n",
    "        # Dimensions\n",
    "        nb_im = img.shape[0] #Batch size\n",
    "        nh = 12 #Number of heads\n",
    "        nb_tokens = h_featmap*w_featmap + 1\n",
    "\n",
    "        # Extract the qkv features of the last attention layer\n",
    "        qkv = feat_out[\"qkv\"].reshape(nb_im, nb_tokens, 3, nh, -1 // nh).permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k = k.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
    "        q = q.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
    "        v = v.transpose(1, 2).reshape(nb_im, nb_tokens, -1)\n",
    "\n",
    "        # Modality selection\n",
    "        if which_features == \"k\":\n",
    "            feats = k\n",
    "        elif which_features == \"q\":\n",
    "            feats = q\n",
    "        elif which_features == \"v\":\n",
    "            feats = v\n",
    "\n",
    "        cls_token = feats[0,0:1,:].cpu().numpy() \n",
    "        \n",
    "    #print(feats.flatten(1).dtype)\n",
    "    return feats.flatten(1).cpu().numpy() \n",
    "\n",
    "    #return cls_token[0]\n",
    "\n",
    "def extract_dino_features(dataloader):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = aff_features(d.to(device))\n",
    "        # Flatten the features\n",
    "        features.append(ft)\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00846cc1",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "arch = 'alexnet'\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', arch)\n",
    "model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "model.eval() # set the model to evaluation mode, since you are not training it\n",
    "\n",
    "train_nodes, _ = get_graph_node_names(model)\n",
    "print(train_nodes)\n",
    "\n",
    "#feature_type =  [\"features.2\"] # \"features.2\" #\"layer2.0.conv1\" # #@param [\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\", \"classifier.2\", \"classifier.5\", \"classifier.6\"] {allow-input: true}\n",
    "#'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', \n",
    "feature_type =  ['features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n",
    "#feature_type =  ['features.10', 'features.11', 'features.12', 'avgpool', 'flatten', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n",
    "\n",
    "feature_extractor = create_feature_extractor(model, return_nodes=feature_type)\n",
    "\n",
    "def extract_alexnet_features(dataloader):\n",
    "\n",
    "    features = []\n",
    "    for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        ft = feature_extractor(d.to(device))\n",
    "        ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "        # Flatten the features\n",
    "        features.append(ft.detach().cpu().numpy())\n",
    "    return np.vstack(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b0a050",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e5b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from models.resnet import resnet_model\n",
    "from utils.utils import (NestedTensor, nested_tensor_from_tensor_list)\n",
    "\n",
    "backbone_model = resnet_model('resnet50', train_backbone=False, return_interm_layers=False, dilation=False)\n",
    "backbone_model = backbone_model.to(device)\n",
    "\n",
    "def extract_resnet_features(dataloader):\n",
    "    features = []\n",
    "    for _, imgs in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        # Extract features\n",
    "        if isinstance(imgs, (list, torch.Tensor)):\n",
    "            imgs = tuple(imgs.to(device))\n",
    "            imgs = nested_tensor_from_tensor_list(imgs)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            backbone_features = backbone_model(imgs)\n",
    "\n",
    "        ft = backbone_features['0'].tensors\n",
    "        ft = torch.hstack([torch.flatten(ft, start_dim=1)])\n",
    "        # Flatten the features\n",
    "        features.append(ft.detach().cpu().numpy())\n",
    "    return np.vstack(features)\n",
    "\n",
    "\n",
    "# from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "# arch = 'resnet50'\n",
    "\n",
    "# model = torch.hub.load('pytorch/vision:v0.10.0', arch)\n",
    "# model.to(device) # send the model to the chosen device ('cpu' or 'cuda')\n",
    "# model.eval() # set the model to evaluation mode, since you are not training it\n",
    "\n",
    "# train_nodes, _ = get_graph_node_names(model)\n",
    "# print(train_nodes)\n",
    "\n",
    "# feature_type =  ['layer4.2.relu_2'] # \"features.2\" #\"layer2.0.conv1\" # #@param [\"features.2\", \"features.5\", \"features.7\", \"features.9\", \"features.12\", \"classifier.2\", \"classifier.5\", \"classifier.6\"] {allow-input: true}\n",
    "\n",
    "# feature_extractor = create_feature_extractor(model, return_nodes=feature_type)\n",
    "\n",
    "# def extract_resnet_features(dataloader):\n",
    "\n",
    "#     features = []\n",
    "#     for _, d in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "#         # Extract features\n",
    "#         ft = feature_extractor(d.to(device))\n",
    "#         print(ft['layer4.2.relu_2'].shape)\n",
    "#         ft = torch.hstack([torch.flatten(l, start_dim=1) for l in ft.values()])\n",
    "#         # Flatten the features\n",
    "#         features.append(ft.detach().cpu().numpy())\n",
    "#     return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e7da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "561e4aa9",
   "metadata": {},
   "source": [
    "## extract and save features - save the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c661f20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308/308 [06:02<00:00,  1.18s/it]\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "2\n",
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308/308 [06:16<00:00,  1.22s/it]\n",
      "100%|██████████| 5/5 [00:05<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "3\n",
      "Training images: 9082\n",
      "Test images: 215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [05:58<00:00,  1.26s/it]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "4\n",
      "Training images: 8779\n",
      "Test images: 395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [05:50<00:00,  1.28s/it]\n",
      "100%|██████████| 13/13 [00:14<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "5\n",
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308/308 [15:15<00:00,  2.97s/it]\n",
      "100%|██████████| 5/5 [00:14<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "6\n",
      "Training images: 9082\n",
      "Test images: 293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 284/284 [09:29<00:00,  2.00s/it]\n",
      "100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "7\n",
      "Training images: 9841\n",
      "Test images: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308/308 [10:34<00:00,  2.06s/it]\n",
      "100%|██████████| 5/5 [00:11<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "8\n",
      "Training images: 8779\n",
      "Test images: 395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275/275 [13:25<00:00,  2.93s/it]\n",
      "100%|██████████| 13/13 [01:15<00:00,  5.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import IncrementalPCA, PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr as corr\n",
    "\n",
    "\n",
    "data_dir = '/engram/nklab/algonauts/algonauts_2023_challenge_data'\n",
    "\n",
    "# device = 'cuda' #@param ['cpu', 'cuda'] {allow-input: true}\n",
    "# device = torch.device(device)\n",
    "\n",
    "feature_name = 'resnet50'\n",
    "image_size = 975\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size,image_size)), # resize the images to 224x24 pixels\n",
    "    transforms.ToTensor(), # convert the images to a PyTorch tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalize the images color channels\n",
    "])\n",
    "\n",
    "for subj in range(1,9): #@param [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\"] {type:\"raw\", allow-input: true}\n",
    "\n",
    "    print(subj)\n",
    "    class argObj:\n",
    "        def __init__(self, data_dir, subj):\n",
    "\n",
    "            self.subj = format(subj, '02')\n",
    "            self.data_dir = os.path.join(data_dir, 'subj'+self.subj)\n",
    "\n",
    "    args = argObj(data_dir, subj)\n",
    "\n",
    "    feature_dir = './saved_image_features/'\n",
    "\n",
    "    subject_feature_dir =  os.path.join(feature_dir, feature_name,format(subj, '02'))\n",
    "\n",
    "    if not os.path.isdir(subject_feature_dir):\n",
    "        os.makedirs(subject_feature_dir)\n",
    "\n",
    "\n",
    "    train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "    test_img_dir  = os.path.join(args.data_dir, 'test_split', 'test_images')\n",
    "    #test_img_dir = os.path.join(args.data_dir, '../nsdsynthetic_stimuli/')\n",
    "\n",
    "    # Create lists will all training and test image file names, sorted\n",
    "    train_img_list = os.listdir(train_img_dir)\n",
    "    train_img_list = [f for f in train_img_list if f.endswith('.png')]\n",
    "    train_img_list.sort()\n",
    "\n",
    "    train_imgs_paths = list(Path(train_img_dir).iterdir())\n",
    "    train_imgs_paths = [f for f in train_imgs_paths if str(f).endswith('.png')]\n",
    "    train_imgs_paths = sorted(train_imgs_paths)\n",
    "\n",
    "    test_img_list = os.listdir(test_img_dir)\n",
    "    test_img_list = [f for f in test_img_list if f.endswith('.png')]\n",
    "    test_img_list.sort()\n",
    "\n",
    "    test_imgs_paths = list(Path(test_img_dir).iterdir())\n",
    "    test_imgs_paths = [f for f in test_imgs_paths if str(f).endswith('.png')]\n",
    "    test_imgs_paths = sorted(test_imgs_paths)\n",
    "\n",
    "    # Create lists with all training and test image file names, sorted\n",
    "    # train_img_list = os.listdir(train_img_dir)\n",
    "    # train_img_list.sort()\n",
    "    # test_img_list = os.listdir(test_img_dir)\n",
    "    # test_img_list.sort()\n",
    "    print('Training images: ' + str(len(train_img_list)))\n",
    "    print('Test images: ' + str(len(test_img_list)))\n",
    "\n",
    "    idxs_train = np.arange(len(train_img_list))\n",
    "    idxs_test = np.arange(len(test_img_list))\n",
    "\n",
    "\n",
    "    class ImageDataset(Dataset):\n",
    "        def __init__(self, imgs_paths, idxs, transform):\n",
    "            self.imgs_paths = np.array(imgs_paths)[idxs]\n",
    "            self.transform = transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.imgs_paths)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            # Load the image\n",
    "            img_path = self.imgs_paths[idx]\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            # Preprocess the image and send it to the chosen device ('cpu' or 'cuda')\n",
    "            if self.transform:\n",
    "                img = self.transform(img).to(device)\n",
    "            return img\n",
    "\n",
    "\n",
    "    batch_size = 32 #@param\n",
    "    # Get the paths of all image files\n",
    "    # train_imgs_paths = sorted(list(Path(train_img_dir).iterdir()))\n",
    "    # test_imgs_paths = sorted(list(Path(test_img_dir).iterdir()))\n",
    "\n",
    "    # The DataLoaders contain the ImageDataset class\n",
    "    train_imgs_dataloader = DataLoader(\n",
    "        ImageDataset(train_imgs_paths, idxs_train, transform), \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    test_imgs_dataloader = DataLoader(\n",
    "        ImageDataset(test_imgs_paths, idxs_test, transform), \n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    if 'alexnet' in feature_name:\n",
    "        features_train = extract_alexnet_features(train_imgs_dataloader)\n",
    "        features_test = extract_alexnet_features(test_imgs_dataloader)\n",
    "    elif 'dino' in feature_name:\n",
    "        features_train = extract_dino_features(train_imgs_dataloader)\n",
    "        features_test = extract_dino_features(test_imgs_dataloader)\n",
    "    elif 'resnet' in feature_name:\n",
    "        features_train = extract_resnet_features(train_imgs_dataloader)\n",
    "        features_test = extract_resnet_features(test_imgs_dataloader)\n",
    "\n",
    "    \n",
    "    # np.save(subject_feature_dir + '/train.npy', features_train)\n",
    "    # np.save(subject_feature_dir + '/test.npy', features_test)\n",
    "\n",
    "    for run in range(1,11):\n",
    "        print(run)\n",
    "        save_dir = subject_feature_dir + '/pca_run' + str(run)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        # pca = fit_pca(feature_extractor, train_imgs_dataloader)\n",
    "        # features_train = extract_features(feature_extractor, train_imgs_dataloader, pca)\n",
    "\n",
    "        num_train = int(np.round(len(features_train) / 100 * 90))\n",
    "        # Shuffle all training stimulus images\n",
    "        idxs = np.arange(len(features_train))\n",
    "\n",
    "        np.random.shuffle(idxs)\n",
    "        np.save(save_dir+ '/idxs.npy', idxs)\n",
    "        \n",
    "        # Assign 90% of the shuffled stimulus images to the training partition,\n",
    "        # and 10% to the test partition\n",
    "        idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "\n",
    "        features_train_run = features_train[idxs_train]\n",
    "        features_val_run = features_train[idxs_val]\n",
    "\n",
    "        pca = PCA(n_components=768)\n",
    "        pca.fit(features_train_run)\n",
    "        features_train_pca = pca.transform(features_train_run)\n",
    "        features_val_pca = pca.transform(features_val_run)\n",
    "        features_test_pca = pca.transform(features_test)\n",
    "\n",
    "        np.save(save_dir + '/train.npy', features_train_pca)\n",
    "        np.save(save_dir + '/val.npy', features_val_pca)\n",
    "        np.save(save_dir + '/test.npy', features_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7a545",
   "metadata": {},
   "source": [
    "## extract and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29ad45d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_image_features/dinov2_q_last/03/train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m feature_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./saved_image_features/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m subject_feature_dir \u001b[38;5;241m=\u001b[39m  os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(feature_dir, feature_name,\u001b[38;5;28mformat\u001b[39m(subj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m features_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubject_feature_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/train.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m features_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(subject_feature_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/test.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m run \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m11\u001b[39m):\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved_image_features/dinov2_q_last/03/train.npy'"
     ]
    }
   ],
   "source": [
    "for subj in range(3,4):\n",
    "    print(subj )\n",
    "    feature_name = 'dinov2_q_last'\n",
    "\n",
    "    feature_dir = './saved_image_features/'\n",
    "    subject_feature_dir =  os.path.join(feature_dir, feature_name,format(subj, '02'))\n",
    "\n",
    "    features_train = np.load(subject_feature_dir+'/train.npy')\n",
    "    features_test = np.load(subject_feature_dir+'/test.npy')\n",
    "\n",
    "    for run in range(1,11):\n",
    "        print(run)\n",
    "        save_dir = subject_feature_dir + '/pca_run' + str(run)\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        \n",
    "        num_train = int(np.round(len(features_train) / 100 * 90))\n",
    "        # Shuffle all training stimulus images\n",
    "        idxs = np.arange(len(features_train))\n",
    "\n",
    "        np.random.shuffle(idxs)\n",
    "        np.save(save_dir+ '/idxs.npy', idxs)\n",
    "        \n",
    "        # Assign 90% of the shuffled stimulus images to the training partition,\n",
    "        # and 10% to the test partition\n",
    "        idxs_train, idxs_val = idxs[:num_train], idxs[num_train:]\n",
    "\n",
    "        features_train_run = features_train[idxs_train]\n",
    "        features_val_run = features_train[idxs_val]\n",
    "\n",
    "        pca = PCA(n_components=768)\n",
    "        pca.fit(features_train_run)\n",
    "        features_train_pca = pca.transform(features_train_run)\n",
    "        features_val_pca = pca.transform(features_val_run)\n",
    "        features_test_pca = pca.transform(features_test)\n",
    "\n",
    "        np.save(save_dir + '/train.npy', features_train_pca)\n",
    "        np.save(save_dir + '/val.npy', features_val_pca)\n",
    "        np.save(save_dir + '/test.npy', features_test_pca)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py39]",
   "language": "python",
   "name": "conda-env-.conda-py39-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
