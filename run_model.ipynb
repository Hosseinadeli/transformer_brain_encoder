{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/transformer_brain_encoder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#switch to the directory where the code is\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training stimulus images: 8857\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 220\n",
      "Using cache found in /home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "xFormers not available\n",
      "xFormers not available\n",
      "Number of model parameters: 67140028\n",
      "brain_encoder(\n",
      "  (transformer): Transformer(\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (memory_proj): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (query_embed): Embedding(39548, 768)\n",
      "  (backbone_model): Joiner(\n",
      "    (0): dino_model_with_hooks(\n",
      "      (backbone): DinoVisionTransformer(\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (blocks): ModuleList(\n",
      "          (0-11): 12 x NestedTensorBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): MemEffAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): LayerScale()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): LayerScale()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (head): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      "  (lh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=19004, bias=True)\n",
      "  )\n",
      "  (rh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=20544, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'transformer.memory_proj.weight', 'transformer.memory_proj.bias', 'query_embed.weight', 'lh_embed.0.weight', 'lh_embed.0.bias', 'rh_embed.0.weight', 'rh_embed.0.bias']\n",
      "Start training\n",
      "Epoch: [0]  [   0/4429]  eta: 1:28:16  lr: 0.000100  loss_labels: 1.6783 (1.6783)  loss: 1.6783 (1.6783)  time: 1.1959\n",
      "Epoch: [0]  [ 100/4429]  eta: 0:41:23  lr: 0.000100  loss_labels: 1.1470 (1.1684)  loss: 1.0377 (1.1684)  time: 0.5798\n",
      "Epoch: [0]  [ 200/4429]  eta: 0:40:45  lr: 0.000100  loss_labels: 1.0025 (1.1241)  loss: 1.0685 (1.1241)  time: 0.5845\n",
      "Epoch: [0]  [ 300/4429]  eta: 0:39:54  lr: 0.000100  loss_labels: 0.9682 (1.0838)  loss: 1.0245 (1.0838)  time: 0.5823\n",
      "Epoch: [0]  [ 400/4429]  eta: 0:38:59  lr: 0.000100  loss_labels: 0.9007 (1.0509)  loss: 0.8895 (1.0509)  time: 0.5840\n",
      "Epoch: [0]  [ 500/4429]  eta: 0:38:04  lr: 0.000100  loss_labels: 0.9350 (1.0360)  loss: 0.9381 (1.0360)  time: 0.5830\n",
      "Epoch: [0]  [ 600/4429]  eta: 0:37:07  lr: 0.000100  loss_labels: 0.9016 (1.0241)  loss: 0.8865 (1.0241)  time: 0.5837\n",
      "Epoch: [0]  [ 700/4429]  eta: 0:36:09  lr: 0.000100  loss_labels: 0.8887 (1.0124)  loss: 0.8372 (1.0124)  time: 0.5846\n",
      "Epoch: [0]  [ 800/4429]  eta: 0:35:12  lr: 0.000100  loss_labels: 0.8612 (1.0016)  loss: 0.8018 (1.0016)  time: 0.5839\n",
      "Epoch: [0]  [ 900/4429]  eta: 0:34:14  lr: 0.000100  loss_labels: 0.8881 (0.9968)  loss: 0.9039 (0.9968)  time: 0.5819\n",
      "Epoch: [0]  [1000/4429]  eta: 0:33:16  lr: 0.000100  loss_labels: 0.8848 (0.9867)  loss: 0.9039 (0.9867)  time: 0.5831\n",
      "Epoch: [0]  [1100/4429]  eta: 0:32:17  lr: 0.000100  loss_labels: 0.8635 (0.9789)  loss: 0.9091 (0.9789)  time: 0.5851\n",
      "Epoch: [0]  [1200/4429]  eta: 0:31:19  lr: 0.000100  loss_labels: 0.8658 (0.9727)  loss: 0.9188 (0.9727)  time: 0.5834\n",
      "Epoch: [0]  [1300/4429]  eta: 0:30:21  lr: 0.000100  loss_labels: 0.8475 (0.9681)  loss: 0.9287 (0.9681)  time: 0.5818\n",
      "Epoch: [0]  [1400/4429]  eta: 0:29:24  lr: 0.000100  loss_labels: 0.8828 (0.9652)  loss: 0.8874 (0.9652)  time: 0.5867\n",
      "Epoch: [0]  [1500/4429]  eta: 0:28:26  lr: 0.000100  loss_labels: 0.8303 (0.9597)  loss: 0.8242 (0.9597)  time: 0.5841\n",
      "Epoch: [0]  [1600/4429]  eta: 0:27:28  lr: 0.000100  loss_labels: 0.8412 (0.9549)  loss: 0.8702 (0.9549)  time: 0.5823\n",
      "Epoch: [0]  [1700/4429]  eta: 0:26:30  lr: 0.000100  loss_labels: 0.8277 (0.9509)  loss: 0.8245 (0.9509)  time: 0.5870\n",
      "Epoch: [0]  [1800/4429]  eta: 0:25:32  lr: 0.000100  loss_labels: 0.8257 (0.9447)  loss: 0.8735 (0.9447)  time: 0.5840\n",
      "Epoch: [0]  [1900/4429]  eta: 0:24:33  lr: 0.000100  loss_labels: 0.8168 (0.9409)  loss: 0.8191 (0.9409)  time: 0.5807\n",
      "Epoch: [0]  [2000/4429]  eta: 0:23:35  lr: 0.000100  loss_labels: 0.8096 (0.9374)  loss: 0.7549 (0.9374)  time: 0.5843\n",
      "Epoch: [0]  [2100/4429]  eta: 0:22:37  lr: 0.000100  loss_labels: 0.8061 (0.9334)  loss: 0.8218 (0.9334)  time: 0.5845\n",
      "Epoch: [0]  [2200/4429]  eta: 0:21:39  lr: 0.000100  loss_labels: 0.8409 (0.9302)  loss: 0.8203 (0.9302)  time: 0.5856\n",
      "Epoch: [0]  [2300/4429]  eta: 0:20:40  lr: 0.000100  loss_labels: 0.7910 (0.9270)  loss: 0.7328 (0.9270)  time: 0.5798\n",
      "Epoch: [0]  [2400/4429]  eta: 0:19:42  lr: 0.000100  loss_labels: 0.8253 (0.9253)  loss: 0.8253 (0.9253)  time: 0.5828\n",
      "Epoch: [0]  [2500/4429]  eta: 0:18:44  lr: 0.000100  loss_labels: 0.8125 (0.9232)  loss: 0.7795 (0.9232)  time: 0.5843\n",
      "Epoch: [0]  [2600/4429]  eta: 0:17:46  lr: 0.000100  loss_labels: 0.8344 (0.9219)  loss: 0.7559 (0.9219)  time: 0.5842\n",
      "Epoch: [0]  [2700/4429]  eta: 0:16:47  lr: 0.000100  loss_labels: 0.8465 (0.9202)  loss: 0.8833 (0.9202)  time: 0.5820\n",
      "Epoch: [0]  [2800/4429]  eta: 0:15:49  lr: 0.000100  loss_labels: 0.8050 (0.9182)  loss: 0.7962 (0.9182)  time: 0.5838\n",
      "Epoch: [0]  [2900/4429]  eta: 0:14:51  lr: 0.000100  loss_labels: 0.7743 (0.9158)  loss: 0.7743 (0.9158)  time: 0.5851\n",
      "Epoch: [0]  [3000/4429]  eta: 0:13:52  lr: 0.000100  loss_labels: 0.8062 (0.9146)  loss: 0.8066 (0.9146)  time: 0.5835\n",
      "Epoch: [0]  [3100/4429]  eta: 0:12:54  lr: 0.000100  loss_labels: 0.7881 (0.9128)  loss: 0.8923 (0.9128)  time: 0.5834\n",
      "Epoch: [0]  [3200/4429]  eta: 0:11:56  lr: 0.000100  loss_labels: 0.7779 (0.9106)  loss: 0.7304 (0.9106)  time: 0.5863\n",
      "Epoch: [0]  [3300/4429]  eta: 0:10:58  lr: 0.000100  loss_labels: 0.7770 (0.9086)  loss: 0.8237 (0.9086)  time: 0.5843\n",
      "Epoch: [0]  [3400/4429]  eta: 0:09:59  lr: 0.000100  loss_labels: 0.7927 (0.9069)  loss: 0.7442 (0.9069)  time: 0.5830\n",
      "Epoch: [0]  [3500/4429]  eta: 0:09:01  lr: 0.000100  loss_labels: 0.8042 (0.9059)  loss: 0.7421 (0.9059)  time: 0.5809\n",
      "Epoch: [0]  [3600/4429]  eta: 0:08:03  lr: 0.000100  loss_labels: 0.7775 (0.9042)  loss: 0.7544 (0.9042)  time: 0.5832\n",
      "Epoch: [0]  [3700/4429]  eta: 0:07:04  lr: 0.000100  loss_labels: 0.7637 (0.9017)  loss: 0.7734 (0.9017)  time: 0.5825\n",
      "Epoch: [0]  [3800/4429]  eta: 0:06:06  lr: 0.000100  loss_labels: 0.7572 (0.8991)  loss: 0.7546 (0.8991)  time: 0.5834\n",
      "Epoch: [0]  [3900/4429]  eta: 0:05:08  lr: 0.000100  loss_labels: 0.7493 (0.8966)  loss: 0.7532 (0.8966)  time: 0.5873\n",
      "Epoch: [0]  [4000/4429]  eta: 0:04:10  lr: 0.000100  loss_labels: 0.7433 (0.8941)  loss: 0.7421 (0.8941)  time: 0.5835\n",
      "Epoch: [0]  [4100/4429]  eta: 0:03:11  lr: 0.000100  loss_labels: 0.7496 (0.8923)  loss: 0.7159 (0.8923)  time: 0.5859\n",
      "Epoch: [0]  [4200/4429]  eta: 0:02:13  lr: 0.000100  loss_labels: 0.7487 (0.8898)  loss: 0.7267 (0.8898)  time: 0.5841\n",
      "Epoch: [0]  [4300/4429]  eta: 0:01:15  lr: 0.000100  loss_labels: 0.7604 (0.8875)  loss: 0.8320 (0.8875)  time: 0.5834\n",
      "Epoch: [0]  [4400/4429]  eta: 0:00:16  lr: 0.000100  loss_labels: 0.7529 (0.8867)  loss: 0.7055 (0.8867)  time: 0.5823\n",
      "Epoch: [0]  [4428/4429]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.7505 (0.8862)  loss: 0.7505 (0.8862)  time: 0.5792\n",
      "Epoch: [0] Total time: 0:43:02 (0.5831 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.7505 (0.8862)  loss: 0.7505 (0.8862)\n",
      "Test:  [  0/492]  eta: 0:03:24  loss_labels: 0.6875 (0.6875)  loss: 0.6875 (0.6875)  time: 0.4160\n",
      "Test:  [100/492]  eta: 0:01:43  loss_labels: 0.7338 (0.7711)  loss: 0.6968 (0.7711)  time: 0.2611\n",
      "Test:  [200/492]  eta: 0:01:16  loss_labels: 0.7494 (0.7857)  loss: 0.7118 (0.7857)  time: 0.2625\n",
      "Test:  [300/492]  eta: 0:00:50  loss_labels: 0.7176 (0.7854)  loss: 0.6384 (0.7854)  time: 0.2648\n",
      "Test:  [400/492]  eta: 0:00:24  loss_labels: 0.7011 (0.7812)  loss: 0.6778 (0.7812)  time: 0.2630\n",
      "Test:  [491/492]  eta: 0:00:00  loss_labels: 0.7413 (0.7811)  loss: 0.7148 (0.7811)  time: 0.2623\n",
      "Test: Total time: 0:02:09 (0.2626 s / it)\n",
      "Averaged stats: loss_labels: 0.7413 (0.7811)  loss: 0.7148 (0.7811)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1609.50it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1604.13it/s]\n",
      "val_perf: 0.43016473326072835\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████| 110/110 [00:49<00:00,  2.23it/s]\n",
      "Epoch: [1]  [   0/4429]  eta: 0:20:53  lr: 0.000100  loss_labels: 0.7147 (0.7147)  loss: 0.7147 (0.7147)  time: 0.2829\n",
      "Epoch: [1]  [ 100/4429]  eta: 0:41:32  lr: 0.000100  loss_labels: 0.7521 (0.8013)  loss: 0.7579 (0.8013)  time: 0.5823\n",
      "Epoch: [1]  [ 200/4429]  eta: 0:40:45  lr: 0.000100  loss_labels: 0.7351 (0.7894)  loss: 0.7088 (0.7894)  time: 0.5807\n",
      "Epoch: [1]  [ 300/4429]  eta: 0:39:48  lr: 0.000100  loss_labels: 0.7383 (0.7868)  loss: 0.7011 (0.7868)  time: 0.5803\n",
      "Epoch: [1]  [ 400/4429]  eta: 0:38:51  lr: 0.000100  loss_labels: 0.7315 (0.7827)  loss: 0.6916 (0.7827)  time: 0.5801\n",
      "Epoch: [1]  [ 500/4429]  eta: 0:37:54  lr: 0.000100  loss_labels: 0.7456 (0.7819)  loss: 0.7299 (0.7819)  time: 0.5822\n",
      "Epoch: [1]  [ 600/4429]  eta: 0:36:58  lr: 0.000100  loss_labels: 0.7161 (0.7840)  loss: 0.7134 (0.7840)  time: 0.5807\n",
      "Epoch: [1]  [ 700/4429]  eta: 0:36:00  lr: 0.000100  loss_labels: 0.7485 (0.7885)  loss: 0.7992 (0.7885)  time: 0.5783\n",
      "Epoch: [1]  [ 800/4429]  eta: 0:35:04  lr: 0.000100  loss_labels: 0.7510 (0.7925)  loss: 0.7510 (0.7925)  time: 0.5807\n",
      "Epoch: [1]  [ 900/4429]  eta: 0:34:06  lr: 0.000100  loss_labels: 0.7274 (0.7914)  loss: 0.6944 (0.7914)  time: 0.5821\n",
      "Epoch: [1]  [1000/4429]  eta: 0:33:08  lr: 0.000100  loss_labels: 0.7355 (0.7893)  loss: 0.7704 (0.7893)  time: 0.5808\n",
      "Epoch: [1]  [1100/4429]  eta: 0:32:11  lr: 0.000100  loss_labels: 0.7265 (0.7866)  loss: 0.7000 (0.7866)  time: 0.5807\n",
      "Epoch: [1]  [1200/4429]  eta: 0:31:13  lr: 0.000100  loss_labels: 0.7251 (0.7848)  loss: 0.7510 (0.7848)  time: 0.5805\n",
      "Epoch: [1]  [1300/4429]  eta: 0:30:14  lr: 0.000100  loss_labels: 0.7507 (0.7868)  loss: 0.7418 (0.7868)  time: 0.5777\n",
      "Epoch: [1]  [1400/4429]  eta: 0:29:16  lr: 0.000100  loss_labels: 0.7497 (0.7896)  loss: 0.7343 (0.7896)  time: 0.5774\n",
      "Epoch: [1]  [1500/4429]  eta: 0:28:18  lr: 0.000100  loss_labels: 0.7178 (0.7897)  loss: 0.7276 (0.7897)  time: 0.5805\n",
      "Epoch: [1]  [1600/4429]  eta: 0:27:20  lr: 0.000100  loss_labels: 0.7220 (0.7886)  loss: 0.6991 (0.7886)  time: 0.5783\n",
      "Epoch: [1]  [1700/4429]  eta: 0:26:22  lr: 0.000100  loss_labels: 0.7626 (0.7889)  loss: 0.7928 (0.7889)  time: 0.5805\n",
      "Epoch: [1]  [1800/4429]  eta: 0:25:24  lr: 0.000100  loss_labels: 0.7430 (0.7893)  loss: 0.7646 (0.7893)  time: 0.5783\n",
      "Epoch: [1]  [1900/4429]  eta: 0:24:26  lr: 0.000100  loss_labels: 0.7191 (0.7871)  loss: 0.6936 (0.7871)  time: 0.5806\n",
      "Epoch: [1]  [2000/4429]  eta: 0:23:28  lr: 0.000100  loss_labels: 0.7468 (0.7875)  loss: 0.6933 (0.7875)  time: 0.5836\n",
      "Epoch: [1]  [2100/4429]  eta: 0:22:30  lr: 0.000100  loss_labels: 0.7162 (0.7860)  loss: 0.6375 (0.7860)  time: 0.5773\n",
      "Epoch: [1]  [2200/4429]  eta: 0:21:32  lr: 0.000100  loss_labels: 0.7249 (0.7855)  loss: 0.6888 (0.7855)  time: 0.5808\n",
      "Epoch: [1]  [2300/4429]  eta: 0:20:34  lr: 0.000100  loss_labels: 0.7325 (0.7853)  loss: 0.7147 (0.7853)  time: 0.5791\n",
      "Epoch: [1]  [2400/4429]  eta: 0:19:36  lr: 0.000100  loss_labels: 0.7608 (0.7848)  loss: 0.7458 (0.7848)  time: 0.5819\n",
      "Epoch: [1]  [2500/4429]  eta: 0:18:38  lr: 0.000100  loss_labels: 0.6955 (0.7842)  loss: 0.6474 (0.7842)  time: 0.5807\n",
      "Epoch: [1]  [2600/4429]  eta: 0:17:40  lr: 0.000100  loss_labels: 0.7200 (0.7835)  loss: 0.7231 (0.7835)  time: 0.5807\n",
      "Epoch: [1]  [2700/4429]  eta: 0:16:42  lr: 0.000100  loss_labels: 0.7165 (0.7819)  loss: 0.6885 (0.7819)  time: 0.5818\n",
      "Epoch: [1]  [2800/4429]  eta: 0:15:44  lr: 0.000100  loss_labels: 0.7455 (0.7819)  loss: 0.7822 (0.7819)  time: 0.5804\n",
      "Epoch: [1]  [2900/4429]  eta: 0:14:46  lr: 0.000100  loss_labels: 0.7460 (0.7818)  loss: 0.7627 (0.7818)  time: 0.5780\n",
      "Epoch: [1]  [3000/4429]  eta: 0:13:48  lr: 0.000100  loss_labels: 0.6923 (0.7804)  loss: 0.6891 (0.7804)  time: 0.5778\n",
      "Epoch: [1]  [3100/4429]  eta: 0:12:50  lr: 0.000100  loss_labels: 0.7000 (0.7797)  loss: 0.7358 (0.7797)  time: 0.5782\n",
      "Epoch: [1]  [3200/4429]  eta: 0:11:52  lr: 0.000100  loss_labels: 0.7676 (0.7807)  loss: 0.7439 (0.7807)  time: 0.5809\n",
      "Epoch: [1]  [3300/4429]  eta: 0:10:54  lr: 0.000100  loss_labels: 0.7254 (0.7825)  loss: 0.7417 (0.7825)  time: 0.5819\n",
      "Epoch: [1]  [3400/4429]  eta: 0:09:56  lr: 0.000100  loss_labels: 0.6975 (0.7813)  loss: 0.6780 (0.7813)  time: 0.5814\n",
      "Epoch: [1]  [3500/4429]  eta: 0:08:58  lr: 0.000100  loss_labels: 0.7020 (0.7813)  loss: 0.7323 (0.7813)  time: 0.5821\n",
      "Epoch: [1]  [3600/4429]  eta: 0:08:00  lr: 0.000100  loss_labels: 0.6874 (0.7806)  loss: 0.6815 (0.7806)  time: 0.5819\n",
      "Epoch: [1]  [3700/4429]  eta: 0:07:02  lr: 0.000100  loss_labels: 0.6974 (0.7802)  loss: 0.6510 (0.7802)  time: 0.5788\n",
      "Epoch: [1]  [3800/4429]  eta: 0:06:04  lr: 0.000100  loss_labels: 0.7109 (0.7792)  loss: 0.6880 (0.7792)  time: 0.5780\n",
      "Epoch: [1]  [3900/4429]  eta: 0:05:06  lr: 0.000100  loss_labels: 0.6904 (0.7783)  loss: 0.6794 (0.7783)  time: 0.5810\n",
      "Epoch: [1]  [4000/4429]  eta: 0:04:08  lr: 0.000100  loss_labels: 0.7078 (0.7774)  loss: 0.7202 (0.7774)  time: 0.5802\n",
      "Epoch: [1]  [4100/4429]  eta: 0:03:10  lr: 0.000100  loss_labels: 0.7067 (0.7773)  loss: 0.6682 (0.7773)  time: 0.5830\n",
      "Epoch: [1]  [4200/4429]  eta: 0:02:12  lr: 0.000100  loss_labels: 0.7279 (0.7771)  loss: 0.7279 (0.7771)  time: 0.5786\n",
      "Epoch: [1]  [4300/4429]  eta: 0:01:14  lr: 0.000100  loss_labels: 0.7041 (0.7767)  loss: 0.7115 (0.7767)  time: 0.5808\n",
      "Epoch: [1]  [4400/4429]  eta: 0:00:16  lr: 0.000100  loss_labels: 0.7251 (0.7767)  loss: 0.6999 (0.7767)  time: 0.5808\n",
      "Epoch: [1]  [4428/4429]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.7079 (0.7766)  loss: 0.7221 (0.7766)  time: 0.5739\n",
      "Epoch: [1] Total time: 0:42:49 (0.5800 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.7079 (0.7766)  loss: 0.7221 (0.7766)\n",
      "Test:  [  0/492]  eta: 0:03:23  loss_labels: 0.6919 (0.6919)  loss: 0.6919 (0.6919)  time: 0.4142\n",
      "Test:  [100/492]  eta: 0:01:43  loss_labels: 0.6945 (0.7307)  loss: 0.6432 (0.7307)  time: 0.2636\n",
      "Test:  [200/492]  eta: 0:01:16  loss_labels: 0.7016 (0.7423)  loss: 0.6852 (0.7423)  time: 0.2626\n",
      "Test:  [300/492]  eta: 0:00:50  loss_labels: 0.6886 (0.7447)  loss: 0.6093 (0.7447)  time: 0.2624\n",
      "Test:  [400/492]  eta: 0:00:24  loss_labels: 0.6819 (0.7423)  loss: 0.6290 (0.7423)  time: 0.2625\n",
      "Test:  [491/492]  eta: 0:00:00  loss_labels: 0.6988 (0.7426)  loss: 0.6905 (0.7426)  time: 0.2625\n",
      "Test: Total time: 0:02:09 (0.2625 s / it)\n",
      "Averaged stats: loss_labels: 0.6988 (0.7426)  loss: 0.6905 (0.7426)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:12<00:00, 1579.32it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:13<00:00, 1558.99it/s]\n",
      "val_perf: 0.45944235593404714\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████| 110/110 [00:48<00:00,  2.27it/s]\n",
      "Epoch: [2]  [   0/4429]  eta: 0:19:43  lr: 0.000100  loss_labels: 0.5613 (0.5613)  loss: 0.5613 (0.5613)  time: 0.2673\n",
      "Epoch: [2]  [ 100/4429]  eta: 0:41:23  lr: 0.000100  loss_labels: 0.6913 (0.7845)  loss: 0.6758 (0.7845)  time: 0.5765\n",
      "Epoch: [2]  [ 200/4429]  eta: 0:40:39  lr: 0.000100  loss_labels: 0.7125 (0.7658)  loss: 0.6752 (0.7658)  time: 0.5789\n",
      "Epoch: [2]  [ 300/4429]  eta: 0:39:45  lr: 0.000100  loss_labels: 0.7225 (0.7606)  loss: 0.7563 (0.7606)  time: 0.5785\n",
      "Epoch: [2]  [ 400/4429]  eta: 0:38:50  lr: 0.000100  loss_labels: 0.7163 (0.7624)  loss: 0.7767 (0.7624)  time: 0.5843\n",
      "Epoch: [2]  [ 500/4429]  eta: 0:37:54  lr: 0.000100  loss_labels: 0.7700 (0.7759)  loss: 0.7910 (0.7759)  time: 0.5792\n",
      "Epoch: [2]  [ 600/4429]  eta: 0:36:57  lr: 0.000100  loss_labels: 0.7040 (0.7710)  loss: 0.6338 (0.7710)  time: 0.5813\n",
      "Epoch: [2]  [ 700/4429]  eta: 0:35:59  lr: 0.000100  loss_labels: 0.6853 (0.7665)  loss: 0.7064 (0.7665)  time: 0.5768\n",
      "Epoch: [2]  [ 800/4429]  eta: 0:35:02  lr: 0.000100  loss_labels: 0.7024 (0.7671)  loss: 0.7514 (0.7671)  time: 0.5787\n",
      "Epoch: [2]  [ 900/4429]  eta: 0:34:04  lr: 0.000100  loss_labels: 0.7039 (0.7672)  loss: 0.7163 (0.7672)  time: 0.5791\n",
      "Epoch: [2]  [1000/4429]  eta: 0:33:05  lr: 0.000100  loss_labels: 0.7319 (0.7665)  loss: 0.7482 (0.7665)  time: 0.5800\n",
      "Epoch: [2]  [1100/4429]  eta: 0:32:08  lr: 0.000100  loss_labels: 0.6806 (0.7636)  loss: 0.6692 (0.7636)  time: 0.5763\n",
      "Epoch: [2]  [1200/4429]  eta: 0:31:10  lr: 0.000100  loss_labels: 0.6873 (0.7613)  loss: 0.6518 (0.7613)  time: 0.5809\n",
      "Epoch: [2]  [1300/4429]  eta: 0:30:12  lr: 0.000100  loss_labels: 0.6963 (0.7596)  loss: 0.6488 (0.7596)  time: 0.5800\n",
      "Epoch: [2]  [1400/4429]  eta: 0:29:15  lr: 0.000100  loss_labels: 0.7013 (0.7594)  loss: 0.7013 (0.7594)  time: 0.5809\n",
      "Epoch: [2]  [1500/4429]  eta: 0:28:17  lr: 0.000100  loss_labels: 0.6813 (0.7576)  loss: 0.7064 (0.7576)  time: 0.5797\n",
      "Epoch: [2]  [1600/4429]  eta: 0:27:19  lr: 0.000100  loss_labels: 0.7142 (0.7573)  loss: 0.6838 (0.7573)  time: 0.5773\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://10.198.24.98:9628/'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "!python main.py --readout_res 'voxels' --save_model 0 --enc_output_layer 1 --batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 3526223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'], returncode=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "script = f'''#!/bin/sh\n",
    "#\n",
    "#\n",
    "#SBATCH --account=nklab\n",
    "#SBATCH --job-name=vox  # The job name.\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --nodelist=ax17\n",
    "#SBATCH --cpus-per-task=16\n",
    "\n",
    "ml load anaconda3-2019.03\n",
    "\n",
    "cd /engram/nklab/hossein/recurrent_models/transformer_brain_encoder/\n",
    "\n",
    "conda activate py39\n",
    "\n",
    "python main.py --subj 5 --wandb_p 'NSD' --wandb_r 'sub5_t_voxel_lr0005_4_g.5' --readout_res 'voxels' --save_model 1 --enc_output_layer 1 --batch_size 2 --lr 0.0005 --lr_drop 4 --run 1\n",
    "\n",
    "'''\n",
    "\n",
    "bash_script_path = \"/engram/nklab/hossein/batch_scripts/imagenet_nb.sh\"\n",
    "os.chdir('/engram/nklab/hossein/batch_scripts/')\n",
    "\n",
    "with open(bash_script_path, \"w+\") as bash_script_file:\n",
    "    bash_script_file.write(script)\n",
    "\n",
    "subprocess.run(['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training stimulus images: 8857\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 220\n",
      "Using cache found in /home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "xFormers not available\n",
      "xFormers not available\n",
      "brain_encoder(\n",
      "  (transformer): Transformer(\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (memory_proj): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (query_embed): Embedding(16, 768)\n",
      "  (backbone_model): Joiner(\n",
      "    (0): dino_model_with_hooks(\n",
      "      (backbone): DinoVisionTransformer(\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (blocks): ModuleList(\n",
      "          (0-11): 12 x NestedTensorBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): MemEffAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): LayerScale()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): LayerScale()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (head): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      "  (lh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=19004, bias=True)\n",
      "  )\n",
      "  (rh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=20544, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'transformer.memory_proj.weight', 'transformer.memory_proj.bias', 'query_embed.weight', 'lh_embed.0.weight', 'lh_embed.0.bias', 'rh_embed.0.weight', 'rh_embed.0.bias']\n",
      "Start training\n",
      "Epoch: [0]  [  0/277]  eta: 0:05:03  lr: 0.000100  loss_labels: 0.0928 (0.0928)  loss: 0.0928 (0.0928)  time: 1.0958\n",
      "Epoch: [0]  [100/277]  eta: 0:02:10  lr: 0.000100  loss_labels: 0.0545 (0.0563)  loss: 0.0478 (0.0563)  time: 0.7328\n",
      "Epoch: [0]  [200/277]  eta: 0:00:58  lr: 0.000100  loss_labels: 0.0469 (0.0515)  loss: 0.0452 (0.0515)  time: 0.7811\n",
      "Epoch: [0]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0446 (0.0495)  loss: 0.0444 (0.0495)  time: 0.7532\n",
      "Epoch: [0] Total time: 0:03:29 (0.7566 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0446 (0.0495)  loss: 0.0444 (0.0495)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0509 (0.0509)  loss: 0.0509 (0.0509)  time: 0.7239\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0426 (0.0429)  loss: 0.0426 (0.0429)  time: 0.7243\n",
      "Test: Total time: 0:00:22 (0.7259 s / it)\n",
      "Averaged stats: loss_labels: 0.0426 (0.0429)  loss: 0.0426 (0.0429)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1708.82it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1699.74it/s]\n",
      "val_perf: 0.026510914900150034\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [1]  [  0/277]  eta: 0:03:20  lr: 0.000100  loss_labels: 0.0398 (0.0398)  loss: 0.0398 (0.0398)  time: 0.7248\n",
      "Epoch: [1]  [100/277]  eta: 0:02:10  lr: 0.000100  loss_labels: 0.0433 (0.0433)  loss: 0.0439 (0.0433)  time: 0.7287\n",
      "Epoch: [1]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0422 (0.0429)  loss: 0.0414 (0.0429)  time: 0.7600\n",
      "Epoch: [1]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0414 (0.0426)  loss: 0.0417 (0.0426)  time: 0.7651\n",
      "Epoch: [1] Total time: 0:03:25 (0.7429 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0414 (0.0426)  loss: 0.0417 (0.0426)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0494 (0.0494)  loss: 0.0494 (0.0494)  time: 0.7202\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0415 (0.0415)  loss: 0.0415 (0.0415)  time: 0.7184\n",
      "Test: Total time: 0:00:22 (0.7189 s / it)\n",
      "Averaged stats: loss_labels: 0.0415 (0.0415)  loss: 0.0415 (0.0415)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1700.64it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1703.31it/s]\n",
      "val_perf: 0.027940151321163592\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [2]  [  0/277]  eta: 0:03:24  lr: 0.000100  loss_labels: 0.0428 (0.0428)  loss: 0.0428 (0.0428)  time: 0.7366\n",
      "Epoch: [2]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0408 (0.0411)  loss: 0.0399 (0.0411)  time: 0.7268\n",
      "Epoch: [2]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0409 (0.0410)  loss: 0.0418 (0.0410)  time: 0.7326\n",
      "Epoch: [2]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0416 (0.0411)  loss: 0.0419 (0.0411)  time: 0.7219\n",
      "Epoch: [2] Total time: 0:03:21 (0.7289 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0416 (0.0411)  loss: 0.0419 (0.0411)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0487 (0.0487)  loss: 0.0487 (0.0487)  time: 0.7253\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0405 (0.0407)  loss: 0.0405 (0.0407)  time: 0.7213\n",
      "Test: Total time: 0:00:22 (0.7190 s / it)\n",
      "Averaged stats: loss_labels: 0.0405 (0.0407)  loss: 0.0405 (0.0407)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1706.20it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1705.49it/s]\n",
      "val_perf: 0.028295141922515758\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [3]  [  0/277]  eta: 0:03:23  lr: 0.000100  loss_labels: 0.0411 (0.0411)  loss: 0.0411 (0.0411)  time: 0.7359\n",
      "Epoch: [3]  [100/277]  eta: 0:02:09  lr: 0.000100  loss_labels: 0.0397 (0.0400)  loss: 0.0390 (0.0400)  time: 0.7316\n",
      "Epoch: [3]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0400 (0.0402)  loss: 0.0386 (0.0402)  time: 0.7323\n",
      "Epoch: [3]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0398 (0.0402)  loss: 0.0409 (0.0402)  time: 0.7214\n",
      "Epoch: [3] Total time: 0:03:21 (0.7291 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0398 (0.0402)  loss: 0.0409 (0.0402)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0477 (0.0477)  loss: 0.0477 (0.0477)  time: 0.7188\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0401 (0.0400)  loss: 0.0401 (0.0400)  time: 0.7194\n",
      "Test: Total time: 0:00:22 (0.7195 s / it)\n",
      "Averaged stats: loss_labels: 0.0401 (0.0400)  loss: 0.0401 (0.0400)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1697.53it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1678.53it/s]\n",
      "val_perf: 0.028733481985735207\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.45s/it]\n",
      "Epoch: [4]  [  0/277]  eta: 0:03:23  lr: 0.000100  loss_labels: 0.0403 (0.0403)  loss: 0.0403 (0.0403)  time: 0.7356\n",
      "Epoch: [4]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0383 (0.0389)  loss: 0.0377 (0.0389)  time: 0.7292\n",
      "Epoch: [4]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0402 (0.0394)  loss: 0.0387 (0.0394)  time: 0.7329\n",
      "Epoch: [4]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0394 (0.0395)  loss: 0.0393 (0.0395)  time: 0.7261\n",
      "Epoch: [4] Total time: 0:03:22 (0.7301 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0394 (0.0395)  loss: 0.0393 (0.0395)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0469 (0.0469)  loss: 0.0469 (0.0469)  time: 0.7224\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0391 (0.0400)  loss: 0.0391 (0.0400)  time: 0.7212\n",
      "Test: Total time: 0:00:22 (0.7214 s / it)\n",
      "Averaged stats: loss_labels: 0.0391 (0.0400)  loss: 0.0391 (0.0400)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1693.22it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1667.95it/s]\n",
      "val_perf: 0.0287298471962317\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [5]  [  0/277]  eta: 0:03:27  lr: 0.000100  loss_labels: 0.0380 (0.0380)  loss: 0.0380 (0.0380)  time: 0.7482\n",
      "Epoch: [5]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0384 (0.0386)  loss: 0.0384 (0.0386)  time: 0.7264\n",
      "Epoch: [5]  [200/277]  eta: 0:00:55  lr: 0.000100  loss_labels: 0.0391 (0.0389)  loss: 0.0377 (0.0389)  time: 0.7300\n",
      "Epoch: [5]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0383 (0.0388)  loss: 0.0388 (0.0388)  time: 0.7216\n",
      "Epoch: [5] Total time: 0:03:21 (0.7263 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0383 (0.0388)  loss: 0.0388 (0.0388)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0484 (0.0484)  loss: 0.0484 (0.0484)  time: 0.7292\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0388 (0.0400)  loss: 0.0388 (0.0400)  time: 0.7210\n",
      "Test: Total time: 0:00:22 (0.7223 s / it)\n",
      "Averaged stats: loss_labels: 0.0388 (0.0400)  loss: 0.0388 (0.0400)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1660.93it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1636.83it/s]\n",
      "val_perf: 0.028914884954650347\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [6]  [  0/277]  eta: 0:03:23  lr: 0.000100  loss_labels: 0.0430 (0.0430)  loss: 0.0430 (0.0430)  time: 0.7356\n",
      "Epoch: [6]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0385 (0.0384)  loss: 0.0387 (0.0384)  time: 0.7294\n",
      "Epoch: [6]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0382 (0.0385)  loss: 0.0383 (0.0385)  time: 0.7574\n",
      "Epoch: [6]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0376 (0.0383)  loss: 0.0368 (0.0383)  time: 0.7231\n",
      "Epoch: [6] Total time: 0:03:22 (0.7297 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0376 (0.0383)  loss: 0.0368 (0.0383)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0486 (0.0486)  loss: 0.0486 (0.0486)  time: 0.7201\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0401 (0.0402)  loss: 0.0401 (0.0402)  time: 0.7155\n",
      "Test: Total time: 0:00:22 (0.7153 s / it)\n",
      "Averaged stats: loss_labels: 0.0401 (0.0402)  loss: 0.0401 (0.0402)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1691.45it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1696.56it/s]\n",
      "val_perf: 0.028816883728600057\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [7]  [  0/277]  eta: 0:03:24  lr: 0.000100  loss_labels: 0.0407 (0.0407)  loss: 0.0407 (0.0407)  time: 0.7388\n",
      "Epoch: [7]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0375 (0.0379)  loss: 0.0375 (0.0379)  time: 0.7286\n",
      "Epoch: [7]  [200/277]  eta: 0:00:55  lr: 0.000100  loss_labels: 0.0374 (0.0378)  loss: 0.0364 (0.0378)  time: 0.7292\n",
      "Epoch: [7]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0373 (0.0378)  loss: 0.0371 (0.0378)  time: 0.7200\n",
      "Epoch: [7] Total time: 0:03:21 (0.7270 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0373 (0.0378)  loss: 0.0371 (0.0378)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0487 (0.0487)  loss: 0.0487 (0.0487)  time: 0.7259\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0402 (0.0405)  loss: 0.0402 (0.0405)  time: 0.7174\n",
      "Test: Total time: 0:00:22 (0.7167 s / it)\n",
      "Averaged stats: loss_labels: 0.0402 (0.0405)  loss: 0.0402 (0.0405)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1678.18it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1681.64it/s]\n",
      "val_perf: 0.028947353625941807\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [8]  [  0/277]  eta: 0:03:19  lr: 0.000100  loss_labels: 0.0375 (0.0375)  loss: 0.0375 (0.0375)  time: 0.7220\n",
      "Epoch: [8]  [100/277]  eta: 0:02:09  lr: 0.000100  loss_labels: 0.0368 (0.0372)  loss: 0.0362 (0.0372)  time: 0.7327\n",
      "Epoch: [8]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0369 (0.0372)  loss: 0.0365 (0.0372)  time: 0.7341\n",
      "Epoch: [8]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0372 (0.0373)  loss: 0.0372 (0.0373)  time: 0.7262\n",
      "Epoch: [8] Total time: 0:03:22 (0.7327 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0372 (0.0373)  loss: 0.0372 (0.0373)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0473 (0.0473)  loss: 0.0473 (0.0473)  time: 0.7194\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0397 (0.0400)  loss: 0.0397 (0.0400)  time: 0.7182\n",
      "Test: Total time: 0:00:22 (0.7200 s / it)\n",
      "Averaged stats: loss_labels: 0.0397 (0.0400)  loss: 0.0397 (0.0400)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1682.49it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1683.11it/s]\n",
      "val_perf: 0.028896020936347716\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [9]  [  0/277]  eta: 0:03:27  lr: 0.000100  loss_labels: 0.0397 (0.0397)  loss: 0.0397 (0.0397)  time: 0.7493\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/main.py\", line 473, in <module>\n",
      "    main(0, 1, args)\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/main.py\", line 373, in main\n",
      "    train_stats = train_one_epoch(\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/engine.py\", line 24, in train_one_epoch\n",
      "    for imgs, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/utils/utils.py\", line 240, in log_every\n",
      "    for obj in iterable:\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/datasets/nsd.py\", line 83, in __getitem__\n",
      "    img = self.transform(img)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n",
      "    img = t(img)\n",
      "          ^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 137, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 166, in to_tensor\n",
      "    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/PIL/Image.py\", line 701, in __array_interface__\n",
      "    new[\"data\"] = self.tobytes()\n",
      "                  ^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/PIL/Image.py\", line 771, in tobytes\n",
      "    l, s, d = e.encode(bufsize)\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --readout_res 'faces' --save_model 1 --enc_output_layer 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py39]",
   "language": "python",
   "name": "conda-env-.conda-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
