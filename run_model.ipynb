{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models\n"
     ]
    }
   ],
   "source": [
    "data_path = '/share/data/imagenet-pytorch'\n",
    "\n",
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/BLT_models/')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/BLT_models/blt.py:142: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  elif conn_input is not 0:\n",
      "| distributed init (rank 1): env://\n",
      "| distributed init (rank 0): env://\n",
      "cuda\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3890/3890 [00:04<00:00, 869.58it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3890/3890 [00:04<00:00, 871.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3890/3890 [00:02<00:00, 1833.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3890/3890 [00:02<00:00, 1603.99it/s]\n",
      "blt(\n",
      "  (conv_input): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
      "  (non_lin_input): ReLU(inplace=True)\n",
      "  (norm_input): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (non_lin_0): ReLU(inplace=True)\n",
      "  (norm_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (output_0): Identity()\n",
      "  (norm_0_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (non_lin_0_0): ReLU(inplace=True)\n",
      "  (conv_0_0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm_0_1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (non_lin_0_1): ReLU(inplace=True)\n",
      "  (conv_0_1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (non_lin_1): ReLU(inplace=True)\n",
      "  (norm_1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (output_1): Identity()\n",
      "  (norm_1_0): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "  (non_lin_1_0): ReLU(inplace=True)\n",
      "  (conv_1_0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (norm_1_1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (non_lin_1_1): ReLU(inplace=True)\n",
      "  (conv_1_1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm_1_2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (non_lin_1_2): ReLU(inplace=True)\n",
      "  (conv_1_2): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (non_lin_2): ReLU(inplace=True)\n",
      "  (norm_2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (output_2): Identity()\n",
      "  (norm_2_1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "  (non_lin_2_1): ReLU(inplace=True)\n",
      "  (conv_2_1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (norm_2_2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (non_lin_2_2): ReLU(inplace=True)\n",
      "  (conv_2_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (norm_2_3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (non_lin_2_3): ReLU(inplace=True)\n",
      "  (conv_2_3): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (non_lin_3): ReLU(inplace=True)\n",
      "  (norm_3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (output_3): Identity()\n",
      "  (norm_3_2): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "  (non_lin_3_2): ReLU(inplace=True)\n",
      "  (conv_3_2): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (norm_3_3): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
      "  (non_lin_3_3): ReLU(inplace=True)\n",
      "  (conv_3_3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (read_out): Sequential(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (flatten): Flatten()\n",
      "    (linear): Linear(in_features=512, out_features=3890, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['conv_input.weight', 'conv_input.bias', 'norm_input.weight', 'norm_input.bias', 'norm_0.weight', 'norm_0.bias', 'norm_0_0.weight', 'norm_0_0.bias', 'conv_0_0.weight', 'conv_0_0.bias', 'norm_0_1.weight', 'norm_0_1.bias', 'conv_0_1.weight', 'conv_0_1.bias', 'norm_1.weight', 'norm_1.bias', 'norm_1_0.weight', 'norm_1_0.bias', 'conv_1_0.weight', 'conv_1_0.bias', 'norm_1_1.weight', 'norm_1_1.bias', 'conv_1_1.weight', 'conv_1_1.bias', 'norm_1_2.weight', 'norm_1_2.bias', 'conv_1_2.weight', 'conv_1_2.bias', 'norm_2.weight', 'norm_2.bias', 'norm_2_1.weight', 'norm_2_1.bias', 'conv_2_1.weight', 'conv_2_1.bias', 'norm_2_2.weight', 'norm_2_2.bias', 'conv_2_2.weight', 'conv_2_2.bias', 'norm_2_3.weight', 'norm_2_3.bias', 'conv_2_3.weight', 'conv_2_3.bias', 'norm_3.weight', 'norm_3.bias', 'norm_3_2.weight', 'norm_3_2.bias', 'conv_3_2.weight', 'conv_3_2.bias', 'norm_3_3.weight', 'norm_3_3.bias', 'conv_3_3.weight', 'conv_3_3.bias', 'read_out.linear.weight', 'read_out.linear.bias']\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhosseinadeli\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/engram/nklab/hossein/recurrent_models/BLT_models/wandb/run-20240512_231944-ztgv9pwt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblt_blt\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/hosseinadeli/vggface2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/hosseinadeli/vggface2/runs/ztgv9pwt\u001b[0m\n",
      "Start training\n",
      "Epoch: [0]  [   0/5008]  eta: 1 day, 8:18:15  lr: 0.000500  loss_labels: 8.3177 (8.3177)  loss: 8.3177 (8.3177)  time: 23.2220\n",
      "Epoch: [0]  [ 100/5008]  eta: 0:29:47  lr: 0.000500  loss_labels: 8.3180 (8.3137)  loss: 8.2885 (8.3137)  time: 0.1456\n",
      "Epoch: [0]  [ 200/5008]  eta: 0:20:49  lr: 0.000500  loss_labels: 8.2718 (8.2911)  loss: 8.2644 (8.2911)  time: 0.1380\n",
      "Epoch: [0]  [ 300/5008]  eta: 0:17:17  lr: 0.000500  loss_labels: 8.2504 (8.2784)  loss: 8.2467 (8.2784)  time: 0.1402\n",
      "Epoch: [0]  [ 400/5008]  eta: 0:15:34  lr: 0.000500  loss_labels: 8.2461 (8.2701)  loss: 8.2394 (8.2701)  time: 0.1379\n",
      "Epoch: [0]  [ 500/5008]  eta: 0:14:16  lr: 0.000500  loss_labels: 8.2375 (8.2630)  loss: 8.2340 (8.2630)  time: 0.1366\n",
      "Epoch: [0]  [ 600/5008]  eta: 0:13:21  lr: 0.000500  loss_labels: 8.2188 (8.2557)  loss: 8.2099 (8.2557)  time: 0.1398\n",
      "Epoch: [0]  [ 700/5008]  eta: 0:12:37  lr: 0.000500  loss_labels: 8.1959 (8.2476)  loss: 8.1847 (8.2476)  time: 0.1439\n",
      "Epoch: [0]  [ 800/5008]  eta: 0:11:56  lr: 0.000500  loss_labels: 8.1578 (8.2370)  loss: 8.1453 (8.2370)  time: 0.1333\n",
      "Epoch: [0]  [ 900/5008]  eta: 0:11:21  lr: 0.000500  loss_labels: 8.1252 (8.2246)  loss: 8.0995 (8.2246)  time: 0.1302\n",
      "Epoch: [0]  [1000/5008]  eta: 0:10:50  lr: 0.000500  loss_labels: 8.0478 (8.2067)  loss: 8.0090 (8.2067)  time: 0.1255\n",
      "Epoch: [0]  [1100/5008]  eta: 0:10:24  lr: 0.000500  loss_labels: 7.9511 (8.1831)  loss: 7.8928 (8.1831)  time: 0.1387\n",
      "Epoch: [0]  [1200/5008]  eta: 0:10:02  lr: 0.000500  loss_labels: 7.8181 (8.1525)  loss: 7.7753 (8.1525)  time: 0.1348\n",
      "Epoch: [0]  [1300/5008]  eta: 0:09:42  lr: 0.000500  loss_labels: 7.7324 (8.1201)  loss: 7.7243 (8.1201)  time: 0.1400\n",
      "Epoch: [0]  [1400/5008]  eta: 0:09:22  lr: 0.000500  loss_labels: 7.6431 (8.0858)  loss: 7.6142 (8.0858)  time: 0.1414\n",
      "Epoch: [0]  [1500/5008]  eta: 0:09:03  lr: 0.000500  loss_labels: 7.5621 (8.0503)  loss: 7.5068 (8.0503)  time: 0.1244\n",
      "Epoch: [0]  [1600/5008]  eta: 0:08:46  lr: 0.000500  loss_labels: 7.4958 (8.0156)  loss: 7.4369 (8.0156)  time: 0.1281\n",
      "Epoch: [0]  [1700/5008]  eta: 0:08:26  lr: 0.000500  loss_labels: 7.4150 (7.9805)  loss: 7.4091 (7.9805)  time: 0.1309\n",
      "Epoch: [0]  [1800/5008]  eta: 0:08:07  lr: 0.000500  loss_labels: 7.3444 (7.9455)  loss: 7.3038 (7.9455)  time: 0.1267\n",
      "Epoch: [0]  [1900/5008]  eta: 0:07:49  lr: 0.000500  loss_labels: 7.2688 (7.9101)  loss: 7.2410 (7.9101)  time: 0.1330\n",
      "Epoch: [0]  [2000/5008]  eta: 0:07:31  lr: 0.000500  loss_labels: 7.2001 (7.8748)  loss: 7.1535 (7.8748)  time: 0.1320\n",
      "Epoch: [0]  [2100/5008]  eta: 0:07:14  lr: 0.000500  loss_labels: 7.1439 (7.8403)  loss: 7.0851 (7.8403)  time: 0.1272\n",
      "Epoch: [0]  [2200/5008]  eta: 0:06:56  lr: 0.000500  loss_labels: 7.0773 (7.8057)  loss: 7.0756 (7.8057)  time: 0.1256\n",
      "Epoch: [0]  [2300/5008]  eta: 0:06:38  lr: 0.000500  loss_labels: 7.0163 (7.7719)  loss: 7.0111 (7.7719)  time: 0.1200\n",
      "Epoch: [0]  [2400/5008]  eta: 0:06:21  lr: 0.000500  loss_labels: 6.9583 (7.7382)  loss: 6.9221 (7.7382)  time: 0.1268\n",
      "Epoch: [0]  [2500/5008]  eta: 0:06:05  lr: 0.000500  loss_labels: 6.8997 (7.7048)  loss: 6.8498 (7.7048)  time: 0.1286\n",
      "Epoch: [0]  [2600/5008]  eta: 0:05:49  lr: 0.000500  loss_labels: 6.8427 (7.6717)  loss: 6.8080 (7.6717)  time: 0.1379\n",
      "Epoch: [0]  [2700/5008]  eta: 0:05:34  lr: 0.000500  loss_labels: 6.7663 (7.6384)  loss: 6.7026 (7.6384)  time: 0.1312\n",
      "Epoch: [0]  [2800/5008]  eta: 0:05:18  lr: 0.000500  loss_labels: 6.7137 (7.6054)  loss: 6.7006 (7.6054)  time: 0.1279\n",
      "Epoch: [0]  [2900/5008]  eta: 0:05:03  lr: 0.000500  loss_labels: 6.6287 (7.5720)  loss: 6.5817 (7.5720)  time: 0.1389\n",
      "Epoch: [0]  [3000/5008]  eta: 0:04:48  lr: 0.000500  loss_labels: 6.5913 (7.5393)  loss: 6.5680 (7.5393)  time: 0.1314\n",
      "Epoch: [0]  [3100/5008]  eta: 0:04:33  lr: 0.000500  loss_labels: 6.5049 (7.5061)  loss: 6.4869 (7.5061)  time: 0.1378\n",
      "Epoch: [0]  [3200/5008]  eta: 0:04:18  lr: 0.000500  loss_labels: 6.4337 (7.4727)  loss: 6.4044 (7.4727)  time: 0.1322\n",
      "Epoch: [0]  [3300/5008]  eta: 0:04:03  lr: 0.000500  loss_labels: 6.3823 (7.4398)  loss: 6.3484 (7.4398)  time: 0.1333\n",
      "Epoch: [0]  [3400/5008]  eta: 0:03:48  lr: 0.000500  loss_labels: 6.3167 (7.4067)  loss: 6.2915 (7.4067)  time: 0.1235\n",
      "Epoch: [0]  [3500/5008]  eta: 0:03:33  lr: 0.000500  loss_labels: 6.2616 (7.3740)  loss: 6.2698 (7.3740)  time: 0.1304\n",
      "Epoch: [0]  [3600/5008]  eta: 0:03:19  lr: 0.000500  loss_labels: 6.2216 (7.3420)  loss: 6.2115 (7.3420)  time: 0.1266\n",
      "Epoch: [0]  [3700/5008]  eta: 0:03:04  lr: 0.000500  loss_labels: 6.1153 (7.3090)  loss: 6.0982 (7.3090)  time: 0.1309\n",
      "Epoch: [0]  [3800/5008]  eta: 0:02:50  lr: 0.000500  loss_labels: 6.0907 (7.2772)  loss: 6.0615 (7.2772)  time: 0.1937\n",
      "Epoch: [0]  [3900/5008]  eta: 0:02:36  lr: 0.000500  loss_labels: 5.9997 (7.2448)  loss: 5.9776 (7.2448)  time: 0.1310\n",
      "Epoch: [0]  [4000/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 5.9751 (7.2128)  loss: 5.9088 (7.2128)  time: 0.1285\n",
      "Epoch: [0]  [4100/5008]  eta: 0:02:09  lr: 0.000500  loss_labels: 5.9277 (7.1818)  loss: 5.9220 (7.1818)  time: 0.2867\n",
      "Epoch: [0]  [4200/5008]  eta: 0:01:57  lr: 0.000500  loss_labels: 5.8406 (7.1503)  loss: 5.7601 (7.1503)  time: 0.5324\n",
      "Epoch: [0]  [4300/5008]  eta: 0:01:44  lr: 0.000500  loss_labels: 5.7957 (7.1190)  loss: 5.7571 (7.1190)  time: 0.1315\n",
      "Epoch: [0]  [4400/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 5.7531 (7.0881)  loss: 5.7109 (7.0881)  time: 0.1298\n",
      "Epoch: [0]  [4500/5008]  eta: 0:01:14  lr: 0.000500  loss_labels: 5.7020 (7.0576)  loss: 5.7202 (7.0576)  time: 0.1329\n",
      "Epoch: [0]  [4600/5008]  eta: 0:00:59  lr: 0.000500  loss_labels: 5.6511 (7.0270)  loss: 5.6106 (7.0270)  time: 0.1406\n",
      "Epoch: [0]  [4700/5008]  eta: 0:00:45  lr: 0.000500  loss_labels: 5.5527 (6.9956)  loss: 5.5543 (6.9956)  time: 0.1345\n",
      "Epoch: [0]  [4800/5008]  eta: 0:00:30  lr: 0.000500  loss_labels: 5.4910 (6.9646)  loss: 5.4820 (6.9646)  time: 0.1296\n",
      "Epoch: [0]  [4900/5008]  eta: 0:00:15  lr: 0.000500  loss_labels: 5.4638 (6.9340)  loss: 5.4152 (6.9340)  time: 0.1302\n",
      "Epoch: [0]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 5.4240 (6.9036)  loss: 5.3452 (6.9036)  time: 0.1319\n",
      "Epoch: [0]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 5.4235 (6.9015)  loss: 5.3301 (6.9015)  time: 0.1319\n",
      "Epoch: [0] Total time: 0:12:09 (0.1456 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 5.4235 (6.9015)  loss: 5.3301 (6.9015)\n",
      "Test:  [  0/565]  eta: 2:59:16  loss_labels: 5.6900 (5.6900)  loss: 5.6900 (5.6900)  time: 19.0382\n",
      "Test:  [100/565]  eta: 0:02:28  loss_labels: 5.5990 (5.6515)  loss: 5.6345 (5.6515)  time: 0.1347\n",
      "Test:  [200/565]  eta: 0:01:22  loss_labels: 5.5813 (5.6215)  loss: 5.7639 (5.6215)  time: 0.1286\n",
      "Test:  [300/565]  eta: 0:00:51  loss_labels: 5.5709 (5.5988)  loss: 5.5389 (5.5988)  time: 0.1276\n",
      "Test:  [400/565]  eta: 0:00:28  loss_labels: 5.6608 (5.6159)  loss: 5.6960 (5.6159)  time: 0.1183\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 5.5455 (5.6015)  loss: 5.8186 (5.6015)  time: 0.1274\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 5.6569 (5.6053)  loss: 5.6664 (5.6053)  time: 0.1263\n",
      "Test: Total time: 0:01:31 (0.1625 s / it)\n",
      "Averaged stats: loss_labels: 5.6569 (5.6053)  loss: 5.6664 (5.6053)\n",
      "acc: 0.08069296181201935\n",
      "top 1 and top 5 accuracies {'top1': 0.08069295961889991, 'top5': 0.20473051570376113, 'loss': tensor(0.0439, device='cuda:0')}\n",
      "Epoch: [1]  [   0/5008]  eta: 1 day, 7:38:30  lr: 0.000500  loss_labels: 5.5273 (5.5273)  loss: 5.5273 (5.5273)  time: 22.7457\n",
      "Epoch: [1]  [ 100/5008]  eta: 0:29:13  lr: 0.000500  loss_labels: 5.3296 (5.3422)  loss: 5.2615 (5.3422)  time: 0.1347\n",
      "Epoch: [1]  [ 200/5008]  eta: 0:19:42  lr: 0.000500  loss_labels: 5.3066 (5.3373)  loss: 5.2392 (5.3373)  time: 0.1365\n",
      "Epoch: [1]  [ 300/5008]  eta: 0:16:24  lr: 0.000500  loss_labels: 5.2239 (5.2995)  loss: 5.1820 (5.2995)  time: 0.1369\n",
      "Epoch: [1]  [ 400/5008]  eta: 0:14:30  lr: 0.000500  loss_labels: 5.1503 (5.2636)  loss: 5.1090 (5.2636)  time: 0.1276\n",
      "Epoch: [1]  [ 500/5008]  eta: 0:13:22  lr: 0.000500  loss_labels: 5.1379 (5.2395)  loss: 5.1095 (5.2395)  time: 0.1364\n",
      "Epoch: [1]  [ 600/5008]  eta: 0:12:30  lr: 0.000500  loss_labels: 5.0478 (5.2064)  loss: 5.0173 (5.2064)  time: 0.1357\n",
      "Epoch: [1]  [ 700/5008]  eta: 0:11:48  lr: 0.000500  loss_labels: 4.9747 (5.1741)  loss: 4.9900 (5.1741)  time: 0.1296\n",
      "Epoch: [1]  [ 800/5008]  eta: 0:11:13  lr: 0.000500  loss_labels: 4.9266 (5.1436)  loss: 4.9403 (5.1436)  time: 0.1284\n",
      "Epoch: [1]  [ 900/5008]  eta: 0:10:44  lr: 0.000500  loss_labels: 4.8888 (5.1150)  loss: 4.8802 (5.1150)  time: 0.1296\n",
      "Epoch: [1]  [1000/5008]  eta: 0:10:19  lr: 0.000500  loss_labels: 4.8462 (5.0881)  loss: 4.8323 (5.0881)  time: 0.1353\n",
      "Epoch: [1]  [1100/5008]  eta: 0:09:55  lr: 0.000500  loss_labels: 4.8038 (5.0633)  loss: 4.7149 (5.0633)  time: 0.1317\n",
      "Epoch: [1]  [1200/5008]  eta: 0:09:32  lr: 0.000500  loss_labels: 4.7731 (5.0398)  loss: 4.7899 (5.0398)  time: 0.1257\n",
      "Epoch: [1]  [1300/5008]  eta: 0:09:10  lr: 0.000500  loss_labels: 4.6928 (5.0126)  loss: 4.6939 (5.0126)  time: 0.1300\n",
      "Epoch: [1]  [1400/5008]  eta: 0:08:50  lr: 0.000500  loss_labels: 4.6381 (4.9849)  loss: 4.5777 (4.9849)  time: 0.1341\n",
      "Epoch: [1]  [1500/5008]  eta: 0:08:32  lr: 0.000500  loss_labels: 4.5855 (4.9589)  loss: 4.5080 (4.9589)  time: 0.1302\n",
      "Epoch: [1]  [1600/5008]  eta: 0:08:13  lr: 0.000500  loss_labels: 4.5598 (4.9339)  loss: 4.5001 (4.9339)  time: 0.1291\n",
      "Epoch: [1]  [1700/5008]  eta: 0:07:55  lr: 0.000500  loss_labels: 4.5066 (4.9096)  loss: 4.6423 (4.9096)  time: 0.1287\n",
      "Epoch: [1]  [1800/5008]  eta: 0:07:38  lr: 0.000500  loss_labels: 4.4595 (4.8844)  loss: 4.3728 (4.8844)  time: 0.1254\n",
      "Epoch: [1]  [1900/5008]  eta: 0:07:21  lr: 0.000500  loss_labels: 4.4138 (4.8613)  loss: 4.5485 (4.8613)  time: 0.1254\n",
      "Epoch: [1]  [2000/5008]  eta: 0:07:09  lr: 0.000500  loss_labels: 4.3520 (4.8362)  loss: 4.2230 (4.8362)  time: 0.1328\n",
      "Epoch: [1]  [2100/5008]  eta: 0:06:53  lr: 0.000500  loss_labels: 4.2951 (4.8109)  loss: 4.2789 (4.8109)  time: 0.1343\n",
      "Epoch: [1]  [2200/5008]  eta: 0:06:37  lr: 0.000500  loss_labels: 4.2208 (4.7848)  loss: 4.1912 (4.7848)  time: 0.1259\n",
      "Epoch: [1]  [2300/5008]  eta: 0:06:21  lr: 0.000500  loss_labels: 4.2247 (4.7613)  loss: 4.1970 (4.7613)  time: 0.1293\n",
      "Epoch: [1]  [2400/5008]  eta: 0:06:05  lr: 0.000500  loss_labels: 4.1822 (4.7373)  loss: 4.1494 (4.7373)  time: 0.1268\n",
      "Epoch: [1]  [2500/5008]  eta: 0:05:50  lr: 0.000500  loss_labels: 4.1375 (4.7138)  loss: 4.1763 (4.7138)  time: 0.1278\n",
      "Epoch: [1]  [2600/5008]  eta: 0:05:35  lr: 0.000500  loss_labels: 4.1024 (4.6913)  loss: 4.0549 (4.6913)  time: 0.1257\n",
      "Epoch: [1]  [2700/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 4.0434 (4.6668)  loss: 3.9561 (4.6668)  time: 0.1273\n",
      "Epoch: [1]  [2800/5008]  eta: 0:05:05  lr: 0.000500  loss_labels: 4.0107 (4.6437)  loss: 3.9993 (4.6437)  time: 0.1222\n",
      "Epoch: [1]  [2900/5008]  eta: 0:04:50  lr: 0.000500  loss_labels: 3.9654 (4.6204)  loss: 3.9042 (4.6204)  time: 0.1276\n",
      "Epoch: [1]  [3000/5008]  eta: 0:04:36  lr: 0.000500  loss_labels: 3.9395 (4.5979)  loss: 3.9813 (4.5979)  time: 0.1327\n",
      "Epoch: [1]  [3100/5008]  eta: 0:04:22  lr: 0.000500  loss_labels: 3.8774 (4.5750)  loss: 3.8550 (4.5750)  time: 0.1317\n",
      "Epoch: [1]  [3200/5008]  eta: 0:04:08  lr: 0.000500  loss_labels: 3.8222 (4.5518)  loss: 3.8156 (4.5518)  time: 0.1241\n",
      "Epoch: [1]  [3300/5008]  eta: 0:03:54  lr: 0.000500  loss_labels: 3.8107 (4.5294)  loss: 3.7935 (4.5294)  time: 0.1364\n",
      "Epoch: [1]  [3400/5008]  eta: 0:03:40  lr: 0.000500  loss_labels: 3.7318 (4.5064)  loss: 3.7281 (4.5064)  time: 0.1289\n",
      "Epoch: [1]  [3500/5008]  eta: 0:03:25  lr: 0.000500  loss_labels: 3.7326 (4.4847)  loss: 3.7346 (4.4847)  time: 0.1250\n",
      "Epoch: [1]  [3600/5008]  eta: 0:03:11  lr: 0.000500  loss_labels: 3.7057 (4.4633)  loss: 3.6762 (4.4633)  time: 0.1280\n",
      "Epoch: [1]  [3700/5008]  eta: 0:02:58  lr: 0.000500  loss_labels: 3.6287 (4.4409)  loss: 3.6579 (4.4409)  time: 0.1343\n",
      "Epoch: [1]  [3800/5008]  eta: 0:02:44  lr: 0.000500  loss_labels: 3.6253 (4.4194)  loss: 3.5422 (4.4194)  time: 0.1255\n",
      "Epoch: [1]  [3900/5008]  eta: 0:02:30  lr: 0.000500  loss_labels: 3.5874 (4.3986)  loss: 3.5504 (4.3986)  time: 0.1277\n",
      "Epoch: [1]  [4000/5008]  eta: 0:02:16  lr: 0.000500  loss_labels: 3.5334 (4.3771)  loss: 3.4833 (4.3771)  time: 0.1296\n",
      "Epoch: [1]  [4100/5008]  eta: 0:02:03  lr: 0.000500  loss_labels: 3.5044 (4.3562)  loss: 3.5405 (4.3562)  time: 0.1298\n",
      "Epoch: [1]  [4200/5008]  eta: 0:01:49  lr: 0.000500  loss_labels: 3.4803 (4.3352)  loss: 3.4350 (4.3352)  time: 0.1302\n",
      "Epoch: [1]  [4300/5008]  eta: 0:01:35  lr: 0.000500  loss_labels: 3.4356 (4.3143)  loss: 3.3551 (4.3143)  time: 0.1245\n",
      "Epoch: [1]  [4400/5008]  eta: 0:01:22  lr: 0.000500  loss_labels: 3.3832 (4.2937)  loss: 3.3832 (4.2937)  time: 0.1288\n",
      "Epoch: [1]  [4500/5008]  eta: 0:01:08  lr: 0.000500  loss_labels: 3.3661 (4.2728)  loss: 3.3832 (4.2728)  time: 0.1181\n",
      "Epoch: [1]  [4600/5008]  eta: 0:00:55  lr: 0.000500  loss_labels: 3.2938 (4.2517)  loss: 3.2328 (4.2517)  time: 0.1189\n",
      "Epoch: [1]  [4700/5008]  eta: 0:00:41  lr: 0.000500  loss_labels: 3.2734 (4.2309)  loss: 3.2665 (4.2309)  time: 0.1228\n",
      "Epoch: [1]  [4800/5008]  eta: 0:00:28  lr: 0.000500  loss_labels: 3.2211 (4.2101)  loss: 3.2293 (4.2101)  time: 0.1243\n",
      "Epoch: [1]  [4900/5008]  eta: 0:00:14  lr: 0.000500  loss_labels: 3.2274 (4.1899)  loss: 3.1579 (4.1899)  time: 0.1218\n",
      "Epoch: [1]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 3.2112 (4.1699)  loss: 3.2329 (4.1699)  time: 0.1260\n",
      "Epoch: [1]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 3.2089 (4.1685)  loss: 3.2288 (4.1685)  time: 0.1235\n",
      "Epoch: [1] Total time: 0:11:12 (0.1343 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 3.2089 (4.1685)  loss: 3.2288 (4.1685)\n",
      "Test:  [  0/565]  eta: 3:19:22  loss_labels: 3.5639 (3.5639)  loss: 3.5639 (3.5639)  time: 21.1732\n",
      "Test:  [100/565]  eta: 0:02:31  loss_labels: 3.8470 (3.8527)  loss: 4.0769 (3.8527)  time: 0.1208\n",
      "Test:  [200/565]  eta: 0:01:21  loss_labels: 3.7502 (3.8357)  loss: 4.0329 (3.8357)  time: 0.1238\n",
      "Test:  [300/565]  eta: 0:00:50  loss_labels: 3.7412 (3.8011)  loss: 3.5742 (3.8011)  time: 0.1266\n",
      "Test:  [400/565]  eta: 0:00:28  loss_labels: 3.8433 (3.8216)  loss: 3.7268 (3.8216)  time: 0.1104\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 3.7233 (3.7999)  loss: 3.9161 (3.7999)  time: 0.1137\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 3.6952 (3.7947)  loss: 3.6952 (3.7947)  time: 0.1373\n",
      "Test: Total time: 0:01:30 (0.1597 s / it)\n",
      "Averaged stats: loss_labels: 3.6952 (3.7947)  loss: 3.6952 (3.7947)\n",
      "acc: 0.3024427890777588\n",
      "top 1 and top 5 accuracies {'top1': 0.3024428072896472, 'top5': 0.5059823851991359, 'loss': tensor(0.0297, device='cuda:0')}\n",
      "Epoch: [2]  [   0/5008]  eta: 1 day, 13:34:11  lr: 0.000500  loss_labels: 3.3527 (3.3527)  loss: 3.3527 (3.3527)  time: 27.0070\n",
      "Epoch: [2]  [ 100/5008]  eta: 0:32:40  lr: 0.000500  loss_labels: 3.1189 (3.1372)  loss: 3.0156 (3.1372)  time: 0.1310\n",
      "Epoch: [2]  [ 200/5008]  eta: 0:22:01  lr: 0.000500  loss_labels: 3.1331 (3.1374)  loss: 3.1093 (3.1374)  time: 0.1276\n",
      "Epoch: [2]  [ 300/5008]  eta: 0:17:48  lr: 0.000500  loss_labels: 3.0875 (3.1191)  loss: 3.1339 (3.1191)  time: 0.1333\n",
      "Epoch: [2]  [ 400/5008]  eta: 0:15:32  lr: 0.000500  loss_labels: 3.0581 (3.1070)  loss: 3.0996 (3.1070)  time: 0.1274\n",
      "Epoch: [2]  [ 500/5008]  eta: 0:14:04  lr: 0.000500  loss_labels: 3.0251 (3.0905)  loss: 3.0261 (3.0905)  time: 0.1322\n",
      "Epoch: [2]  [ 600/5008]  eta: 0:13:00  lr: 0.000500  loss_labels: 2.9905 (3.0762)  loss: 2.9657 (3.0762)  time: 0.1309\n",
      "Epoch: [2]  [ 700/5008]  eta: 0:12:24  lr: 0.000500  loss_labels: 2.9550 (3.0578)  loss: 2.9579 (3.0578)  time: 0.1868\n",
      "Epoch: [2]  [ 800/5008]  eta: 0:11:46  lr: 0.000500  loss_labels: 2.9122 (3.0404)  loss: 2.9441 (3.0404)  time: 0.1358\n",
      "Epoch: [2]  [ 900/5008]  eta: 0:11:13  lr: 0.000500  loss_labels: 2.9045 (3.0277)  loss: 2.8676 (3.0277)  time: 0.1318\n",
      "Epoch: [2]  [1000/5008]  eta: 0:10:44  lr: 0.000500  loss_labels: 2.8884 (3.0133)  loss: 2.8823 (3.0133)  time: 0.1344\n",
      "Epoch: [2]  [1100/5008]  eta: 0:10:17  lr: 0.000500  loss_labels: 2.8820 (3.0001)  loss: 2.8159 (3.0001)  time: 0.1346\n",
      "Epoch: [2]  [1200/5008]  eta: 0:09:54  lr: 0.000500  loss_labels: 2.8536 (2.9866)  loss: 2.8207 (2.9866)  time: 0.1330\n",
      "Epoch: [2]  [1300/5008]  eta: 0:09:32  lr: 0.000500  loss_labels: 2.8509 (2.9742)  loss: 2.8421 (2.9742)  time: 0.1334\n",
      "Epoch: [2]  [1400/5008]  eta: 0:09:10  lr: 0.000500  loss_labels: 2.7840 (2.9613)  loss: 2.7808 (2.9613)  time: 0.1297\n",
      "Epoch: [2]  [1500/5008]  eta: 0:08:49  lr: 0.000500  loss_labels: 2.7962 (2.9491)  loss: 2.6710 (2.9491)  time: 0.1302\n",
      "Epoch: [2]  [1600/5008]  eta: 0:08:29  lr: 0.000500  loss_labels: 2.7213 (2.9368)  loss: 2.8021 (2.9368)  time: 0.1289\n",
      "Epoch: [2]  [1700/5008]  eta: 0:08:10  lr: 0.000500  loss_labels: 2.7609 (2.9255)  loss: 2.7933 (2.9255)  time: 0.1316\n",
      "Epoch: [2]  [1800/5008]  eta: 0:07:52  lr: 0.000500  loss_labels: 2.6585 (2.9120)  loss: 2.6523 (2.9120)  time: 0.1335\n",
      "Epoch: [2]  [1900/5008]  eta: 0:07:35  lr: 0.000500  loss_labels: 2.6640 (2.9001)  loss: 2.7119 (2.9001)  time: 0.1326\n",
      "Epoch: [2]  [2000/5008]  eta: 0:07:18  lr: 0.000500  loss_labels: 2.6475 (2.8875)  loss: 2.5820 (2.8875)  time: 0.1290\n",
      "Epoch: [2]  [2100/5008]  eta: 0:07:01  lr: 0.000500  loss_labels: 2.6088 (2.8741)  loss: 2.5947 (2.8741)  time: 0.1305\n",
      "Epoch: [2]  [2200/5008]  eta: 0:06:48  lr: 0.000500  loss_labels: 2.5679 (2.8605)  loss: 2.5476 (2.8605)  time: 0.1311\n",
      "Epoch: [2]  [2300/5008]  eta: 0:06:32  lr: 0.000500  loss_labels: 2.6161 (2.8498)  loss: 2.5423 (2.8498)  time: 0.1728\n",
      "Epoch: [2]  [2400/5008]  eta: 0:06:16  lr: 0.000500  loss_labels: 2.5531 (2.8382)  loss: 2.5637 (2.8382)  time: 0.1222\n",
      "Epoch: [2]  [2500/5008]  eta: 0:06:00  lr: 0.000500  loss_labels: 2.5454 (2.8265)  loss: 2.5266 (2.8265)  time: 0.1312\n",
      "Epoch: [2]  [2600/5008]  eta: 0:05:45  lr: 0.000500  loss_labels: 2.5218 (2.8156)  loss: 2.4527 (2.8156)  time: 0.1336\n",
      "Epoch: [2]  [2700/5008]  eta: 0:05:30  lr: 0.000500  loss_labels: 2.4837 (2.8035)  loss: 2.4415 (2.8035)  time: 0.1315\n",
      "Epoch: [2]  [2800/5008]  eta: 0:05:14  lr: 0.000500  loss_labels: 2.4496 (2.7914)  loss: 2.4496 (2.7914)  time: 0.1307\n",
      "Epoch: [2]  [2900/5008]  eta: 0:04:59  lr: 0.000500  loss_labels: 2.4345 (2.7797)  loss: 2.4154 (2.7797)  time: 0.1253\n",
      "Epoch: [2]  [3000/5008]  eta: 0:04:44  lr: 0.000500  loss_labels: 2.4526 (2.7685)  loss: 2.4526 (2.7685)  time: 0.1313\n",
      "Epoch: [2]  [3100/5008]  eta: 0:04:29  lr: 0.000500  loss_labels: 2.3788 (2.7564)  loss: 2.3260 (2.7564)  time: 0.1291\n",
      "Epoch: [2]  [3200/5008]  eta: 0:04:14  lr: 0.000500  loss_labels: 2.3785 (2.7448)  loss: 2.3983 (2.7448)  time: 0.1293\n",
      "Epoch: [2]  [3300/5008]  eta: 0:03:59  lr: 0.000500  loss_labels: 2.3366 (2.7327)  loss: 2.3318 (2.7327)  time: 0.1254\n",
      "Epoch: [2]  [3400/5008]  eta: 0:03:45  lr: 0.000500  loss_labels: 2.3369 (2.7208)  loss: 2.3591 (2.7208)  time: 0.1336\n",
      "Epoch: [2]  [3500/5008]  eta: 0:03:31  lr: 0.000500  loss_labels: 2.3131 (2.7100)  loss: 2.3635 (2.7100)  time: 0.1380\n",
      "Epoch: [2]  [3600/5008]  eta: 0:03:17  lr: 0.000500  loss_labels: 2.2969 (2.6991)  loss: 2.2786 (2.6991)  time: 0.2543\n",
      "Epoch: [2]  [3700/5008]  eta: 0:03:03  lr: 0.000500  loss_labels: 2.2485 (2.6873)  loss: 2.2185 (2.6873)  time: 0.1304\n",
      "Epoch: [2]  [3800/5008]  eta: 0:02:49  lr: 0.000500  loss_labels: 2.2683 (2.6764)  loss: 2.1864 (2.6764)  time: 0.1315\n",
      "Epoch: [2]  [3900/5008]  eta: 0:02:34  lr: 0.000500  loss_labels: 2.2458 (2.6659)  loss: 2.2129 (2.6659)  time: 0.1335\n",
      "Epoch: [2]  [4000/5008]  eta: 0:02:21  lr: 0.000500  loss_labels: 2.2463 (2.6553)  loss: 2.2148 (2.6553)  time: 0.1762\n",
      "Epoch: [2]  [4100/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 2.1949 (2.6441)  loss: 2.2289 (2.6441)  time: 0.1317\n",
      "Epoch: [2]  [4200/5008]  eta: 0:01:53  lr: 0.000500  loss_labels: 2.1935 (2.6335)  loss: 2.1541 (2.6335)  time: 0.1867\n",
      "Epoch: [2]  [4300/5008]  eta: 0:01:40  lr: 0.000500  loss_labels: 2.1627 (2.6230)  loss: 2.0937 (2.6230)  time: 0.1444\n",
      "Epoch: [2]  [4400/5008]  eta: 0:01:26  lr: 0.000500  loss_labels: 2.1870 (2.6132)  loss: 2.1632 (2.6132)  time: 0.1444\n",
      "Epoch: [2]  [4500/5008]  eta: 0:01:12  lr: 0.000500  loss_labels: 2.1502 (2.6032)  loss: 2.1148 (2.6032)  time: 0.1834\n",
      "Epoch: [2]  [4600/5008]  eta: 0:00:58  lr: 0.000500  loss_labels: 2.1044 (2.5925)  loss: 2.1262 (2.5925)  time: 0.1344\n",
      "Epoch: [2]  [4700/5008]  eta: 0:00:43  lr: 0.000500  loss_labels: 2.1317 (2.5825)  loss: 2.1828 (2.5825)  time: 0.1344\n",
      "Epoch: [2]  [4800/5008]  eta: 0:00:29  lr: 0.000500  loss_labels: 2.0625 (2.5718)  loss: 2.1211 (2.5718)  time: 0.1355\n",
      "Epoch: [2]  [4900/5008]  eta: 0:00:15  lr: 0.000500  loss_labels: 2.0989 (2.5620)  loss: 2.0130 (2.5620)  time: 0.1399\n",
      "Epoch: [2]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 2.0905 (2.5521)  loss: 2.1280 (2.5521)  time: 0.1422\n",
      "Epoch: [2]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 2.0976 (2.5515)  loss: 2.1367 (2.5515)  time: 0.1367\n",
      "Epoch: [2] Total time: 0:11:52 (0.1423 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 2.0976 (2.5515)  loss: 2.1367 (2.5515)\n",
      "Test:  [  0/565]  eta: 2:45:22  loss_labels: 2.7297 (2.7297)  loss: 2.7297 (2.7297)  time: 17.5624\n",
      "Test:  [100/565]  eta: 0:02:15  loss_labels: 3.2548 (3.2602)  loss: 3.2548 (3.2602)  time: 0.1121\n",
      "Test:  [200/565]  eta: 0:01:19  loss_labels: 3.1232 (3.2083)  loss: 3.3224 (3.2083)  time: 0.1186\n",
      "Test:  [300/565]  eta: 0:00:49  loss_labels: 2.9569 (3.1343)  loss: 2.9046 (3.1343)  time: 0.1247\n",
      "Test:  [400/565]  eta: 0:00:29  loss_labels: 3.1052 (3.1496)  loss: 2.9348 (3.1496)  time: 0.1101\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 2.9042 (3.1079)  loss: 3.4075 (3.1079)  time: 0.1220\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.9490 (3.0955)  loss: 3.2299 (3.0955)  time: 0.1118\n",
      "Test: Total time: 0:01:31 (0.1622 s / it)\n",
      "Averaged stats: loss_labels: 2.9490 (3.0955)  loss: 3.2299 (3.0955)\n",
      "acc: 0.408173143863678\n",
      "top 1 and top 5 accuracies {'top1': 0.40817315681604166, 'top5': 0.6153963330194427, 'loss': tensor(0.0242, device='cuda:0')}\n",
      "Epoch: [3]  [   0/5008]  eta: 1 day, 6:36:23  lr: 0.000500  loss_labels: 2.2028 (2.2028)  loss: 2.2028 (2.2028)  time: 22.0014\n",
      "Epoch: [3]  [ 100/5008]  eta: 0:28:55  lr: 0.000500  loss_labels: 2.0273 (2.0427)  loss: 1.9482 (2.0427)  time: 0.1255\n",
      "Epoch: [3]  [ 200/5008]  eta: 0:19:29  lr: 0.000500  loss_labels: 2.0426 (2.0443)  loss: 2.0382 (2.0443)  time: 0.1320\n",
      "Epoch: [3]  [ 300/5008]  eta: 0:16:07  lr: 0.000500  loss_labels: 2.0072 (2.0355)  loss: 2.0522 (2.0355)  time: 0.1265\n",
      "Epoch: [3]  [ 400/5008]  eta: 0:14:18  lr: 0.000500  loss_labels: 2.0061 (2.0296)  loss: 2.0332 (2.0296)  time: 0.1300\n",
      "Epoch: [3]  [ 500/5008]  eta: 0:13:07  lr: 0.000500  loss_labels: 1.9688 (2.0190)  loss: 1.9539 (2.0190)  time: 0.1275\n",
      "Epoch: [3]  [ 600/5008]  eta: 0:12:17  lr: 0.000500  loss_labels: 1.9555 (2.0080)  loss: 1.9058 (2.0080)  time: 0.1340\n",
      "Epoch: [3]  [ 700/5008]  eta: 0:11:38  lr: 0.000500  loss_labels: 1.9806 (2.0008)  loss: 2.0105 (2.0008)  time: 0.1251\n",
      "Epoch: [3]  [ 800/5008]  eta: 0:11:05  lr: 0.000500  loss_labels: 1.9116 (1.9915)  loss: 1.9102 (1.9915)  time: 0.1298\n",
      "Epoch: [3]  [ 900/5008]  eta: 0:10:38  lr: 0.000500  loss_labels: 1.9239 (1.9851)  loss: 1.8841 (1.9851)  time: 0.1368\n",
      "Epoch: [3]  [1000/5008]  eta: 0:10:13  lr: 0.000500  loss_labels: 1.8946 (1.9788)  loss: 1.8826 (1.9788)  time: 0.1335\n",
      "Epoch: [3]  [1100/5008]  eta: 0:09:50  lr: 0.000500  loss_labels: 1.8896 (1.9723)  loss: 1.8389 (1.9723)  time: 0.1266\n",
      "Epoch: [3]  [1200/5008]  eta: 0:09:38  lr: 0.000500  loss_labels: 1.8986 (1.9665)  loss: 1.8771 (1.9665)  time: 0.2849\n",
      "Epoch: [3]  [1300/5008]  eta: 0:09:15  lr: 0.000500  loss_labels: 1.8819 (1.9600)  loss: 1.8322 (1.9600)  time: 0.1242\n",
      "Epoch: [3]  [1400/5008]  eta: 0:08:56  lr: 0.000500  loss_labels: 1.8680 (1.9541)  loss: 1.8625 (1.9541)  time: 0.1319\n",
      "Epoch: [3]  [1500/5008]  eta: 0:08:37  lr: 0.000500  loss_labels: 1.8988 (1.9500)  loss: 1.7935 (1.9500)  time: 0.1303\n",
      "Epoch: [3]  [1600/5008]  eta: 0:08:20  lr: 0.000500  loss_labels: 1.8613 (1.9443)  loss: 1.8535 (1.9443)  time: 0.1327\n",
      "Epoch: [3]  [1700/5008]  eta: 0:08:03  lr: 0.000500  loss_labels: 1.8699 (1.9391)  loss: 1.8823 (1.9391)  time: 0.1263\n",
      "Epoch: [3]  [1800/5008]  eta: 0:07:46  lr: 0.000500  loss_labels: 1.8165 (1.9330)  loss: 1.7918 (1.9330)  time: 0.1622\n",
      "Epoch: [3]  [1900/5008]  eta: 0:07:28  lr: 0.000500  loss_labels: 1.8100 (1.9273)  loss: 1.8469 (1.9273)  time: 0.1228\n",
      "Epoch: [3]  [2000/5008]  eta: 0:07:11  lr: 0.000500  loss_labels: 1.8115 (1.9212)  loss: 1.6843 (1.9212)  time: 0.1278\n",
      "Epoch: [3]  [2100/5008]  eta: 0:06:54  lr: 0.000500  loss_labels: 1.7533 (1.9137)  loss: 1.7532 (1.9137)  time: 0.1267\n",
      "Epoch: [3]  [2200/5008]  eta: 0:06:38  lr: 0.000500  loss_labels: 1.7450 (1.9061)  loss: 1.7138 (1.9061)  time: 0.1256\n",
      "Epoch: [3]  [2300/5008]  eta: 0:06:22  lr: 0.000500  loss_labels: 1.7841 (1.9012)  loss: 1.6827 (1.9012)  time: 0.1296\n",
      "Epoch: [3]  [2400/5008]  eta: 0:06:06  lr: 0.000500  loss_labels: 1.7680 (1.8960)  loss: 1.7909 (1.8960)  time: 0.1253\n",
      "Epoch: [3]  [2500/5008]  eta: 0:05:52  lr: 0.000500  loss_labels: 1.7296 (1.8899)  loss: 1.7133 (1.8899)  time: 0.1270\n",
      "Epoch: [3]  [2600/5008]  eta: 0:05:37  lr: 0.000500  loss_labels: 1.7429 (1.8848)  loss: 1.6814 (1.8848)  time: 0.1258\n",
      "Epoch: [3]  [2700/5008]  eta: 0:05:22  lr: 0.000500  loss_labels: 1.7232 (1.8794)  loss: 1.6807 (1.8794)  time: 0.1317\n",
      "Epoch: [3]  [2800/5008]  eta: 0:05:07  lr: 0.000500  loss_labels: 1.6611 (1.8729)  loss: 1.7017 (1.8729)  time: 0.1209\n",
      "Epoch: [3]  [2900/5008]  eta: 0:04:53  lr: 0.000500  loss_labels: 1.6804 (1.8667)  loss: 1.6481 (1.8667)  time: 0.1269\n",
      "Epoch: [3]  [3000/5008]  eta: 0:04:39  lr: 0.000500  loss_labels: 1.7215 (1.8622)  loss: 1.6858 (1.8622)  time: 0.1280\n",
      "Epoch: [3]  [3100/5008]  eta: 0:04:25  lr: 0.000500  loss_labels: 1.6766 (1.8564)  loss: 1.6696 (1.8564)  time: 0.1293\n",
      "Epoch: [3]  [3200/5008]  eta: 0:04:10  lr: 0.000500  loss_labels: 1.6672 (1.8510)  loss: 1.6715 (1.8510)  time: 0.1286\n",
      "Epoch: [3]  [3300/5008]  eta: 0:03:56  lr: 0.000500  loss_labels: 1.6613 (1.8452)  loss: 1.6393 (1.8452)  time: 0.1256\n",
      "Epoch: [3]  [3400/5008]  eta: 0:03:41  lr: 0.000500  loss_labels: 1.6271 (1.8387)  loss: 1.6357 (1.8387)  time: 0.1229\n",
      "Epoch: [3]  [3500/5008]  eta: 0:03:27  lr: 0.000500  loss_labels: 1.6511 (1.8335)  loss: 1.6672 (1.8335)  time: 0.1320\n",
      "Epoch: [3]  [3600/5008]  eta: 0:03:13  lr: 0.000500  loss_labels: 1.6342 (1.8284)  loss: 1.5900 (1.8284)  time: 0.1279\n",
      "Epoch: [3]  [3700/5008]  eta: 0:02:59  lr: 0.000500  loss_labels: 1.5987 (1.8223)  loss: 1.5447 (1.8223)  time: 0.1263\n",
      "Epoch: [3]  [3800/5008]  eta: 0:02:45  lr: 0.000500  loss_labels: 1.6352 (1.8173)  loss: 1.5539 (1.8173)  time: 0.1226\n",
      "Epoch: [3]  [3900/5008]  eta: 0:02:31  lr: 0.000500  loss_labels: 1.6171 (1.8123)  loss: 1.6171 (1.8123)  time: 0.1271\n",
      "Epoch: [3]  [4000/5008]  eta: 0:02:17  lr: 0.000500  loss_labels: 1.6030 (1.8074)  loss: 1.6014 (1.8074)  time: 0.1263\n",
      "Epoch: [3]  [4100/5008]  eta: 0:02:03  lr: 0.000500  loss_labels: 1.5756 (1.8016)  loss: 1.5253 (1.8016)  time: 0.1274\n",
      "Epoch: [3]  [4200/5008]  eta: 0:01:50  lr: 0.000500  loss_labels: 1.5728 (1.7960)  loss: 1.5884 (1.7960)  time: 0.1940\n",
      "Epoch: [3]  [4300/5008]  eta: 0:01:36  lr: 0.000500  loss_labels: 1.5365 (1.7901)  loss: 1.4893 (1.7901)  time: 0.1362\n",
      "Epoch: [3]  [4400/5008]  eta: 0:01:23  lr: 0.000500  loss_labels: 1.5829 (1.7856)  loss: 1.5122 (1.7856)  time: 0.1443\n",
      "Epoch: [3]  [4500/5008]  eta: 0:01:09  lr: 0.000500  loss_labels: 1.5394 (1.7803)  loss: 1.5394 (1.7803)  time: 0.1315\n",
      "Epoch: [3]  [4600/5008]  eta: 0:00:55  lr: 0.000500  loss_labels: 1.5256 (1.7749)  loss: 1.5318 (1.7749)  time: 0.1322\n",
      "Epoch: [3]  [4700/5008]  eta: 0:00:42  lr: 0.000500  loss_labels: 1.5328 (1.7699)  loss: 1.5707 (1.7699)  time: 0.1418\n",
      "Epoch: [3]  [4800/5008]  eta: 0:00:28  lr: 0.000500  loss_labels: 1.4912 (1.7644)  loss: 1.5924 (1.7644)  time: 0.1280\n",
      "Epoch: [3]  [4900/5008]  eta: 0:00:14  lr: 0.000500  loss_labels: 1.4950 (1.7593)  loss: 1.4374 (1.7593)  time: 0.1299\n",
      "Epoch: [3]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 1.4782 (1.7540)  loss: 1.4748 (1.7540)  time: 0.2509\n",
      "Epoch: [3]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 1.4782 (1.7537)  loss: 1.4915 (1.7537)  time: 0.2908\n",
      "Epoch: [3] Total time: 0:11:35 (0.1389 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 1.4782 (1.7537)  loss: 1.4915 (1.7537)\n",
      "Test:  [  0/565]  eta: 3:52:45  loss_labels: 2.3349 (2.3349)  loss: 2.3349 (2.3349)  time: 24.7175\n",
      "Test:  [100/565]  eta: 0:02:50  loss_labels: 2.7945 (2.8252)  loss: 3.0128 (2.8252)  time: 0.1287\n",
      "Test:  [200/565]  eta: 0:01:29  loss_labels: 2.6582 (2.7715)  loss: 2.9412 (2.7715)  time: 0.1210\n",
      "Test:  [300/565]  eta: 0:00:53  loss_labels: 2.5145 (2.7072)  loss: 2.4432 (2.7072)  time: 0.1302\n",
      "Test:  [400/565]  eta: 0:00:30  loss_labels: 2.7541 (2.7329)  loss: 2.5551 (2.7329)  time: 0.1150\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 2.4692 (2.6919)  loss: 2.9060 (2.6919)  time: 0.2542\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.6001 (2.6794)  loss: 2.7882 (2.6794)  time: 0.4630\n",
      "Test: Total time: 0:01:58 (0.2093 s / it)\n",
      "Averaged stats: loss_labels: 2.6001 (2.6794)  loss: 2.7882 (2.6794)\n",
      "acc: 0.4838946461677551\n",
      "top 1 and top 5 accuracies {'top1': 0.48389464354954853, 'top5': 0.6783360106353514, 'loss': tensor(0.0210, device='cuda:0')}\n",
      "Epoch: [4]  [   0/5008]  eta: 1 day, 6:49:35  lr: 0.000500  loss_labels: 1.5561 (1.5561)  loss: 1.5561 (1.5561)  time: 22.1596\n",
      "Epoch: [4]  [ 100/5008]  eta: 0:34:15  lr: 0.000500  loss_labels: 1.4795 (1.4922)  loss: 1.4061 (1.4922)  time: 0.1358\n",
      "Epoch: [4]  [ 200/5008]  eta: 0:22:15  lr: 0.000500  loss_labels: 1.4849 (1.4902)  loss: 1.4600 (1.4902)  time: 0.1362\n",
      "Epoch: [4]  [ 300/5008]  eta: 0:18:17  lr: 0.000500  loss_labels: 1.4635 (1.4814)  loss: 1.4930 (1.4814)  time: 0.1305\n",
      "Epoch: [4]  [ 400/5008]  eta: 0:17:05  lr: 0.000500  loss_labels: 1.4671 (1.4804)  loss: 1.4813 (1.4804)  time: 0.1308\n",
      "Epoch: [4]  [ 500/5008]  eta: 0:15:18  lr: 0.000500  loss_labels: 1.4574 (1.4758)  loss: 1.3915 (1.4758)  time: 0.1335\n",
      "Epoch: [4]  [ 600/5008]  eta: 0:14:06  lr: 0.000500  loss_labels: 1.4401 (1.4690)  loss: 1.3983 (1.4690)  time: 0.1275\n",
      "Epoch: [4]  [ 700/5008]  eta: 0:13:07  lr: 0.000500  loss_labels: 1.4395 (1.4653)  loss: 1.4299 (1.4653)  time: 0.1338\n",
      "Epoch: [4]  [ 800/5008]  eta: 0:12:21  lr: 0.000500  loss_labels: 1.4374 (1.4627)  loss: 1.4265 (1.4627)  time: 0.1325\n",
      "Epoch: [4]  [ 900/5008]  eta: 0:11:40  lr: 0.000500  loss_labels: 1.4029 (1.4580)  loss: 1.3930 (1.4580)  time: 0.1236\n",
      "Epoch: [4]  [1000/5008]  eta: 0:11:05  lr: 0.000500  loss_labels: 1.4150 (1.4551)  loss: 1.3947 (1.4551)  time: 0.1281\n",
      "Epoch: [4]  [1100/5008]  eta: 0:10:42  lr: 0.000500  loss_labels: 1.3700 (1.4493)  loss: 1.3579 (1.4493)  time: 0.2082\n",
      "Epoch: [4]  [1200/5008]  eta: 0:10:22  lr: 0.000500  loss_labels: 1.4168 (1.4469)  loss: 1.4191 (1.4469)  time: 0.1296\n",
      "Epoch: [4]  [1300/5008]  eta: 0:09:57  lr: 0.000500  loss_labels: 1.4181 (1.4438)  loss: 1.3715 (1.4438)  time: 0.1301\n",
      "Epoch: [4]  [1400/5008]  eta: 0:09:33  lr: 0.000500  loss_labels: 1.3854 (1.4405)  loss: 1.3878 (1.4405)  time: 0.1278\n",
      "Epoch: [4]  [1500/5008]  eta: 0:09:10  lr: 0.000500  loss_labels: 1.4061 (1.4383)  loss: 1.3485 (1.4383)  time: 0.1246\n",
      "Epoch: [4]  [1600/5008]  eta: 0:08:48  lr: 0.000500  loss_labels: 1.4021 (1.4354)  loss: 1.4021 (1.4354)  time: 0.1305\n",
      "Epoch: [4]  [1700/5008]  eta: 0:08:28  lr: 0.000500  loss_labels: 1.3867 (1.4322)  loss: 1.3908 (1.4322)  time: 0.1302\n",
      "Epoch: [4]  [1800/5008]  eta: 0:08:09  lr: 0.000500  loss_labels: 1.3573 (1.4285)  loss: 1.3233 (1.4285)  time: 0.1452\n",
      "Epoch: [4]  [1900/5008]  eta: 0:07:50  lr: 0.000500  loss_labels: 1.3356 (1.4245)  loss: 1.3782 (1.4245)  time: 0.1257\n",
      "Epoch: [4]  [2000/5008]  eta: 0:07:31  lr: 0.000500  loss_labels: 1.3862 (1.4225)  loss: 1.2445 (1.4225)  time: 0.1226\n",
      "Epoch: [4]  [2100/5008]  eta: 0:07:12  lr: 0.000500  loss_labels: 1.3093 (1.4178)  loss: 1.3438 (1.4178)  time: 0.1277\n",
      "Epoch: [4]  [2200/5008]  eta: 0:06:55  lr: 0.000500  loss_labels: 1.3187 (1.4138)  loss: 1.2764 (1.4138)  time: 0.1248\n",
      "Epoch: [4]  [2300/5008]  eta: 0:06:37  lr: 0.000500  loss_labels: 1.3647 (1.4118)  loss: 1.3210 (1.4118)  time: 0.1219\n",
      "Epoch: [4]  [2400/5008]  eta: 0:06:21  lr: 0.000500  loss_labels: 1.3312 (1.4084)  loss: 1.3149 (1.4084)  time: 0.1294\n",
      "Epoch: [4]  [2500/5008]  eta: 0:06:04  lr: 0.000500  loss_labels: 1.3042 (1.4044)  loss: 1.2849 (1.4044)  time: 0.1249\n",
      "Epoch: [4]  [2600/5008]  eta: 0:05:48  lr: 0.000500  loss_labels: 1.3202 (1.4016)  loss: 1.2460 (1.4016)  time: 0.1439\n",
      "Epoch: [4]  [2700/5008]  eta: 0:05:33  lr: 0.000500  loss_labels: 1.2878 (1.3980)  loss: 1.2606 (1.3980)  time: 0.1377\n",
      "Epoch: [4]  [2800/5008]  eta: 0:05:18  lr: 0.000500  loss_labels: 1.2783 (1.3939)  loss: 1.2712 (1.3939)  time: 0.1345\n",
      "Epoch: [4]  [2900/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 1.2915 (1.3904)  loss: 1.2962 (1.3904)  time: 0.1269\n",
      "Epoch: [4]  [3000/5008]  eta: 0:04:47  lr: 0.000500  loss_labels: 1.3210 (1.3878)  loss: 1.2623 (1.3878)  time: 0.1295\n",
      "Epoch: [4]  [3100/5008]  eta: 0:04:32  lr: 0.000500  loss_labels: 1.2838 (1.3849)  loss: 1.2745 (1.3849)  time: 0.1308\n",
      "Epoch: [4]  [3200/5008]  eta: 0:04:17  lr: 0.000500  loss_labels: 1.2571 (1.3815)  loss: 1.2656 (1.3815)  time: 0.1320\n",
      "Epoch: [4]  [3300/5008]  eta: 0:04:02  lr: 0.000500  loss_labels: 1.2722 (1.3786)  loss: 1.3158 (1.3786)  time: 0.1284\n",
      "Epoch: [4]  [3400/5008]  eta: 0:03:47  lr: 0.000500  loss_labels: 1.2676 (1.3750)  loss: 1.2452 (1.3750)  time: 0.1319\n",
      "Epoch: [4]  [3500/5008]  eta: 0:03:34  lr: 0.000500  loss_labels: 1.2622 (1.3719)  loss: 1.2641 (1.3719)  time: 0.1334\n",
      "Epoch: [4]  [3600/5008]  eta: 0:03:20  lr: 0.000500  loss_labels: 1.2591 (1.3691)  loss: 1.2211 (1.3691)  time: 0.1598\n",
      "Epoch: [4]  [3700/5008]  eta: 0:03:06  lr: 0.000500  loss_labels: 1.2180 (1.3651)  loss: 1.2249 (1.3651)  time: 0.1451\n",
      "Epoch: [4]  [3800/5008]  eta: 0:02:51  lr: 0.000500  loss_labels: 1.2769 (1.3624)  loss: 1.2483 (1.3624)  time: 0.1301\n",
      "Epoch: [4]  [3900/5008]  eta: 0:02:37  lr: 0.000500  loss_labels: 1.2416 (1.3598)  loss: 1.2445 (1.3598)  time: 0.1371\n",
      "Epoch: [4]  [4000/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 1.2324 (1.3566)  loss: 1.2784 (1.3566)  time: 0.1497\n",
      "Epoch: [4]  [4100/5008]  eta: 0:02:08  lr: 0.000500  loss_labels: 1.2155 (1.3532)  loss: 1.2155 (1.3532)  time: 0.1282\n",
      "Epoch: [4]  [4200/5008]  eta: 0:01:54  lr: 0.000500  loss_labels: 1.2123 (1.3500)  loss: 1.2432 (1.3500)  time: 0.1441\n",
      "Epoch: [4]  [4300/5008]  eta: 0:01:39  lr: 0.000500  loss_labels: 1.1967 (1.3463)  loss: 1.1250 (1.3463)  time: 0.1402\n",
      "Epoch: [4]  [4400/5008]  eta: 0:01:25  lr: 0.000500  loss_labels: 1.2641 (1.3439)  loss: 1.1546 (1.3439)  time: 0.1330\n",
      "Epoch: [4]  [4500/5008]  eta: 0:01:11  lr: 0.000500  loss_labels: 1.1825 (1.3406)  loss: 1.1940 (1.3406)  time: 0.1489\n",
      "Epoch: [4]  [4600/5008]  eta: 0:00:57  lr: 0.000500  loss_labels: 1.2316 (1.3378)  loss: 1.1992 (1.3378)  time: 0.1710\n",
      "Epoch: [4]  [4700/5008]  eta: 0:00:44  lr: 0.000500  loss_labels: 1.1915 (1.3348)  loss: 1.2448 (1.3348)  time: 0.1357\n",
      "Epoch: [4]  [4800/5008]  eta: 0:00:30  lr: 0.000500  loss_labels: 1.1506 (1.3315)  loss: 1.2182 (1.3315)  time: 0.1359\n",
      "Epoch: [4]  [4900/5008]  eta: 0:00:15  lr: 0.000500  loss_labels: 1.1597 (1.3282)  loss: 1.1344 (1.3282)  time: 0.1398\n",
      "Epoch: [4]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 1.1777 (1.3249)  loss: 1.1814 (1.3249)  time: 0.1346\n",
      "Epoch: [4]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 1.1777 (1.3247)  loss: 1.1369 (1.3247)  time: 0.1342\n",
      "Epoch: [4] Total time: 0:12:03 (0.1445 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 1.1777 (1.3247)  loss: 1.1369 (1.3247)\n",
      "Test:  [  0/565]  eta: 3:28:49  loss_labels: 2.1863 (2.1863)  loss: 2.1863 (2.1863)  time: 22.1755\n",
      "Test:  [100/565]  eta: 0:02:39  loss_labels: 2.5057 (2.6260)  loss: 2.6218 (2.6260)  time: 0.1214\n",
      "Test:  [200/565]  eta: 0:01:26  loss_labels: 2.4169 (2.5625)  loss: 2.6284 (2.5625)  time: 0.1189\n",
      "Test:  [300/565]  eta: 0:00:53  loss_labels: 2.2717 (2.4982)  loss: 2.0881 (2.4982)  time: 0.1370\n",
      "Test:  [400/565]  eta: 0:00:30  loss_labels: 2.5527 (2.5267)  loss: 2.4812 (2.5267)  time: 0.1240\n",
      "Test:  [500/565]  eta: 0:00:11  loss_labels: 2.2664 (2.4878)  loss: 2.6016 (2.4878)  time: 0.1203\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.3657 (2.4720)  loss: 2.4712 (2.4720)  time: 0.1572\n",
      "Test: Total time: 0:01:35 (0.1698 s / it)\n",
      "Averaged stats: loss_labels: 2.3657 (2.4720)  loss: 2.4712 (2.4720)\n",
      "acc: 0.5269483923912048\n",
      "top 1 and top 5 accuracies {'top1': 0.5269484296238852, 'top5': 0.7114883952805627, 'loss': tensor(0.0193, device='cuda:0')}\n",
      "Epoch: [5]  [   0/5008]  eta: 2 days, 3:36:23  lr: 0.000500  loss_labels: 1.2451 (1.2451)  loss: 1.2451 (1.2451)  time: 37.0974\n",
      "Epoch: [5]  [ 100/5008]  eta: 0:41:16  lr: 0.000500  loss_labels: 1.1525 (1.1642)  loss: 1.1130 (1.1642)  time: 0.1339\n",
      "Epoch: [5]  [ 200/5008]  eta: 0:25:47  lr: 0.000500  loss_labels: 1.1803 (1.1672)  loss: 1.1757 (1.1672)  time: 0.1337\n",
      "Epoch: [5]  [ 300/5008]  eta: 0:20:24  lr: 0.000500  loss_labels: 1.1575 (1.1608)  loss: 1.2106 (1.1608)  time: 0.1340\n",
      "Epoch: [5]  [ 400/5008]  eta: 0:17:38  lr: 0.000500  loss_labels: 1.1462 (1.1593)  loss: 1.1405 (1.1593)  time: 0.1383\n",
      "Epoch: [5]  [ 500/5008]  eta: 0:15:53  lr: 0.000500  loss_labels: 1.1469 (1.1569)  loss: 1.1678 (1.1569)  time: 0.1356\n",
      "Epoch: [5]  [ 600/5008]  eta: 0:14:38  lr: 0.000500  loss_labels: 1.1213 (1.1508)  loss: 1.1053 (1.1508)  time: 0.1394\n",
      "Epoch: [5]  [ 700/5008]  eta: 0:13:45  lr: 0.000500  loss_labels: 1.1087 (1.1472)  loss: 1.1034 (1.1472)  time: 0.1598\n",
      "Epoch: [5]  [ 800/5008]  eta: 0:13:00  lr: 0.000500  loss_labels: 1.1188 (1.1454)  loss: 1.1188 (1.1454)  time: 0.1380\n",
      "Epoch: [5]  [ 900/5008]  eta: 0:12:26  lr: 0.000500  loss_labels: 1.1243 (1.1422)  loss: 1.0620 (1.1422)  time: 0.1299\n",
      "Epoch: [5]  [1000/5008]  eta: 0:12:29  lr: 0.000500  loss_labels: 1.1035 (1.1398)  loss: 1.0763 (1.1398)  time: 0.1297\n",
      "Epoch: [5]  [1100/5008]  eta: 0:11:51  lr: 0.000500  loss_labels: 1.0798 (1.1353)  loss: 1.0572 (1.1353)  time: 0.1271\n",
      "Epoch: [5]  [1200/5008]  eta: 0:11:49  lr: 0.000500  loss_labels: 1.1419 (1.1345)  loss: 1.1328 (1.1345)  time: 0.1276\n",
      "Epoch: [5]  [1300/5008]  eta: 0:11:15  lr: 0.000500  loss_labels: 1.1031 (1.1328)  loss: 1.0601 (1.1328)  time: 0.1319\n",
      "Epoch: [5]  [1400/5008]  eta: 0:10:44  lr: 0.000500  loss_labels: 1.1043 (1.1302)  loss: 1.1172 (1.1302)  time: 0.1367\n",
      "Epoch: [5]  [1500/5008]  eta: 0:10:17  lr: 0.000500  loss_labels: 1.0927 (1.1289)  loss: 1.0171 (1.1289)  time: 0.1256\n",
      "Epoch: [5]  [1600/5008]  eta: 0:09:48  lr: 0.000500  loss_labels: 1.0896 (1.1271)  loss: 1.0896 (1.1271)  time: 0.1226\n",
      "Epoch: [5]  [1700/5008]  eta: 0:09:22  lr: 0.000500  loss_labels: 1.0840 (1.1246)  loss: 1.0776 (1.1246)  time: 0.1285\n",
      "Epoch: [5]  [1800/5008]  eta: 0:08:58  lr: 0.000500  loss_labels: 1.0635 (1.1216)  loss: 1.0830 (1.1216)  time: 0.1256\n",
      "Epoch: [5]  [1900/5008]  eta: 0:08:34  lr: 0.000500  loss_labels: 1.0271 (1.1182)  loss: 1.0656 (1.1182)  time: 0.1251\n",
      "Epoch: [5]  [2000/5008]  eta: 0:08:12  lr: 0.000500  loss_labels: 1.0877 (1.1170)  loss: 0.9657 (1.1170)  time: 0.1344\n",
      "Epoch: [5]  [2100/5008]  eta: 0:07:51  lr: 0.000500  loss_labels: 1.0414 (1.1135)  loss: 1.0135 (1.1135)  time: 0.1256\n",
      "Epoch: [5]  [2200/5008]  eta: 0:07:30  lr: 0.000500  loss_labels: 1.0559 (1.1110)  loss: 1.0468 (1.1110)  time: 0.1262\n",
      "Epoch: [5]  [2300/5008]  eta: 0:07:10  lr: 0.000500  loss_labels: 1.0974 (1.1106)  loss: 1.0443 (1.1106)  time: 0.1282\n",
      "Epoch: [5]  [2400/5008]  eta: 0:06:51  lr: 0.000500  loss_labels: 1.0577 (1.1086)  loss: 1.0766 (1.1086)  time: 0.1259\n",
      "Epoch: [5]  [2500/5008]  eta: 0:06:32  lr: 0.000500  loss_labels: 1.0364 (1.1061)  loss: 0.9973 (1.1061)  time: 0.1303\n",
      "Epoch: [5]  [2600/5008]  eta: 0:06:14  lr: 0.000500  loss_labels: 1.0546 (1.1044)  loss: 1.0091 (1.1044)  time: 0.1298\n",
      "Epoch: [5]  [2700/5008]  eta: 0:06:04  lr: 0.000500  loss_labels: 1.0119 (1.1022)  loss: 0.9872 (1.1022)  time: 0.4894\n",
      "Epoch: [5]  [2800/5008]  eta: 0:05:56  lr: 0.000500  loss_labels: 1.0203 (1.0994)  loss: 1.0169 (1.0994)  time: 0.1367\n",
      "Epoch: [5]  [2900/5008]  eta: 0:05:38  lr: 0.000500  loss_labels: 1.0390 (1.0972)  loss: 0.9935 (1.0972)  time: 0.1428\n",
      "Epoch: [5]  [3000/5008]  eta: 0:05:20  lr: 0.000500  loss_labels: 1.0427 (1.0954)  loss: 0.9860 (1.0954)  time: 0.1260\n",
      "Epoch: [5]  [3100/5008]  eta: 0:05:08  lr: 0.000500  loss_labels: 1.0161 (1.0933)  loss: 0.9570 (1.0933)  time: 0.5621\n",
      "Epoch: [5]  [3200/5008]  eta: 0:04:53  lr: 0.000500  loss_labels: 0.9963 (1.0908)  loss: 1.0481 (1.0908)  time: 0.1314\n",
      "Epoch: [5]  [3300/5008]  eta: 0:04:35  lr: 0.000500  loss_labels: 1.0124 (1.0888)  loss: 1.0124 (1.0888)  time: 0.1275\n",
      "Epoch: [5]  [3400/5008]  eta: 0:04:17  lr: 0.000500  loss_labels: 1.0058 (1.0862)  loss: 0.9577 (1.0862)  time: 0.1356\n",
      "Epoch: [5]  [3500/5008]  eta: 0:04:01  lr: 0.000500  loss_labels: 1.0001 (1.0839)  loss: 1.0445 (1.0839)  time: 0.1359\n",
      "Epoch: [5]  [3600/5008]  eta: 0:03:45  lr: 0.000500  loss_labels: 1.0098 (1.0820)  loss: 0.9809 (1.0820)  time: 0.1440\n",
      "Epoch: [5]  [3700/5008]  eta: 0:03:28  lr: 0.000500  loss_labels: 0.9767 (1.0794)  loss: 0.9625 (1.0794)  time: 0.1382\n",
      "Epoch: [5]  [3800/5008]  eta: 0:03:11  lr: 0.000500  loss_labels: 1.0138 (1.0776)  loss: 1.0237 (1.0776)  time: 0.1387\n",
      "Epoch: [5]  [3900/5008]  eta: 0:02:55  lr: 0.000500  loss_labels: 1.0087 (1.0759)  loss: 1.0331 (1.0759)  time: 0.1327\n",
      "Epoch: [5]  [4000/5008]  eta: 0:02:38  lr: 0.000500  loss_labels: 0.9781 (1.0735)  loss: 1.0044 (1.0735)  time: 0.1304\n",
      "Epoch: [5]  [4100/5008]  eta: 0:02:22  lr: 0.000500  loss_labels: 0.9736 (1.0711)  loss: 0.9739 (1.0711)  time: 0.1326\n",
      "Epoch: [5]  [4200/5008]  eta: 0:02:06  lr: 0.000500  loss_labels: 0.9686 (1.0689)  loss: 1.0020 (1.0689)  time: 0.1238\n",
      "Epoch: [5]  [4300/5008]  eta: 0:01:51  lr: 0.000500  loss_labels: 0.9517 (1.0664)  loss: 0.9466 (1.0664)  time: 0.1361\n",
      "Epoch: [5]  [4400/5008]  eta: 0:01:34  lr: 0.000500  loss_labels: 1.0050 (1.0648)  loss: 0.9594 (1.0648)  time: 0.1296\n",
      "Epoch: [5]  [4500/5008]  eta: 0:01:19  lr: 0.000500  loss_labels: 0.9694 (1.0626)  loss: 0.9795 (1.0626)  time: 0.1255\n",
      "Epoch: [5]  [4600/5008]  eta: 0:01:03  lr: 0.000500  loss_labels: 1.0081 (1.0612)  loss: 1.0081 (1.0612)  time: 0.1340\n",
      "Epoch: [5]  [4700/5008]  eta: 0:00:47  lr: 0.000500  loss_labels: 0.9773 (1.0592)  loss: 0.9888 (1.0592)  time: 0.1417\n",
      "Epoch: [5]  [4800/5008]  eta: 0:00:32  lr: 0.000500  loss_labels: 0.9505 (1.0573)  loss: 1.0683 (1.0573)  time: 0.1261\n",
      "Epoch: [5]  [4900/5008]  eta: 0:00:16  lr: 0.000500  loss_labels: 0.9340 (1.0550)  loss: 0.8520 (1.0550)  time: 0.1297\n",
      "Epoch: [5]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.9431 (1.0528)  loss: 0.9864 (1.0528)  time: 0.1257\n",
      "Epoch: [5]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.9385 (1.0527)  loss: 0.9287 (1.0527)  time: 0.1263\n",
      "Epoch: [5] Total time: 0:12:47 (0.1533 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.9385 (1.0527)  loss: 0.9287 (1.0527)\n",
      "Test:  [  0/565]  eta: 2:46:33  loss_labels: 1.9829 (1.9829)  loss: 1.9829 (1.9829)  time: 17.6873\n",
      "Test:  [100/565]  eta: 0:02:21  loss_labels: 2.3460 (2.4664)  loss: 2.4992 (2.4664)  time: 0.1267\n",
      "Test:  [200/565]  eta: 0:01:19  loss_labels: 2.3011 (2.4170)  loss: 2.4252 (2.4170)  time: 0.1229\n",
      "Test:  [300/565]  eta: 0:00:50  loss_labels: 2.0996 (2.3399)  loss: 1.9947 (2.3399)  time: 0.1411\n",
      "Test:  [400/565]  eta: 0:00:28  loss_labels: 2.3790 (2.3681)  loss: 2.3338 (2.3681)  time: 0.1170\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 2.0650 (2.3260)  loss: 2.3828 (2.3260)  time: 0.1163\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.1614 (2.3128)  loss: 2.5907 (2.3128)  time: 0.1341\n",
      "Test: Total time: 0:01:30 (0.1602 s / it)\n",
      "Averaged stats: loss_labels: 2.1614 (2.3128)  loss: 2.5907 (2.3128)\n",
      "acc: 0.5579266548156738\n",
      "top 1 and top 5 accuracies {'top1': 0.5579266603888551, 'top5': 0.7357502908103917, 'loss': tensor(0.0181, device='cuda:0')}\n",
      "Epoch: [6]  [   0/5008]  eta: 1 day, 7:18:17  lr: 0.000500  loss_labels: 1.0413 (1.0413)  loss: 1.0413 (1.0413)  time: 22.5034\n",
      "Epoch: [6]  [ 100/5008]  eta: 0:30:26  lr: 0.000500  loss_labels: 0.9253 (0.9401)  loss: 0.8847 (0.9401)  time: 0.1323\n",
      "Epoch: [6]  [ 200/5008]  eta: 0:20:18  lr: 0.000500  loss_labels: 0.9298 (0.9421)  loss: 0.9106 (0.9421)  time: 0.1305\n",
      "Epoch: [6]  [ 300/5008]  eta: 0:17:16  lr: 0.000500  loss_labels: 0.9372 (0.9373)  loss: 1.0028 (0.9373)  time: 0.1312\n",
      "Epoch: [6]  [ 400/5008]  eta: 0:15:57  lr: 0.000500  loss_labels: 0.9330 (0.9380)  loss: 0.9330 (0.9380)  time: 0.1315\n",
      "Epoch: [6]  [ 500/5008]  eta: 0:14:23  lr: 0.000500  loss_labels: 0.9218 (0.9385)  loss: 0.9125 (0.9385)  time: 0.1220\n",
      "Epoch: [6]  [ 600/5008]  eta: 0:13:15  lr: 0.000500  loss_labels: 0.9075 (0.9336)  loss: 0.8435 (0.9336)  time: 0.1282\n",
      "Epoch: [6]  [ 700/5008]  eta: 0:12:22  lr: 0.000500  loss_labels: 0.9090 (0.9311)  loss: 0.9207 (0.9311)  time: 0.1219\n",
      "Epoch: [6]  [ 800/5008]  eta: 0:11:38  lr: 0.000500  loss_labels: 0.9012 (0.9308)  loss: 0.9233 (0.9308)  time: 0.1191\n",
      "Epoch: [6]  [ 900/5008]  eta: 0:11:08  lr: 0.000500  loss_labels: 0.9009 (0.9294)  loss: 0.8904 (0.9294)  time: 0.1268\n",
      "Epoch: [6]  [1000/5008]  eta: 0:10:54  lr: 0.000500  loss_labels: 0.9071 (0.9271)  loss: 0.8986 (0.9271)  time: 0.2524\n",
      "Epoch: [6]  [1100/5008]  eta: 0:11:06  lr: 0.000500  loss_labels: 0.8956 (0.9242)  loss: 0.8179 (0.9242)  time: 0.4027\n",
      "Epoch: [6]  [1200/5008]  eta: 0:10:54  lr: 0.000500  loss_labels: 0.9278 (0.9239)  loss: 0.9157 (0.9239)  time: 0.2019\n",
      "Epoch: [6]  [1300/5008]  eta: 0:10:26  lr: 0.000500  loss_labels: 0.8935 (0.9229)  loss: 0.8434 (0.9229)  time: 0.1331\n",
      "Epoch: [6]  [1400/5008]  eta: 0:09:59  lr: 0.000500  loss_labels: 0.8765 (0.9204)  loss: 0.8656 (0.9204)  time: 0.1324\n",
      "Epoch: [6]  [1500/5008]  eta: 0:09:34  lr: 0.000500  loss_labels: 0.9112 (0.9204)  loss: 0.8344 (0.9204)  time: 0.1221\n",
      "Epoch: [6]  [1600/5008]  eta: 0:09:09  lr: 0.000500  loss_labels: 0.9051 (0.9191)  loss: 0.8592 (0.9191)  time: 0.1233\n",
      "Epoch: [6]  [1700/5008]  eta: 0:08:54  lr: 0.000500  loss_labels: 0.8882 (0.9174)  loss: 0.8790 (0.9174)  time: 0.3354\n",
      "Epoch: [6]  [1800/5008]  eta: 0:08:32  lr: 0.000500  loss_labels: 0.8702 (0.9154)  loss: 0.8857 (0.9154)  time: 0.1342\n",
      "Epoch: [6]  [1900/5008]  eta: 0:08:11  lr: 0.000500  loss_labels: 0.8489 (0.9129)  loss: 0.8597 (0.9129)  time: 0.1272\n",
      "Epoch: [6]  [2000/5008]  eta: 0:07:50  lr: 0.000500  loss_labels: 0.8845 (0.9121)  loss: 0.8066 (0.9121)  time: 0.1246\n",
      "Epoch: [6]  [2100/5008]  eta: 0:07:53  lr: 0.000500  loss_labels: 0.8402 (0.9096)  loss: 0.7961 (0.9096)  time: 0.5180\n",
      "Epoch: [6]  [2200/5008]  eta: 0:07:35  lr: 0.000500  loss_labels: 0.8747 (0.9075)  loss: 0.8754 (0.9075)  time: 0.1233\n",
      "Epoch: [6]  [2300/5008]  eta: 0:07:15  lr: 0.000500  loss_labels: 0.8865 (0.9072)  loss: 0.8506 (0.9072)  time: 0.1287\n",
      "Epoch: [6]  [2400/5008]  eta: 0:06:56  lr: 0.000500  loss_labels: 0.8696 (0.9058)  loss: 0.9194 (0.9058)  time: 0.1230\n",
      "Epoch: [6]  [2500/5008]  eta: 0:06:37  lr: 0.000500  loss_labels: 0.8695 (0.9041)  loss: 0.8318 (0.9041)  time: 0.1366\n",
      "Epoch: [6]  [2600/5008]  eta: 0:06:19  lr: 0.000500  loss_labels: 0.8661 (0.9027)  loss: 0.8039 (0.9027)  time: 0.1388\n",
      "Epoch: [6]  [2700/5008]  eta: 0:06:01  lr: 0.000500  loss_labels: 0.8488 (0.9009)  loss: 0.7618 (0.9009)  time: 0.1353\n",
      "Epoch: [6]  [2800/5008]  eta: 0:05:44  lr: 0.000500  loss_labels: 0.8359 (0.8990)  loss: 0.8271 (0.8990)  time: 0.1333\n",
      "Epoch: [6]  [2900/5008]  eta: 0:05:27  lr: 0.000500  loss_labels: 0.8261 (0.8973)  loss: 0.8143 (0.8973)  time: 0.1317\n",
      "Epoch: [6]  [3000/5008]  eta: 0:05:10  lr: 0.000500  loss_labels: 0.8597 (0.8960)  loss: 0.8285 (0.8960)  time: 0.1318\n",
      "Epoch: [6]  [3100/5008]  eta: 0:04:53  lr: 0.000500  loss_labels: 0.8517 (0.8948)  loss: 0.8061 (0.8948)  time: 0.1258\n",
      "Epoch: [6]  [3200/5008]  eta: 0:04:36  lr: 0.000500  loss_labels: 0.8155 (0.8929)  loss: 0.8377 (0.8929)  time: 0.1328\n",
      "Epoch: [6]  [3300/5008]  eta: 0:04:20  lr: 0.000500  loss_labels: 0.8509 (0.8918)  loss: 0.8354 (0.8918)  time: 0.1381\n",
      "Epoch: [6]  [3400/5008]  eta: 0:04:03  lr: 0.000500  loss_labels: 0.8173 (0.8896)  loss: 0.8088 (0.8896)  time: 0.1309\n",
      "Epoch: [6]  [3500/5008]  eta: 0:03:47  lr: 0.000500  loss_labels: 0.8321 (0.8881)  loss: 0.9024 (0.8881)  time: 0.1306\n",
      "Epoch: [6]  [3600/5008]  eta: 0:03:31  lr: 0.000500  loss_labels: 0.8332 (0.8868)  loss: 0.8014 (0.8868)  time: 0.1248\n",
      "Epoch: [6]  [3700/5008]  eta: 0:03:16  lr: 0.000500  loss_labels: 0.8207 (0.8849)  loss: 0.8435 (0.8849)  time: 0.1314\n",
      "Epoch: [6]  [3800/5008]  eta: 0:03:00  lr: 0.000500  loss_labels: 0.8195 (0.8834)  loss: 0.8631 (0.8834)  time: 0.1299\n",
      "Epoch: [6]  [3900/5008]  eta: 0:02:45  lr: 0.000500  loss_labels: 0.8358 (0.8823)  loss: 0.8436 (0.8823)  time: 0.1258\n",
      "Epoch: [6]  [4000/5008]  eta: 0:02:29  lr: 0.000500  loss_labels: 0.7857 (0.8803)  loss: 0.7837 (0.8803)  time: 0.1298\n",
      "Epoch: [6]  [4100/5008]  eta: 0:02:14  lr: 0.000500  loss_labels: 0.8065 (0.8787)  loss: 0.8423 (0.8787)  time: 0.1239\n",
      "Epoch: [6]  [4200/5008]  eta: 0:01:59  lr: 0.000500  loss_labels: 0.8103 (0.8772)  loss: 0.8589 (0.8772)  time: 0.1295\n",
      "Epoch: [6]  [4300/5008]  eta: 0:01:44  lr: 0.000500  loss_labels: 0.7657 (0.8751)  loss: 0.7307 (0.8751)  time: 0.1222\n",
      "Epoch: [6]  [4400/5008]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.8223 (0.8740)  loss: 0.7783 (0.8740)  time: 0.1196\n",
      "Epoch: [6]  [4500/5008]  eta: 0:01:14  lr: 0.000500  loss_labels: 0.7950 (0.8722)  loss: 0.8091 (0.8722)  time: 0.1231\n",
      "Epoch: [6]  [4600/5008]  eta: 0:00:59  lr: 0.000500  loss_labels: 0.8207 (0.8710)  loss: 0.8207 (0.8710)  time: 0.1246\n",
      "Epoch: [6]  [4700/5008]  eta: 0:00:44  lr: 0.000500  loss_labels: 0.8028 (0.8695)  loss: 0.7947 (0.8695)  time: 0.1275\n",
      "Epoch: [6]  [4800/5008]  eta: 0:00:30  lr: 0.000500  loss_labels: 0.7748 (0.8679)  loss: 0.8752 (0.8679)  time: 0.1329\n",
      "Epoch: [6]  [4900/5008]  eta: 0:00:15  lr: 0.000500  loss_labels: 0.7650 (0.8662)  loss: 0.7195 (0.8662)  time: 0.1309\n",
      "Epoch: [6]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.7911 (0.8646)  loss: 0.8347 (0.8646)  time: 0.1297\n",
      "Epoch: [6]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7897 (0.8645)  loss: 0.7956 (0.8645)  time: 0.1253\n",
      "Epoch: [6] Total time: 0:12:04 (0.1446 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7897 (0.8645)  loss: 0.7956 (0.8645)\n",
      "Test:  [  0/565]  eta: 2:43:42  loss_labels: 2.0804 (2.0804)  loss: 2.0804 (2.0804)  time: 17.3853\n",
      "Test:  [100/565]  eta: 0:02:19  loss_labels: 2.2902 (2.3837)  loss: 2.5038 (2.3837)  time: 0.1198\n",
      "Test:  [200/565]  eta: 0:01:17  loss_labels: 2.1592 (2.3298)  loss: 2.4230 (2.3298)  time: 0.1179\n",
      "Test:  [300/565]  eta: 0:00:48  loss_labels: 1.9401 (2.2527)  loss: 1.8579 (2.2527)  time: 0.1324\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 2.2805 (2.2777)  loss: 2.2197 (2.2777)  time: 0.1139\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 1.9975 (2.2397)  loss: 2.4391 (2.2397)  time: 0.1114\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.0828 (2.2243)  loss: 2.3789 (2.2243)  time: 0.1388\n",
      "Test: Total time: 0:01:28 (0.1569 s / it)\n",
      "Averaged stats: loss_labels: 2.0828 (2.2243)  loss: 2.3789 (2.2243)\n",
      "acc: 0.5763862133026123\n",
      "top 1 and top 5 accuracies {'top1': 0.5763861962000776, 'top5': 0.7522295463357891, 'loss': tensor(0.0174, device='cuda:0')}\n",
      "Epoch: [7]  [   0/5008]  eta: 1 day, 6:14:55  lr: 0.000500  loss_labels: 0.9761 (0.9761)  loss: 0.9761 (0.9761)  time: 21.7444\n",
      "Epoch: [7]  [ 100/5008]  eta: 0:28:50  lr: 0.000500  loss_labels: 0.7827 (0.7965)  loss: 0.7461 (0.7965)  time: 0.1350\n",
      "Epoch: [7]  [ 200/5008]  eta: 0:19:22  lr: 0.000500  loss_labels: 0.7971 (0.7908)  loss: 0.7277 (0.7908)  time: 0.1315\n",
      "Epoch: [7]  [ 300/5008]  eta: 0:15:55  lr: 0.000500  loss_labels: 0.7713 (0.7852)  loss: 0.8231 (0.7852)  time: 0.1251\n",
      "Epoch: [7]  [ 400/5008]  eta: 0:14:08  lr: 0.000500  loss_labels: 0.7770 (0.7839)  loss: 0.7770 (0.7839)  time: 0.1281\n",
      "Epoch: [7]  [ 500/5008]  eta: 0:13:07  lr: 0.000500  loss_labels: 0.7834 (0.7851)  loss: 0.7834 (0.7851)  time: 0.1433\n",
      "Epoch: [7]  [ 600/5008]  eta: 0:12:13  lr: 0.000500  loss_labels: 0.7501 (0.7800)  loss: 0.6947 (0.7800)  time: 0.1319\n",
      "Epoch: [7]  [ 700/5008]  eta: 0:11:34  lr: 0.000500  loss_labels: 0.7582 (0.7782)  loss: 0.7540 (0.7782)  time: 0.1270\n",
      "Epoch: [7]  [ 800/5008]  eta: 0:11:00  lr: 0.000500  loss_labels: 0.7543 (0.7777)  loss: 0.7516 (0.7777)  time: 0.1283\n",
      "Epoch: [7]  [ 900/5008]  eta: 0:10:31  lr: 0.000500  loss_labels: 0.7372 (0.7744)  loss: 0.7222 (0.7744)  time: 0.1258\n",
      "Epoch: [7]  [1000/5008]  eta: 0:10:05  lr: 0.000500  loss_labels: 0.7483 (0.7720)  loss: 0.7326 (0.7720)  time: 0.1260\n",
      "Epoch: [7]  [1100/5008]  eta: 0:09:41  lr: 0.000500  loss_labels: 0.7416 (0.7692)  loss: 0.6959 (0.7692)  time: 0.1334\n",
      "Epoch: [7]  [1200/5008]  eta: 0:09:20  lr: 0.000500  loss_labels: 0.7400 (0.7678)  loss: 0.7784 (0.7678)  time: 0.1264\n",
      "Epoch: [7]  [1300/5008]  eta: 0:09:00  lr: 0.000500  loss_labels: 0.7278 (0.7661)  loss: 0.6664 (0.7661)  time: 0.1322\n",
      "Epoch: [7]  [1400/5008]  eta: 0:08:41  lr: 0.000500  loss_labels: 0.7451 (0.7639)  loss: 0.7228 (0.7639)  time: 0.1307\n",
      "Epoch: [7]  [1500/5008]  eta: 0:08:23  lr: 0.000500  loss_labels: 0.7506 (0.7636)  loss: 0.6999 (0.7636)  time: 0.1231\n",
      "Epoch: [7]  [1600/5008]  eta: 0:08:05  lr: 0.000500  loss_labels: 0.7297 (0.7621)  loss: 0.7322 (0.7621)  time: 0.1253\n",
      "Epoch: [7]  [1700/5008]  eta: 0:07:48  lr: 0.000500  loss_labels: 0.7221 (0.7600)  loss: 0.6784 (0.7600)  time: 0.1231\n",
      "Epoch: [7]  [1800/5008]  eta: 0:07:31  lr: 0.000500  loss_labels: 0.7110 (0.7580)  loss: 0.7041 (0.7580)  time: 0.1275\n",
      "Epoch: [7]  [1900/5008]  eta: 0:07:14  lr: 0.000500  loss_labels: 0.7200 (0.7563)  loss: 0.7635 (0.7563)  time: 0.1251\n",
      "Epoch: [7]  [2000/5008]  eta: 0:06:58  lr: 0.000500  loss_labels: 0.7087 (0.7553)  loss: 0.6915 (0.7553)  time: 0.1259\n",
      "Epoch: [7]  [2100/5008]  eta: 0:06:42  lr: 0.000500  loss_labels: 0.7109 (0.7536)  loss: 0.6691 (0.7536)  time: 0.1205\n",
      "Epoch: [7]  [2200/5008]  eta: 0:06:28  lr: 0.000500  loss_labels: 0.7261 (0.7521)  loss: 0.7318 (0.7521)  time: 0.1470\n",
      "Epoch: [7]  [2300/5008]  eta: 0:06:14  lr: 0.000500  loss_labels: 0.7427 (0.7525)  loss: 0.7138 (0.7525)  time: 0.1322\n",
      "Epoch: [7]  [2400/5008]  eta: 0:05:59  lr: 0.000500  loss_labels: 0.7407 (0.7521)  loss: 0.7437 (0.7521)  time: 0.1303\n",
      "Epoch: [7]  [2500/5008]  eta: 0:05:44  lr: 0.000500  loss_labels: 0.7150 (0.7509)  loss: 0.6863 (0.7509)  time: 0.1296\n",
      "Epoch: [7]  [2600/5008]  eta: 0:05:30  lr: 0.000500  loss_labels: 0.7152 (0.7499)  loss: 0.6766 (0.7499)  time: 0.1237\n",
      "Epoch: [7]  [2700/5008]  eta: 0:05:16  lr: 0.000500  loss_labels: 0.7081 (0.7485)  loss: 0.6562 (0.7485)  time: 0.1231\n",
      "Epoch: [7]  [2800/5008]  eta: 0:05:02  lr: 0.000500  loss_labels: 0.7044 (0.7473)  loss: 0.7070 (0.7473)  time: 0.1211\n",
      "Epoch: [7]  [2900/5008]  eta: 0:04:47  lr: 0.000500  loss_labels: 0.7038 (0.7460)  loss: 0.7186 (0.7460)  time: 0.1303\n",
      "Epoch: [7]  [3000/5008]  eta: 0:04:33  lr: 0.000500  loss_labels: 0.7011 (0.7449)  loss: 0.6865 (0.7449)  time: 0.1324\n",
      "Epoch: [7]  [3100/5008]  eta: 0:04:20  lr: 0.000500  loss_labels: 0.7138 (0.7440)  loss: 0.6256 (0.7440)  time: 0.1274\n",
      "Epoch: [7]  [3200/5008]  eta: 0:04:07  lr: 0.000500  loss_labels: 0.6732 (0.7425)  loss: 0.7284 (0.7425)  time: 0.1275\n",
      "Epoch: [7]  [3300/5008]  eta: 0:03:53  lr: 0.000500  loss_labels: 0.7139 (0.7417)  loss: 0.6924 (0.7417)  time: 0.1292\n",
      "Epoch: [7]  [3400/5008]  eta: 0:03:39  lr: 0.000500  loss_labels: 0.6507 (0.7397)  loss: 0.6476 (0.7397)  time: 0.1288\n",
      "Epoch: [7]  [3500/5008]  eta: 0:03:25  lr: 0.000500  loss_labels: 0.6982 (0.7385)  loss: 0.7604 (0.7385)  time: 0.1260\n",
      "Epoch: [7]  [3600/5008]  eta: 0:03:12  lr: 0.000500  loss_labels: 0.7072 (0.7380)  loss: 0.6633 (0.7380)  time: 0.1283\n",
      "Epoch: [7]  [3700/5008]  eta: 0:02:58  lr: 0.000500  loss_labels: 0.6877 (0.7365)  loss: 0.7106 (0.7365)  time: 0.1403\n",
      "Epoch: [7]  [3800/5008]  eta: 0:02:44  lr: 0.000500  loss_labels: 0.6821 (0.7353)  loss: 0.7019 (0.7353)  time: 0.1297\n",
      "Epoch: [7]  [3900/5008]  eta: 0:02:30  lr: 0.000500  loss_labels: 0.6885 (0.7345)  loss: 0.6578 (0.7345)  time: 0.1266\n",
      "Epoch: [7]  [4000/5008]  eta: 0:02:16  lr: 0.000500  loss_labels: 0.6619 (0.7330)  loss: 0.6754 (0.7330)  time: 0.1282\n",
      "Epoch: [7]  [4100/5008]  eta: 0:02:03  lr: 0.000500  loss_labels: 0.6854 (0.7316)  loss: 0.6970 (0.7316)  time: 0.1306\n",
      "Epoch: [7]  [4200/5008]  eta: 0:01:49  lr: 0.000500  loss_labels: 0.6946 (0.7305)  loss: 0.7030 (0.7305)  time: 0.1301\n",
      "Epoch: [7]  [4300/5008]  eta: 0:01:35  lr: 0.000500  loss_labels: 0.6486 (0.7290)  loss: 0.6432 (0.7290)  time: 0.1328\n",
      "Epoch: [7]  [4400/5008]  eta: 0:01:22  lr: 0.000500  loss_labels: 0.6928 (0.7282)  loss: 0.6872 (0.7282)  time: 0.1601\n",
      "Epoch: [7]  [4500/5008]  eta: 0:01:10  lr: 0.000500  loss_labels: 0.6631 (0.7269)  loss: 0.6583 (0.7269)  time: 0.3439\n",
      "Epoch: [7]  [4600/5008]  eta: 0:00:56  lr: 0.000500  loss_labels: 0.6742 (0.7259)  loss: 0.6744 (0.7259)  time: 0.1310\n",
      "Epoch: [7]  [4700/5008]  eta: 0:00:42  lr: 0.000500  loss_labels: 0.6472 (0.7247)  loss: 0.6409 (0.7247)  time: 0.1266\n",
      "Epoch: [7]  [4800/5008]  eta: 0:00:28  lr: 0.000500  loss_labels: 0.6821 (0.7237)  loss: 0.7449 (0.7237)  time: 0.1257\n",
      "Epoch: [7]  [4900/5008]  eta: 0:00:15  lr: 0.000500  loss_labels: 0.6530 (0.7224)  loss: 0.6326 (0.7224)  time: 0.1244\n",
      "Epoch: [7]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.6622 (0.7212)  loss: 0.6874 (0.7212)  time: 0.1280\n",
      "Epoch: [7]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.6622 (0.7211)  loss: 0.6798 (0.7211)  time: 0.1285\n",
      "Epoch: [7] Total time: 0:11:35 (0.1389 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.6622 (0.7211)  loss: 0.6798 (0.7211)\n",
      "Test:  [  0/565]  eta: 2:45:42  loss_labels: 2.0791 (2.0791)  loss: 2.0791 (2.0791)  time: 17.5965\n",
      "Test:  [100/565]  eta: 0:02:19  loss_labels: 2.2582 (2.3992)  loss: 2.4027 (2.3992)  time: 0.1287\n",
      "Test:  [200/565]  eta: 0:01:17  loss_labels: 2.2039 (2.3504)  loss: 2.4880 (2.3504)  time: 0.1250\n",
      "Test:  [300/565]  eta: 0:00:48  loss_labels: 1.9766 (2.2738)  loss: 1.8058 (2.2738)  time: 0.1278\n",
      "Test:  [400/565]  eta: 0:00:27  loss_labels: 2.3548 (2.3064)  loss: 2.3627 (2.3064)  time: 0.1158\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 2.0336 (2.2700)  loss: 2.3621 (2.2700)  time: 0.3080\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.1288 (2.2574)  loss: 2.4376 (2.2574)  time: 0.3291\n",
      "Test: Total time: 0:01:48 (0.1926 s / it)\n",
      "Averaged stats: loss_labels: 2.1288 (2.2574)  loss: 2.4376 (2.2574)\n",
      "acc: 0.5773278474807739\n",
      "top 1 and top 5 accuracies {'top1': 0.577327867944386, 'top5': 0.7539882567994239, 'loss': tensor(0.0177, device='cuda:0')}\n",
      "Epoch: [8]  [   0/5008]  eta: 1 day, 6:19:35  lr: 0.000500  loss_labels: 0.8430 (0.8430)  loss: 0.8430 (0.8430)  time: 21.8002\n",
      "Epoch: [8]  [ 100/5008]  eta: 0:29:29  lr: 0.000500  loss_labels: 0.6442 (0.6630)  loss: 0.5961 (0.6630)  time: 0.1193\n",
      "Epoch: [8]  [ 200/5008]  eta: 0:19:24  lr: 0.000500  loss_labels: 0.6546 (0.6629)  loss: 0.6021 (0.6629)  time: 0.1261\n",
      "Epoch: [8]  [ 300/5008]  eta: 0:16:01  lr: 0.000500  loss_labels: 0.6434 (0.6610)  loss: 0.7382 (0.6610)  time: 0.1309\n",
      "Epoch: [8]  [ 400/5008]  eta: 0:14:20  lr: 0.000500  loss_labels: 0.6538 (0.6592)  loss: 0.6585 (0.6592)  time: 0.1340\n",
      "Epoch: [8]  [ 500/5008]  eta: 0:13:08  lr: 0.000500  loss_labels: 0.6489 (0.6613)  loss: 0.7129 (0.6613)  time: 0.1259\n",
      "Epoch: [8]  [ 600/5008]  eta: 0:12:17  lr: 0.000500  loss_labels: 0.6239 (0.6549)  loss: 0.5939 (0.6549)  time: 0.1262\n",
      "Epoch: [8]  [ 700/5008]  eta: 0:11:40  lr: 0.000500  loss_labels: 0.6361 (0.6526)  loss: 0.6030 (0.6526)  time: 0.1428\n",
      "Epoch: [8]  [ 800/5008]  eta: 0:11:07  lr: 0.000500  loss_labels: 0.6453 (0.6529)  loss: 0.6453 (0.6529)  time: 0.1348\n",
      "Epoch: [8]  [ 900/5008]  eta: 0:10:38  lr: 0.000500  loss_labels: 0.6286 (0.6506)  loss: 0.6127 (0.6506)  time: 0.1311\n",
      "Epoch: [8]  [1000/5008]  eta: 0:10:16  lr: 0.000500  loss_labels: 0.6277 (0.6482)  loss: 0.6545 (0.6482)  time: 0.1387\n",
      "Epoch: [8]  [1100/5008]  eta: 0:09:54  lr: 0.000500  loss_labels: 0.6167 (0.6457)  loss: 0.6273 (0.6457)  time: 0.1346\n",
      "Epoch: [8]  [1200/5008]  eta: 0:09:34  lr: 0.000500  loss_labels: 0.6226 (0.6438)  loss: 0.6451 (0.6438)  time: 0.1356\n",
      "Epoch: [8]  [1300/5008]  eta: 0:09:14  lr: 0.000500  loss_labels: 0.6008 (0.6423)  loss: 0.5940 (0.6423)  time: 0.1330\n",
      "Epoch: [8]  [1400/5008]  eta: 0:08:54  lr: 0.000500  loss_labels: 0.6199 (0.6410)  loss: 0.6311 (0.6410)  time: 0.1348\n",
      "Epoch: [8]  [1500/5008]  eta: 0:08:36  lr: 0.000500  loss_labels: 0.6295 (0.6407)  loss: 0.5966 (0.6407)  time: 0.1290\n",
      "Epoch: [8]  [1600/5008]  eta: 0:08:18  lr: 0.000500  loss_labels: 0.6064 (0.6397)  loss: 0.5571 (0.6397)  time: 0.1263\n",
      "Epoch: [8]  [1700/5008]  eta: 0:08:01  lr: 0.000500  loss_labels: 0.6145 (0.6384)  loss: 0.5925 (0.6384)  time: 0.1356\n",
      "Epoch: [8]  [1800/5008]  eta: 0:07:45  lr: 0.000500  loss_labels: 0.5914 (0.6372)  loss: 0.5426 (0.6372)  time: 0.1364\n",
      "Epoch: [8]  [1900/5008]  eta: 0:07:28  lr: 0.000500  loss_labels: 0.6071 (0.6357)  loss: 0.6436 (0.6357)  time: 0.1308\n",
      "Epoch: [8]  [2000/5008]  eta: 0:07:13  lr: 0.000500  loss_labels: 0.6093 (0.6350)  loss: 0.6085 (0.6350)  time: 0.1309\n",
      "Epoch: [8]  [2100/5008]  eta: 0:06:56  lr: 0.000500  loss_labels: 0.5889 (0.6338)  loss: 0.5366 (0.6338)  time: 0.1248\n",
      "Epoch: [8]  [2200/5008]  eta: 0:06:40  lr: 0.000500  loss_labels: 0.6000 (0.6327)  loss: 0.6107 (0.6327)  time: 0.1247\n",
      "Epoch: [8]  [2300/5008]  eta: 0:06:24  lr: 0.000500  loss_labels: 0.6305 (0.6333)  loss: 0.5909 (0.6333)  time: 0.1371\n",
      "Epoch: [8]  [2400/5008]  eta: 0:06:08  lr: 0.000500  loss_labels: 0.6163 (0.6325)  loss: 0.5915 (0.6325)  time: 0.1332\n",
      "Epoch: [8]  [2500/5008]  eta: 0:05:55  lr: 0.000500  loss_labels: 0.6233 (0.6318)  loss: 0.6233 (0.6318)  time: 0.1256\n",
      "Epoch: [8]  [2600/5008]  eta: 0:05:39  lr: 0.000500  loss_labels: 0.6083 (0.6311)  loss: 0.5461 (0.6311)  time: 0.1178\n",
      "Epoch: [8]  [2700/5008]  eta: 0:05:23  lr: 0.000500  loss_labels: 0.5782 (0.6300)  loss: 0.5396 (0.6300)  time: 0.1224\n",
      "Epoch: [8]  [2800/5008]  eta: 0:05:09  lr: 0.000500  loss_labels: 0.6064 (0.6292)  loss: 0.6078 (0.6292)  time: 0.1227\n",
      "Epoch: [8]  [2900/5008]  eta: 0:04:55  lr: 0.000500  loss_labels: 0.6029 (0.6286)  loss: 0.6130 (0.6286)  time: 0.1295\n",
      "Epoch: [8]  [3000/5008]  eta: 0:04:41  lr: 0.000500  loss_labels: 0.5862 (0.6277)  loss: 0.5889 (0.6277)  time: 0.1381\n",
      "Epoch: [8]  [3100/5008]  eta: 0:04:26  lr: 0.000500  loss_labels: 0.5842 (0.6268)  loss: 0.5182 (0.6268)  time: 0.1400\n",
      "Epoch: [8]  [3200/5008]  eta: 0:04:12  lr: 0.000500  loss_labels: 0.5730 (0.6257)  loss: 0.6088 (0.6257)  time: 0.1294\n",
      "Epoch: [8]  [3300/5008]  eta: 0:03:58  lr: 0.000500  loss_labels: 0.6019 (0.6251)  loss: 0.5518 (0.6251)  time: 0.1661\n",
      "Epoch: [8]  [3400/5008]  eta: 0:03:44  lr: 0.000500  loss_labels: 0.5602 (0.6235)  loss: 0.5800 (0.6235)  time: 0.1387\n",
      "Epoch: [8]  [3500/5008]  eta: 0:03:30  lr: 0.000500  loss_labels: 0.5825 (0.6225)  loss: 0.6520 (0.6225)  time: 0.1317\n",
      "Epoch: [8]  [3600/5008]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.5886 (0.6219)  loss: 0.5871 (0.6219)  time: 0.1350\n",
      "Epoch: [8]  [3700/5008]  eta: 0:03:01  lr: 0.000500  loss_labels: 0.5727 (0.6207)  loss: 0.6081 (0.6207)  time: 0.1269\n",
      "Epoch: [8]  [3800/5008]  eta: 0:02:47  lr: 0.000500  loss_labels: 0.5749 (0.6197)  loss: 0.5942 (0.6197)  time: 0.1367\n",
      "Epoch: [8]  [3900/5008]  eta: 0:02:33  lr: 0.000500  loss_labels: 0.5987 (0.6193)  loss: 0.5655 (0.6193)  time: 0.1397\n",
      "Epoch: [8]  [4000/5008]  eta: 0:02:19  lr: 0.000500  loss_labels: 0.5468 (0.6180)  loss: 0.5146 (0.6180)  time: 0.1253\n",
      "Epoch: [8]  [4100/5008]  eta: 0:02:05  lr: 0.000500  loss_labels: 0.5718 (0.6169)  loss: 0.5837 (0.6169)  time: 0.1307\n",
      "Epoch: [8]  [4200/5008]  eta: 0:01:51  lr: 0.000500  loss_labels: 0.5693 (0.6159)  loss: 0.5841 (0.6159)  time: 0.1302\n",
      "Epoch: [8]  [4300/5008]  eta: 0:01:37  lr: 0.000500  loss_labels: 0.5394 (0.6146)  loss: 0.5474 (0.6146)  time: 0.1327\n",
      "Epoch: [8]  [4400/5008]  eta: 0:01:23  lr: 0.000500  loss_labels: 0.5644 (0.6138)  loss: 0.5636 (0.6138)  time: 0.1295\n",
      "Epoch: [8]  [4500/5008]  eta: 0:01:10  lr: 0.000500  loss_labels: 0.5732 (0.6129)  loss: 0.5831 (0.6129)  time: 0.1259\n",
      "Epoch: [8]  [4600/5008]  eta: 0:00:56  lr: 0.000500  loss_labels: 0.5737 (0.6121)  loss: 0.5612 (0.6121)  time: 0.1292\n",
      "Epoch: [8]  [4700/5008]  eta: 0:00:42  lr: 0.000500  loss_labels: 0.5600 (0.6112)  loss: 0.5570 (0.6112)  time: 0.1252\n",
      "Epoch: [8]  [4800/5008]  eta: 0:00:28  lr: 0.000500  loss_labels: 0.5573 (0.6104)  loss: 0.5816 (0.6104)  time: 0.1352\n",
      "Epoch: [8]  [4900/5008]  eta: 0:00:14  lr: 0.000500  loss_labels: 0.5416 (0.6091)  loss: 0.5188 (0.6091)  time: 0.1368\n",
      "Epoch: [8]  [5000/5008]  eta: 0:00:01  lr: 0.000500  loss_labels: 0.5469 (0.6083)  loss: 0.6086 (0.6083)  time: 0.1358\n",
      "Epoch: [8]  [5007/5008]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.5482 (0.6083)  loss: 0.5777 (0.6083)  time: 0.1326\n",
      "Epoch: [8] Total time: 0:11:31 (0.1381 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.5482 (0.6083)  loss: 0.5777 (0.6083)\n",
      "Test:  [  0/565]  eta: 2:59:31  loss_labels: 2.3050 (2.3050)  loss: 2.3050 (2.3050)  time: 19.0653\n",
      "Test:  [100/565]  eta: 0:02:23  loss_labels: 2.4273 (2.5280)  loss: 2.5557 (2.5280)  time: 0.1177\n",
      "Test:  [200/565]  eta: 0:01:25  loss_labels: 2.3321 (2.4536)  loss: 2.6132 (2.4536)  time: 0.2799\n",
      "Test:  [300/565]  eta: 0:00:52  loss_labels: 2.0975 (2.3668)  loss: 1.8289 (2.3668)  time: 0.1285\n",
      "Test:  [400/565]  eta: 0:00:29  loss_labels: 2.4361 (2.4004)  loss: 2.4199 (2.4004)  time: 0.1120\n",
      "Test:  [500/565]  eta: 0:00:10  loss_labels: 2.1206 (2.3627)  loss: 2.6049 (2.3627)  time: 0.1090\n",
      "Test:  [564/565]  eta: 0:00:00  loss_labels: 2.1481 (2.3463)  loss: 2.4092 (2.3463)  time: 0.1476\n",
      "Test: Total time: 0:01:32 (0.1642 s / it)\n",
      "Averaged stats: loss_labels: 2.1481 (2.3463)  loss: 2.4092 (2.3463)\n",
      "acc: 0.5747244358062744\n",
      "top 1 and top 5 accuracies {'top1': 0.574724422533651, 'top5': 0.7503323547332853, 'loss': tensor(0.0184, device='cuda:0')}\n",
      "Epoch: [9]  [   0/5008]  eta: 1 day, 8:55:29  lr: 0.000500  loss_labels: 0.7567 (0.7567)  loss: 0.7567 (0.7567)  time: 23.6680\n",
      "Epoch: [9]  [ 100/5008]  eta: 0:30:28  lr: 0.000500  loss_labels: 0.5467 (0.5651)  loss: 0.5086 (0.5651)  time: 0.1305\n",
      "Epoch: [9]  [ 200/5008]  eta: 0:20:18  lr: 0.000500  loss_labels: 0.5557 (0.5630)  loss: 0.5043 (0.5630)  time: 0.1353\n",
      "Epoch: [9]  [ 300/5008]  eta: 0:16:46  lr: 0.000500  loss_labels: 0.5538 (0.5596)  loss: 0.6109 (0.5596)  time: 0.1342\n",
      "Epoch: [9]  [ 400/5008]  eta: 0:14:54  lr: 0.000500  loss_labels: 0.5499 (0.5601)  loss: 0.5499 (0.5601)  time: 0.1348\n",
      "Epoch: [9]  [ 500/5008]  eta: 0:13:33  lr: 0.000500  loss_labels: 0.5704 (0.5620)  loss: 0.5845 (0.5620)  time: 0.1266\n",
      "Epoch: [9]  [ 600/5008]  eta: 0:12:35  lr: 0.000500  loss_labels: 0.5136 (0.5564)  loss: 0.4954 (0.5564)  time: 0.1290\n",
      "Epoch: [9]  [ 700/5008]  eta: 0:11:52  lr: 0.000500  loss_labels: 0.5407 (0.5550)  loss: 0.4983 (0.5550)  time: 0.1295\n",
      "Epoch: [9]  [ 800/5008]  eta: 0:11:21  lr: 0.000500  loss_labels: 0.5392 (0.5547)  loss: 0.5634 (0.5547)  time: 0.1433\n",
      "Epoch: [9]  [ 900/5008]  eta: 0:10:54  lr: 0.000500  loss_labels: 0.5448 (0.5537)  loss: 0.5098 (0.5537)  time: 0.1417\n",
      "Epoch: [9]  [1000/5008]  eta: 0:10:27  lr: 0.000500  loss_labels: 0.5194 (0.5516)  loss: 0.5849 (0.5516)  time: 0.1277\n",
      "Epoch: [9]  [1100/5008]  eta: 0:10:01  lr: 0.000500  loss_labels: 0.5235 (0.5497)  loss: 0.5096 (0.5497)  time: 0.1235\n",
      "Epoch: [9]  [1200/5008]  eta: 0:09:36  lr: 0.000500  loss_labels: 0.5337 (0.5489)  loss: 0.5508 (0.5489)  time: 0.1227\n",
      "Epoch: [9]  [1300/5008]  eta: 0:09:15  lr: 0.000500  loss_labels: 0.5137 (0.5471)  loss: 0.4982 (0.5471)  time: 0.1335\n",
      "Epoch: [9]  [1400/5008]  eta: 0:08:54  lr: 0.000500  loss_labels: 0.5207 (0.5461)  loss: 0.4702 (0.5461)  time: 0.1272\n",
      "Epoch: [9]  [1500/5008]  eta: 0:08:35  lr: 0.000500  loss_labels: 0.5405 (0.5459)  loss: 0.5366 (0.5459)  time: 0.1278\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/BLT_models/main.py\", line 301, in <module>\n",
      "    mp.spawn(main, args=(args.world_size, args), nprocs=args.world_size)\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 239, in spawn\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 197, in start_processes\n",
      "    while not context.join():\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/multiprocessing/spawn.py\", line 109, in join\n",
      "    ready = multiprocessing.connection.wait(\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/multiprocessing/connection.py\", line 930, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --dataset 'vggface2' --wandb_p 'vggface2' --wandb_r 'blt_blt'  --model 'blt_blt' --epochs 100 --lr .0005 --data_path '/share/data/imagenet-pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-#-output_path './results/' --data_path '/share/data/imagenet-pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0, 12510, 25020, 37530, 50040, 62550])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "save_train_epochs=.1  # how often save output during training\n",
    "save_val_epochs=.5  # how often save output during validation\n",
    "save_model_epochs=5  # how often save model weigths\n",
    "save_model_secs=60 * 10  # how often save model (in sec)\n",
    "\n",
    "\n",
    "nsteps = 2502\n",
    "epochs = 25\n",
    "if save_train_epochs is not None:\n",
    "    save_train_steps = (np.arange(0, epochs + 1,\n",
    "                                  save_train_epochs) * nsteps).astype(int)\n",
    "if save_val_epochs is not None:\n",
    "    save_val_steps = (np.arange(0, epochs + 1,\n",
    "                                save_val_epochs) * nsteps).astype(int)\n",
    "if save_model_epochs is not None:\n",
    "    save_model_steps = (np.arange(0, epochs + 1,\n",
    "                                  save_model_epochs) * nsteps).astype(int)\n",
    "    \n",
    "save_model_steps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
