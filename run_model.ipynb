{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/transformer_brain_encoder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#switch to the directory where the code is\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training stimulus images: 8857\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 220\n",
      "Using cache found in /home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "xFormers not available\n",
      "xFormers not available\n",
      "brain_encoder(\n",
      "  (transformer): Transformer(\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (memory_proj): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (query_embed): Embedding(16, 768)\n",
      "  (backbone_model): Joiner(\n",
      "    (0): dino_model_with_hooks(\n",
      "      (backbone): DinoVisionTransformer(\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (blocks): ModuleList(\n",
      "          (0-11): 12 x NestedTensorBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): MemEffAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): LayerScale()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): LayerScale()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (head): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      "  (lh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=19004, bias=True)\n",
      "  )\n",
      "  (rh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=20544, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'transformer.memory_proj.weight', 'transformer.memory_proj.bias', 'query_embed.weight', 'lh_embed.0.weight', 'lh_embed.0.bias', 'rh_embed.0.weight', 'rh_embed.0.bias']\n",
      "Start training\n",
      "Epoch: [0]  [  0/554]  eta: 0:07:05  lr: 0.000500  loss_labels: 1.5541 (1.5541)  loss: 1.5541 (1.5541)  time: 0.7683\n",
      "Epoch: [0]  [100/554]  eta: 0:02:41  lr: 0.000500  loss_labels: 0.9773 (1.0027)  loss: 0.8802 (1.0027)  time: 0.3574\n",
      "Epoch: [0]  [200/554]  eta: 0:02:06  lr: 0.000500  loss_labels: 0.8782 (0.9475)  loss: 0.8475 (0.9475)  time: 0.3567\n",
      "Epoch: [0]  [300/554]  eta: 0:01:30  lr: 0.000500  loss_labels: 0.8401 (0.9156)  loss: 0.8198 (0.9156)  time: 0.3598\n",
      "Epoch: [0]  [400/554]  eta: 0:00:55  lr: 0.000500  loss_labels: 0.8338 (0.8953)  loss: 0.8089 (0.8953)  time: 0.3613\n",
      "Epoch: [0]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.8083 (0.8794)  loss: 0.7924 (0.8794)  time: 0.3576\n",
      "Epoch: [0]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.8083 (0.8728)  loss: 0.7917 (0.8728)  time: 0.3577\n",
      "Epoch: [0] Total time: 0:03:18 (0.3580 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.8083 (0.8728)  loss: 0.7917 (0.8728)\n",
      "Test:  [ 0/62]  eta: 0:00:59  loss_labels: 1.0837 (1.0837)  loss: 1.0837 (1.0837)  time: 0.9551\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7793 (0.8008)  loss: 0.7390 (0.8008)  time: 0.3475\n",
      "Test: Total time: 0:00:22 (0.3640 s / it)\n",
      "Averaged stats: loss_labels: 0.7793 (0.8008)  loss: 0.7390 (0.8008)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1708.17it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1708.52it/s]\n",
      "val_perf: 0.42251507078820183\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|███████████████████████████████████████████| 14/14 [00:16<00:00,  1.20s/it]\n",
      "Epoch: [1]  [  0/554]  eta: 0:03:49  lr: 0.000500  loss_labels: 0.6994 (0.6994)  loss: 0.6994 (0.6994)  time: 0.4142\n",
      "Epoch: [1]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.7819 (0.7872)  loss: 0.7674 (0.7872)  time: 0.3531\n",
      "Epoch: [1]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.7887 (0.7891)  loss: 0.7781 (0.7891)  time: 0.3501\n",
      "Epoch: [1]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.7733 (0.7891)  loss: 0.7712 (0.7891)  time: 0.3543\n",
      "Epoch: [1]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.7778 (0.7885)  loss: 0.7481 (0.7885)  time: 0.3593\n",
      "Epoch: [1]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.7584 (0.7846)  loss: 0.7528 (0.7846)  time: 0.3534\n",
      "Epoch: [1]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7631 (0.7839)  loss: 0.7751 (0.7839)  time: 0.3412\n",
      "Epoch: [1] Total time: 0:03:16 (0.3538 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7631 (0.7839)  loss: 0.7751 (0.7839)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0452 (1.0452)  loss: 1.0452 (1.0452)  time: 0.3498\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7732 (0.7955)  loss: 0.7547 (0.7955)  time: 0.3488\n",
      "Test: Total time: 0:00:21 (0.3503 s / it)\n",
      "Averaged stats: loss_labels: 0.7732 (0.7955)  loss: 0.7547 (0.7955)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1708.86it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1708.35it/s]\n",
      "val_perf: 0.43955611260966687\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|███████████████████████████████████████████| 14/14 [00:16<00:00,  1.20s/it]\n",
      "Epoch: [2]  [  0/554]  eta: 0:03:18  lr: 0.000500  loss_labels: 0.8937 (0.8937)  loss: 0.8937 (0.8937)  time: 0.3590\n",
      "Epoch: [2]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.7596 (0.7621)  loss: 0.7567 (0.7621)  time: 0.3497\n",
      "Epoch: [2]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.7443 (0.7575)  loss: 0.7252 (0.7575)  time: 0.3497\n",
      "Epoch: [2]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.7382 (0.7547)  loss: 0.7467 (0.7547)  time: 0.3500\n",
      "Epoch: [2]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.7544 (0.7570)  loss: 0.7356 (0.7570)  time: 0.3583\n",
      "Epoch: [2]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.7531 (0.7581)  loss: 0.7744 (0.7581)  time: 0.3523\n",
      "Epoch: [2]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7559 (0.7594)  loss: 0.7602 (0.7594)  time: 0.3482\n",
      "Epoch: [2] Total time: 0:03:15 (0.3535 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7559 (0.7594)  loss: 0.7602 (0.7594)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0194 (1.0194)  loss: 1.0194 (1.0194)  time: 0.3484\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7517 (0.7712)  loss: 0.7193 (0.7712)  time: 0.3456\n",
      "Test: Total time: 0:00:21 (0.3519 s / it)\n",
      "Averaged stats: loss_labels: 0.7517 (0.7712)  loss: 0.7193 (0.7712)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1684.67it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1692.63it/s]\n",
      "val_perf: 0.45162895797908653\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|███████████████████████████████████████████| 14/14 [00:16<00:00,  1.20s/it]\n",
      "Epoch: [3]  [  0/554]  eta: 0:03:57  lr: 0.000500  loss_labels: 0.7431 (0.7431)  loss: 0.7431 (0.7431)  time: 0.4293\n",
      "Epoch: [3]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.7478 (0.7492)  loss: 0.7659 (0.7492)  time: 0.3509\n",
      "Epoch: [3]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.7281 (0.7441)  loss: 0.7170 (0.7441)  time: 0.3522\n",
      "Epoch: [3]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.7491 (0.7444)  loss: 0.7469 (0.7444)  time: 0.3540\n",
      "Epoch: [3]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.7446 (0.7464)  loss: 0.7132 (0.7464)  time: 0.3529\n",
      "Epoch: [3]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.7273 (0.7447)  loss: 0.7173 (0.7447)  time: 0.3530\n",
      "Epoch: [3]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7299 (0.7462)  loss: 0.7281 (0.7462)  time: 0.3465\n",
      "Epoch: [3] Total time: 0:03:14 (0.3520 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7299 (0.7462)  loss: 0.7281 (0.7462)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 0.9954 (0.9954)  loss: 0.9954 (0.9954)  time: 0.3429\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7469 (0.7601)  loss: 0.7068 (0.7601)  time: 0.3493\n",
      "Test: Total time: 0:00:21 (0.3502 s / it)\n",
      "Averaged stats: loss_labels: 0.7469 (0.7601)  loss: 0.7068 (0.7601)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1692.02it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1688.23it/s]\n",
      "val_perf: 0.4586984235461049\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|███████████████████████████████████████████| 14/14 [00:16<00:00,  1.20s/it]\n",
      "Epoch: [4]  [  0/554]  eta: 0:03:13  lr: 0.000500  loss_labels: 0.6669 (0.6669)  loss: 0.6669 (0.6669)  time: 0.3486\n",
      "Epoch: [4]  [100/554]  eta: 0:02:38  lr: 0.000500  loss_labels: 0.7222 (0.7298)  loss: 0.7102 (0.7298)  time: 0.3499\n",
      "Epoch: [4]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.7144 (0.7255)  loss: 0.7021 (0.7255)  time: 0.3492\n",
      "Epoch: [4]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.7378 (0.7322)  loss: 0.7530 (0.7322)  time: 0.3501\n",
      "Epoch: [4]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.7334 (0.7341)  loss: 0.7245 (0.7341)  time: 0.3529\n",
      "Epoch: [4]  [500/554]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.7244 (0.7347)  loss: 0.7170 (0.7347)  time: 0.3544\n",
      "Epoch: [4]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7257 (0.7359)  loss: 0.7368 (0.7359)  time: 0.3509\n",
      "Epoch: [4] Total time: 0:03:15 (0.3521 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7257 (0.7359)  loss: 0.7368 (0.7359)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0151 (1.0151)  loss: 1.0151 (1.0151)  time: 0.3514\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7388 (0.7606)  loss: 0.6937 (0.7606)  time: 0.3482\n",
      "Test: Total time: 0:00:21 (0.3511 s / it)\n",
      "Averaged stats: loss_labels: 0.7388 (0.7606)  loss: 0.6937 (0.7606)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1682.24it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1682.44it/s]\n",
      "val_perf: 0.46127765341293925\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|███████████████████████████████████████████| 14/14 [00:16<00:00,  1.19s/it]\n",
      "Epoch: [5]  [  0/554]  eta: 0:03:45  lr: 0.000500  loss_labels: 0.6881 (0.6881)  loss: 0.6881 (0.6881)  time: 0.4070\n",
      "Epoch: [5]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.7090 (0.7238)  loss: 0.7167 (0.7238)  time: 0.3540\n",
      "Epoch: [5]  [200/554]  eta: 0:02:05  lr: 0.000500  loss_labels: 0.7213 (0.7301)  loss: 0.7203 (0.7301)  time: 0.3521\n",
      "Epoch: [5]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.7180 (0.7303)  loss: 0.7208 (0.7303)  time: 0.3564\n",
      "Epoch: [5]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.7209 (0.7290)  loss: 0.7229 (0.7290)  time: 0.3543\n",
      "Epoch: [5]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.7099 (0.7281)  loss: 0.7209 (0.7281)  time: 0.3579\n",
      "Epoch: [5]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7054 (0.7274)  loss: 0.6928 (0.7274)  time: 0.3502\n",
      "Epoch: [5] Total time: 0:03:17 (0.3558 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7054 (0.7274)  loss: 0.6928 (0.7274)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0164 (1.0164)  loss: 1.0164 (1.0164)  time: 0.3458\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7337 (0.7560)  loss: 0.7105 (0.7560)  time: 0.3513\n",
      "Test: Total time: 0:00:21 (0.3517 s / it)\n",
      "Averaged stats: loss_labels: 0.7337 (0.7560)  loss: 0.7105 (0.7560)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1685.40it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1691.49it/s]\n",
      "val_perf: 0.4625688079238587\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|███████████████████████████████████████████| 14/14 [00:16<00:00,  1.19s/it]\n",
      "Epoch: [6]  [  0/554]  eta: 0:03:39  lr: 0.000500  loss_labels: 0.7885 (0.7885)  loss: 0.7885 (0.7885)  time: 0.3969\n",
      "Epoch: [6]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.6888 (0.7006)  loss: 0.6888 (0.7006)  time: 0.3529\n",
      "Epoch: [6]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.7066 (0.7086)  loss: 0.7178 (0.7086)  time: 0.3497\n",
      "Epoch: [6]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.7237 (0.7160)  loss: 0.7311 (0.7160)  time: 0.3527\n",
      "Epoch: [6]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.7114 (0.7162)  loss: 0.6901 (0.7162)  time: 0.3778\n",
      "Epoch: [6]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.7221 (0.7177)  loss: 0.7142 (0.7177)  time: 0.3511\n",
      "Epoch: [6]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7231 (0.7194)  loss: 0.7246 (0.7194)  time: 0.3484\n",
      "Epoch: [6] Total time: 0:03:15 (0.3529 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7231 (0.7194)  loss: 0.7246 (0.7194)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0253 (1.0253)  loss: 1.0253 (1.0253)  time: 0.3528\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7363 (0.7610)  loss: 0.6944 (0.7610)  time: 0.3425\n",
      "Test: Total time: 0:00:21 (0.3492 s / it)\n",
      "Averaged stats: loss_labels: 0.7363 (0.7610)  loss: 0.6944 (0.7610)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1674.33it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1655.48it/s]\n",
      "val_perf: 0.46273846261584495\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|███████████████████████████████████████████| 14/14 [00:16<00:00,  1.20s/it]\n",
      "Epoch: [7]  [  0/554]  eta: 0:03:26  lr: 0.000500  loss_labels: 0.6334 (0.6334)  loss: 0.6334 (0.6334)  time: 0.3730\n",
      "Epoch: [7]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.7083 (0.7107)  loss: 0.7367 (0.7107)  time: 0.3536\n",
      "Epoch: [7]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.6971 (0.7100)  loss: 0.7259 (0.7100)  time: 0.3549\n",
      "Epoch: [7]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.6973 (0.7089)  loss: 0.6973 (0.7089)  time: 0.3502\n",
      "Epoch: [7]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.7121 (0.7108)  loss: 0.7144 (0.7108)  time: 0.3535\n",
      "Epoch: [7]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.7165 (0.7129)  loss: 0.7400 (0.7129)  time: 0.3529\n",
      "Epoch: [7]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7165 (0.7130)  loss: 0.6982 (0.7130)  time: 0.3453\n",
      "Epoch: [7] Total time: 0:03:15 (0.3535 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7165 (0.7130)  loss: 0.6982 (0.7130)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 0.9754 (0.9754)  loss: 0.9754 (0.9754)  time: 0.3537\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7414 (0.7628)  loss: 0.7254 (0.7628)  time: 0.3441\n",
      "Test: Total time: 0:00:21 (0.3484 s / it)\n",
      "Averaged stats: loss_labels: 0.7414 (0.7628)  loss: 0.7254 (0.7628)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1676.04it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1637.20it/s]\n",
      "val_perf: 0.4605233427779557\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [8]  [  0/554]  eta: 0:03:15  lr: 0.000500  loss_labels: 0.6908 (0.6908)  loss: 0.6908 (0.6908)  time: 0.3531\n",
      "Epoch: [8]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.6991 (0.7072)  loss: 0.6833 (0.7072)  time: 0.3499\n",
      "Epoch: [8]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.6905 (0.7033)  loss: 0.7222 (0.7033)  time: 0.3534\n",
      "Epoch: [8]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.6984 (0.7044)  loss: 0.6811 (0.7044)  time: 0.3556\n",
      "Epoch: [8]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.6931 (0.7044)  loss: 0.6567 (0.7044)  time: 0.3556\n",
      "Epoch: [8]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.6974 (0.7042)  loss: 0.6920 (0.7042)  time: 0.3582\n",
      "Epoch: [8]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.7035 (0.7057)  loss: 0.7113 (0.7057)  time: 0.3505\n",
      "Epoch: [8] Total time: 0:03:16 (0.3543 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.7035 (0.7057)  loss: 0.7113 (0.7057)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0086 (1.0086)  loss: 1.0086 (1.0086)  time: 0.3535\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7411 (0.7619)  loss: 0.7014 (0.7619)  time: 0.3464\n",
      "Test: Total time: 0:00:21 (0.3492 s / it)\n",
      "Averaged stats: loss_labels: 0.7411 (0.7619)  loss: 0.7014 (0.7619)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1679.10it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1686.53it/s]\n",
      "val_perf: 0.46015561189963383\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [9]  [  0/554]  eta: 0:03:22  lr: 0.000500  loss_labels: 0.7274 (0.7274)  loss: 0.7274 (0.7274)  time: 0.3654\n",
      "Epoch: [9]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.6710 (0.6892)  loss: 0.6478 (0.6892)  time: 0.3495\n",
      "Epoch: [9]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.7023 (0.6943)  loss: 0.7035 (0.6943)  time: 0.3540\n",
      "Epoch: [9]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.6941 (0.6962)  loss: 0.6961 (0.6962)  time: 0.3504\n",
      "Epoch: [9]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.6980 (0.6965)  loss: 0.6826 (0.6965)  time: 0.3499\n",
      "Epoch: [9]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.6987 (0.6988)  loss: 0.6813 (0.6988)  time: 0.3533\n",
      "Epoch: [9]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.6950 (0.6977)  loss: 0.7026 (0.6977)  time: 0.3444\n",
      "Epoch: [9] Total time: 0:03:15 (0.3523 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.6950 (0.6977)  loss: 0.7026 (0.6977)\n",
      "Test:  [ 0/62]  eta: 0:00:22  loss_labels: 0.9988 (0.9988)  loss: 0.9988 (0.9988)  time: 0.3603\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7376 (0.7594)  loss: 0.7152 (0.7594)  time: 0.3473\n",
      "Test: Total time: 0:00:21 (0.3497 s / it)\n",
      "Averaged stats: loss_labels: 0.7376 (0.7594)  loss: 0.7152 (0.7594)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1687.76it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1688.89it/s]\n",
      "val_perf: 0.46104524628649096\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [10]  [  0/554]  eta: 0:05:26  lr: 0.000500  loss_labels: 0.7186 (0.7186)  loss: 0.7186 (0.7186)  time: 0.5892\n",
      "Epoch: [10]  [100/554]  eta: 0:02:39  lr: 0.000500  loss_labels: 0.6713 (0.6830)  loss: 0.6926 (0.6830)  time: 0.3505\n",
      "Epoch: [10]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.6874 (0.6900)  loss: 0.6912 (0.6900)  time: 0.3508\n",
      "Epoch: [10]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.6704 (0.6889)  loss: 0.6754 (0.6889)  time: 0.3552\n",
      "Epoch: [10]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.6937 (0.6895)  loss: 0.6907 (0.6895)  time: 0.3532\n",
      "Epoch: [10]  [500/554]  eta: 0:00:19  lr: 0.000500  loss_labels: 0.6939 (0.6910)  loss: 0.6839 (0.6910)  time: 0.3561\n",
      "Epoch: [10]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.6857 (0.6908)  loss: 0.6857 (0.6908)  time: 0.3483\n",
      "Epoch: [10] Total time: 0:03:16 (0.3545 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.6857 (0.6908)  loss: 0.6857 (0.6908)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0281 (1.0281)  loss: 1.0281 (1.0281)  time: 0.3506\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7635 (0.7813)  loss: 0.7365 (0.7813)  time: 0.3453\n",
      "Test: Total time: 0:00:21 (0.3488 s / it)\n",
      "Averaged stats: loss_labels: 0.7635 (0.7813)  loss: 0.7365 (0.7813)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1679.89it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1685.91it/s]\n",
      "val_perf: 0.45388192634277125\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [11]  [  0/554]  eta: 0:03:12  lr: 0.000500  loss_labels: 0.6359 (0.6359)  loss: 0.6359 (0.6359)  time: 0.3471\n",
      "Epoch: [11]  [100/554]  eta: 0:02:38  lr: 0.000500  loss_labels: 0.6818 (0.6842)  loss: 0.6752 (0.6842)  time: 0.3497\n",
      "Epoch: [11]  [200/554]  eta: 0:02:04  lr: 0.000500  loss_labels: 0.6706 (0.6818)  loss: 0.6602 (0.6818)  time: 0.3518\n",
      "Epoch: [11]  [300/554]  eta: 0:01:29  lr: 0.000500  loss_labels: 0.6771 (0.6823)  loss: 0.6575 (0.6823)  time: 0.3511\n",
      "Epoch: [11]  [400/554]  eta: 0:00:54  lr: 0.000500  loss_labels: 0.6896 (0.6844)  loss: 0.6899 (0.6844)  time: 0.3501\n",
      "Epoch: [11]  [500/554]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.6760 (0.6844)  loss: 0.7108 (0.6844)  time: 0.3523\n",
      "Epoch: [11]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.6796 (0.6842)  loss: 0.6617 (0.6842)  time: 0.3443\n",
      "Epoch: [11] Total time: 0:03:14 (0.3513 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.6796 (0.6842)  loss: 0.6617 (0.6842)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0022 (1.0022)  loss: 1.0022 (1.0022)  time: 0.3519\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7422 (0.7647)  loss: 0.7055 (0.7647)  time: 0.3406\n",
      "Test: Total time: 0:00:21 (0.3473 s / it)\n",
      "Averaged stats: loss_labels: 0.7422 (0.7647)  loss: 0.7055 (0.7647)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1669.74it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1659.72it/s]\n",
      "val_perf: 0.4598064298637857\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [12]  [  0/554]  eta: 0:03:08  lr: 0.000500  loss_labels: 0.6644 (0.6644)  loss: 0.6644 (0.6644)  time: 0.3399\n",
      "Epoch: [12]  [100/554]  eta: 0:02:38  lr: 0.000500  loss_labels: 0.6571 (0.6605)  loss: 0.6641 (0.6605)  time: 0.3494\n",
      "Epoch: [12]  [200/554]  eta: 0:02:03  lr: 0.000500  loss_labels: 0.6653 (0.6710)  loss: 0.6949 (0.6710)  time: 0.3515\n",
      "Epoch: [12]  [300/554]  eta: 0:01:28  lr: 0.000500  loss_labels: 0.6775 (0.6743)  loss: 0.6778 (0.6743)  time: 0.3503\n",
      "Epoch: [12]  [400/554]  eta: 0:00:53  lr: 0.000500  loss_labels: 0.6773 (0.6772)  loss: 0.6842 (0.6772)  time: 0.3502\n",
      "Epoch: [12]  [500/554]  eta: 0:00:18  lr: 0.000500  loss_labels: 0.6695 (0.6773)  loss: 0.6641 (0.6773)  time: 0.3516\n",
      "Epoch: [12]  [553/554]  eta: 0:00:00  lr: 0.000500  loss_labels: 0.6696 (0.6774)  loss: 0.6576 (0.6774)  time: 0.3426\n",
      "Epoch: [12] Total time: 0:03:14 (0.3504 s / it)\n",
      "Averaged stats: lr: 0.000500  loss_labels: 0.6696 (0.6774)  loss: 0.6576 (0.6774)\n",
      "Test:  [ 0/62]  eta: 0:00:21  loss_labels: 1.0420 (1.0420)  loss: 1.0420 (1.0420)  time: 0.3547\n",
      "Test:  [61/62]  eta: 0:00:00  loss_labels: 0.7530 (0.7676)  loss: 0.7072 (0.7676)  time: 0.3462\n",
      "Test: Total time: 0:00:21 (0.3490 s / it)\n",
      "Averaged stats: loss_labels: 0.7530 (0.7676)  loss: 0.7072 (0.7676)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1685.64it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1687.75it/s]\n",
      "val_perf: 0.4562094186953921\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [13]  [  0/554]  eta: 0:03:18  lr: 0.000500  loss_labels: 0.7306 (0.7306)  loss: 0.7306 (0.7306)  time: 0.3575\n",
      "Epoch: [13]  [100/554]  eta: 0:02:40  lr: 0.000500  loss_labels: 0.6554 (0.6606)  loss: 0.6677 (0.6606)  time: 0.3540\n",
      "Epoch: [13]  [200/554]  eta: 0:02:06  lr: 0.000500  loss_labels: 0.6495 (0.6595)  loss: 0.6495 (0.6595)  time: 0.3638\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/main.py\", line 474, in <module>\n",
      "    main(0, 1, args)\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/main.py\", line 373, in main\n",
      "    train_stats = train_one_epoch(\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/engine.py\", line 35, in train_one_epoch\n",
      "    outputs = model(imgs)\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/models/brain_encoder.py\", line 59, in forward\n",
      "    features, pos = self.backbone_model(samples)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/models/backbone.py\", line 27, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/models/dino.py\", line 51, in forward\n",
      "    xs = self.backbone.get_intermediate_layers(xs)[0]\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\", line 275, in get_intermediate_layers\n",
      "    outputs = self._get_intermediate_layers_not_chunked(x, n)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py\", line 244, in _get_intermediate_layers_not_chunked\n",
      "    x = blk(x)\n",
      "        ^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py\", line 247, in forward\n",
      "    return super().forward(x_or_x_list)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py\", line 106, in forward\n",
      "    x = x + ffn_residual_func(x)\n",
      "            ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py\", line 87, in ffn_residual_func\n",
      "    return self.ls2(self.mlp(self.norm2(x)))\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/mlp.py\", line 38, in forward\n",
      "    x = self.drop(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/dropout.py\", line 59, in forward\n",
      "    return F.dropout(input, self.p, self.training, self.inplace)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/nn/functional.py\", line 1252, in dropout\n",
      "    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n",
      "                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --readout_res 'streams_inc' --save_model 0 --enc_output_layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Training stimulus images: 8857\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 220\n",
      "Using cache found in /home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "xFormers not available\n",
      "xFormers not available\n",
      "brain_encoder(\n",
      "  (transformer): Transformer(\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=768, bias=True)\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (memory_proj): Conv2d(768, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (query_embed): Embedding(16, 768)\n",
      "  (backbone_model): Joiner(\n",
      "    (0): dino_model_with_hooks(\n",
      "      (backbone): DinoVisionTransformer(\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (blocks): ModuleList(\n",
      "          (0-11): 12 x NestedTensorBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): MemEffAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): LayerScale()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): LayerScale()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (head): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      "  (lh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=19004, bias=True)\n",
      "  )\n",
      "  (rh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=20544, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['transformer.decoder.layers.0.self_attn.in_proj_weight', 'transformer.decoder.layers.0.self_attn.in_proj_bias', 'transformer.decoder.layers.0.self_attn.out_proj.weight', 'transformer.decoder.layers.0.self_attn.out_proj.bias', 'transformer.decoder.layers.0.multihead_attn.in_proj_weight', 'transformer.decoder.layers.0.multihead_attn.in_proj_bias', 'transformer.decoder.layers.0.multihead_attn.out_proj.weight', 'transformer.decoder.layers.0.multihead_attn.out_proj.bias', 'transformer.decoder.layers.0.linear1.weight', 'transformer.decoder.layers.0.linear1.bias', 'transformer.decoder.layers.0.linear2.weight', 'transformer.decoder.layers.0.linear2.bias', 'transformer.decoder.layers.0.norm1.weight', 'transformer.decoder.layers.0.norm1.bias', 'transformer.decoder.layers.0.norm2.weight', 'transformer.decoder.layers.0.norm2.bias', 'transformer.decoder.layers.0.norm3.weight', 'transformer.decoder.layers.0.norm3.bias', 'transformer.decoder.norm.weight', 'transformer.decoder.norm.bias', 'transformer.memory_proj.weight', 'transformer.memory_proj.bias', 'query_embed.weight', 'lh_embed.0.weight', 'lh_embed.0.bias', 'rh_embed.0.weight', 'rh_embed.0.bias']\n",
      "Start training\n",
      "Epoch: [0]  [  0/277]  eta: 0:05:03  lr: 0.000100  loss_labels: 0.0928 (0.0928)  loss: 0.0928 (0.0928)  time: 1.0958\n",
      "Epoch: [0]  [100/277]  eta: 0:02:10  lr: 0.000100  loss_labels: 0.0545 (0.0563)  loss: 0.0478 (0.0563)  time: 0.7328\n",
      "Epoch: [0]  [200/277]  eta: 0:00:58  lr: 0.000100  loss_labels: 0.0469 (0.0515)  loss: 0.0452 (0.0515)  time: 0.7811\n",
      "Epoch: [0]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0446 (0.0495)  loss: 0.0444 (0.0495)  time: 0.7532\n",
      "Epoch: [0] Total time: 0:03:29 (0.7566 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0446 (0.0495)  loss: 0.0444 (0.0495)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0509 (0.0509)  loss: 0.0509 (0.0509)  time: 0.7239\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0426 (0.0429)  loss: 0.0426 (0.0429)  time: 0.7243\n",
      "Test: Total time: 0:00:22 (0.7259 s / it)\n",
      "Averaged stats: loss_labels: 0.0426 (0.0429)  loss: 0.0426 (0.0429)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1708.82it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1699.74it/s]\n",
      "val_perf: 0.026510914900150034\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [1]  [  0/277]  eta: 0:03:20  lr: 0.000100  loss_labels: 0.0398 (0.0398)  loss: 0.0398 (0.0398)  time: 0.7248\n",
      "Epoch: [1]  [100/277]  eta: 0:02:10  lr: 0.000100  loss_labels: 0.0433 (0.0433)  loss: 0.0439 (0.0433)  time: 0.7287\n",
      "Epoch: [1]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0422 (0.0429)  loss: 0.0414 (0.0429)  time: 0.7600\n",
      "Epoch: [1]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0414 (0.0426)  loss: 0.0417 (0.0426)  time: 0.7651\n",
      "Epoch: [1] Total time: 0:03:25 (0.7429 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0414 (0.0426)  loss: 0.0417 (0.0426)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0494 (0.0494)  loss: 0.0494 (0.0494)  time: 0.7202\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0415 (0.0415)  loss: 0.0415 (0.0415)  time: 0.7184\n",
      "Test: Total time: 0:00:22 (0.7189 s / it)\n",
      "Averaged stats: loss_labels: 0.0415 (0.0415)  loss: 0.0415 (0.0415)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1700.64it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1703.31it/s]\n",
      "val_perf: 0.027940151321163592\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [2]  [  0/277]  eta: 0:03:24  lr: 0.000100  loss_labels: 0.0428 (0.0428)  loss: 0.0428 (0.0428)  time: 0.7366\n",
      "Epoch: [2]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0408 (0.0411)  loss: 0.0399 (0.0411)  time: 0.7268\n",
      "Epoch: [2]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0409 (0.0410)  loss: 0.0418 (0.0410)  time: 0.7326\n",
      "Epoch: [2]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0416 (0.0411)  loss: 0.0419 (0.0411)  time: 0.7219\n",
      "Epoch: [2] Total time: 0:03:21 (0.7289 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0416 (0.0411)  loss: 0.0419 (0.0411)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0487 (0.0487)  loss: 0.0487 (0.0487)  time: 0.7253\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0405 (0.0407)  loss: 0.0405 (0.0407)  time: 0.7213\n",
      "Test: Total time: 0:00:22 (0.7190 s / it)\n",
      "Averaged stats: loss_labels: 0.0405 (0.0407)  loss: 0.0405 (0.0407)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1706.20it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1705.49it/s]\n",
      "val_perf: 0.028295141922515758\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [3]  [  0/277]  eta: 0:03:23  lr: 0.000100  loss_labels: 0.0411 (0.0411)  loss: 0.0411 (0.0411)  time: 0.7359\n",
      "Epoch: [3]  [100/277]  eta: 0:02:09  lr: 0.000100  loss_labels: 0.0397 (0.0400)  loss: 0.0390 (0.0400)  time: 0.7316\n",
      "Epoch: [3]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0400 (0.0402)  loss: 0.0386 (0.0402)  time: 0.7323\n",
      "Epoch: [3]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0398 (0.0402)  loss: 0.0409 (0.0402)  time: 0.7214\n",
      "Epoch: [3] Total time: 0:03:21 (0.7291 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0398 (0.0402)  loss: 0.0409 (0.0402)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0477 (0.0477)  loss: 0.0477 (0.0477)  time: 0.7188\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0401 (0.0400)  loss: 0.0401 (0.0400)  time: 0.7194\n",
      "Test: Total time: 0:00:22 (0.7195 s / it)\n",
      "Averaged stats: loss_labels: 0.0401 (0.0400)  loss: 0.0401 (0.0400)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1697.53it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1678.53it/s]\n",
      "val_perf: 0.028733481985735207\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.45s/it]\n",
      "Epoch: [4]  [  0/277]  eta: 0:03:23  lr: 0.000100  loss_labels: 0.0403 (0.0403)  loss: 0.0403 (0.0403)  time: 0.7356\n",
      "Epoch: [4]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0383 (0.0389)  loss: 0.0377 (0.0389)  time: 0.7292\n",
      "Epoch: [4]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0402 (0.0394)  loss: 0.0387 (0.0394)  time: 0.7329\n",
      "Epoch: [4]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0394 (0.0395)  loss: 0.0393 (0.0395)  time: 0.7261\n",
      "Epoch: [4] Total time: 0:03:22 (0.7301 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0394 (0.0395)  loss: 0.0393 (0.0395)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0469 (0.0469)  loss: 0.0469 (0.0469)  time: 0.7224\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0391 (0.0400)  loss: 0.0391 (0.0400)  time: 0.7212\n",
      "Test: Total time: 0:00:22 (0.7214 s / it)\n",
      "Averaged stats: loss_labels: 0.0391 (0.0400)  loss: 0.0391 (0.0400)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1693.22it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1667.95it/s]\n",
      "val_perf: 0.0287298471962317\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [5]  [  0/277]  eta: 0:03:27  lr: 0.000100  loss_labels: 0.0380 (0.0380)  loss: 0.0380 (0.0380)  time: 0.7482\n",
      "Epoch: [5]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0384 (0.0386)  loss: 0.0384 (0.0386)  time: 0.7264\n",
      "Epoch: [5]  [200/277]  eta: 0:00:55  lr: 0.000100  loss_labels: 0.0391 (0.0389)  loss: 0.0377 (0.0389)  time: 0.7300\n",
      "Epoch: [5]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0383 (0.0388)  loss: 0.0388 (0.0388)  time: 0.7216\n",
      "Epoch: [5] Total time: 0:03:21 (0.7263 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0383 (0.0388)  loss: 0.0388 (0.0388)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0484 (0.0484)  loss: 0.0484 (0.0484)  time: 0.7292\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0388 (0.0400)  loss: 0.0388 (0.0400)  time: 0.7210\n",
      "Test: Total time: 0:00:22 (0.7223 s / it)\n",
      "Averaged stats: loss_labels: 0.0388 (0.0400)  loss: 0.0388 (0.0400)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1660.93it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1636.83it/s]\n",
      "val_perf: 0.028914884954650347\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [6]  [  0/277]  eta: 0:03:23  lr: 0.000100  loss_labels: 0.0430 (0.0430)  loss: 0.0430 (0.0430)  time: 0.7356\n",
      "Epoch: [6]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0385 (0.0384)  loss: 0.0387 (0.0384)  time: 0.7294\n",
      "Epoch: [6]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0382 (0.0385)  loss: 0.0383 (0.0385)  time: 0.7574\n",
      "Epoch: [6]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0376 (0.0383)  loss: 0.0368 (0.0383)  time: 0.7231\n",
      "Epoch: [6] Total time: 0:03:22 (0.7297 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0376 (0.0383)  loss: 0.0368 (0.0383)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0486 (0.0486)  loss: 0.0486 (0.0486)  time: 0.7201\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0401 (0.0402)  loss: 0.0401 (0.0402)  time: 0.7155\n",
      "Test: Total time: 0:00:22 (0.7153 s / it)\n",
      "Averaged stats: loss_labels: 0.0401 (0.0402)  loss: 0.0401 (0.0402)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1691.45it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1696.56it/s]\n",
      "val_perf: 0.028816883728600057\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [7]  [  0/277]  eta: 0:03:24  lr: 0.000100  loss_labels: 0.0407 (0.0407)  loss: 0.0407 (0.0407)  time: 0.7388\n",
      "Epoch: [7]  [100/277]  eta: 0:02:08  lr: 0.000100  loss_labels: 0.0375 (0.0379)  loss: 0.0375 (0.0379)  time: 0.7286\n",
      "Epoch: [7]  [200/277]  eta: 0:00:55  lr: 0.000100  loss_labels: 0.0374 (0.0378)  loss: 0.0364 (0.0378)  time: 0.7292\n",
      "Epoch: [7]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0373 (0.0378)  loss: 0.0371 (0.0378)  time: 0.7200\n",
      "Epoch: [7] Total time: 0:03:21 (0.7270 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0373 (0.0378)  loss: 0.0371 (0.0378)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0487 (0.0487)  loss: 0.0487 (0.0487)  time: 0.7259\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0402 (0.0405)  loss: 0.0402 (0.0405)  time: 0.7174\n",
      "Test: Total time: 0:00:22 (0.7167 s / it)\n",
      "Averaged stats: loss_labels: 0.0402 (0.0405)  loss: 0.0402 (0.0405)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1678.18it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1681.64it/s]\n",
      "val_perf: 0.028947353625941807\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:17<00:00,  2.44s/it]\n",
      "Epoch: [8]  [  0/277]  eta: 0:03:19  lr: 0.000100  loss_labels: 0.0375 (0.0375)  loss: 0.0375 (0.0375)  time: 0.7220\n",
      "Epoch: [8]  [100/277]  eta: 0:02:09  lr: 0.000100  loss_labels: 0.0368 (0.0372)  loss: 0.0362 (0.0372)  time: 0.7327\n",
      "Epoch: [8]  [200/277]  eta: 0:00:56  lr: 0.000100  loss_labels: 0.0369 (0.0372)  loss: 0.0365 (0.0372)  time: 0.7341\n",
      "Epoch: [8]  [276/277]  eta: 0:00:00  lr: 0.000100  loss_labels: 0.0372 (0.0373)  loss: 0.0372 (0.0373)  time: 0.7262\n",
      "Epoch: [8] Total time: 0:03:22 (0.7327 s / it)\n",
      "Averaged stats: lr: 0.000100  loss_labels: 0.0372 (0.0373)  loss: 0.0372 (0.0373)\n",
      "Test:  [ 0/31]  eta: 0:00:22  loss_labels: 0.0473 (0.0473)  loss: 0.0473 (0.0473)  time: 0.7194\n",
      "Test:  [30/31]  eta: 0:00:00  loss_labels: 0.0397 (0.0400)  loss: 0.0397 (0.0400)  time: 0.7182\n",
      "Test: Total time: 0:00:22 (0.7200 s / it)\n",
      "Averaged stats: loss_labels: 0.0397 (0.0400)  loss: 0.0397 (0.0400)\n",
      "100%|███████████████████████████████████| 19004/19004 [00:11<00:00, 1682.49it/s]\n",
      "100%|███████████████████████████████████| 20544/20544 [00:12<00:00, 1683.11it/s]\n",
      "val_perf: 0.028896020936347716\n",
      "shape of rh_fmri_val_pred (984, 20544)\n",
      "Epoch: [9]  [  0/277]  eta: 0:03:27  lr: 0.000100  loss_labels: 0.0397 (0.0397)  loss: 0.0397 (0.0397)  time: 0.7493\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/main.py\", line 473, in <module>\n",
      "    main(0, 1, args)\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/main.py\", line 373, in main\n",
      "    train_stats = train_one_epoch(\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/engine.py\", line 24, in train_one_epoch\n",
      "    for imgs, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/utils/utils.py\", line 240, in log_every\n",
      "    for obj in iterable:\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/datasets/nsd.py\", line 83, in __getitem__\n",
      "    img = self.transform(img)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n",
      "    img = t(img)\n",
      "          ^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 137, in __call__\n",
      "    return F.to_tensor(pic)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/torchvision/transforms/functional.py\", line 166, in to_tensor\n",
      "    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/PIL/Image.py\", line 701, in __array_interface__\n",
      "    new[\"data\"] = self.tobytes()\n",
      "                  ^^^^^^^^^^^^^^\n",
      "  File \"/home/ha2366/.conda/envs/pytorch/lib/python3.11/site-packages/PIL/Image.py\", line 771, in tobytes\n",
      "    l, s, d = e.encode(bufsize)\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python main.py --readout_res 'faces' --save_model 1 --enc_output_layer 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
