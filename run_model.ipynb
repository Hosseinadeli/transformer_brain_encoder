{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/transformer_brain_encoder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#switch to the directory where the code is\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "roi_name_maps: [{0: 'Unknown', 1: 'V1v', 2: 'V1d', 3: 'V2v', 4: 'V2d', 5: 'V3v', 6: 'V3d', 7: 'hV4'}, {0: 'Unknown', 1: 'EBA', 2: 'FBA-1', 3: 'FBA-2', 4: 'mTL-bodies'}, {0: 'Unknown', 1: 'OFA', 2: 'FFA-1', 3: 'FFA-2', 4: 'mTL-faces', 5: 'aTL-faces'}, {0: 'Unknown', 1: 'OPA', 2: 'PPA', 3: 'RSC'}, {0: 'Unknown', 1: 'OWFA', 2: 'VWFA-1', 3: 'VWFA-2', 4: 'mfs-words', 5: 'mTL-words'}, {0: 'Unknown', 1: 'early', 2: 'midventral', 3: 'midlateral', 4: 'midparietal', 5: 'ventral', 6: 'lateral', 7: 'parietal'}]\n",
      "lh_challenge_rois: 6\n",
      "lh_challenge_rois_s: torch.Size([25, 19004])\n",
      "Training stimulus images: 8857\n",
      "Validation stimulus images: 984\n",
      "\n",
      "Test stimulus images: 159\n",
      "Using cache found in /home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "Number of model parameters: 68418040\n",
      "brain_encoder(\n",
      "  (backbone_model): Joiner(\n",
      "    (0): dino_model_with_hooks(\n",
      "      (backbone): DinoVisionTransformer(\n",
      "        (patch_embed): PatchEmbed(\n",
      "          (proj): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
      "          (norm): Identity()\n",
      "        )\n",
      "        (blocks): ModuleList(\n",
      "          (0-11): 12 x NestedTensorBlock(\n",
      "            (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (attn): MemEffAttention(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls1): LayerScale()\n",
      "            (drop_path1): Identity()\n",
      "            (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "            (mlp): Mlp(\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (act): GELU(approximate='none')\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (drop): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (ls2): LayerScale()\n",
      "            (drop_path2): Identity()\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (head): Identity()\n",
      "      )\n",
      "    )\n",
      "    (1): PositionEmbeddingSine()\n",
      "  )\n",
      "  (spatial_embed): Embedding(39548, 961)\n",
      "  (lh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=19004, bias=True)\n",
      "  )\n",
      "  (rh_embed): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=20544, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "train_params ['spatial_embed.weight', 'lh_embed.0.weight', 'lh_embed.0.bias', 'rh_embed.0.weight', 'rh_embed.0.bias']\n",
      "Start training\n",
      "Traceback (most recent call last):\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/main.py\", line 491, in <module>\n",
      "    main(0, 1, args)\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/main.py\", line 377, in main\n",
      "    train_stats = train_one_epoch(\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/engine.py\", line 35, in train_one_epoch\n",
      "    outputs = model(imgs)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/models/brain_encoder.py\", line 230, in forward\n",
      "    lh_f_pred = self.lh_embed(output_tokens[:,:self.lh_vs,:])\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/container.py\", line 217, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ha2366/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 43.05 GiB (GPU 0; 44.31 GiB total capacity; 6.61 GiB already allocated; 36.87 GiB free; 6.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "!python main.py --subj 1 --run 30 --dataset \"nsd_algo\"  --backbone_arch \"dinov2_q\" --epochs 40 --encoder_arch 'spatial_feature' --readout_res 'voxels' --save_model 0 --enc_output_layer 1 --batch_size 32 --lr 0.0005 --lr_drop 4 #--pre_norm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SBATCH --time=1-12:00:00\n",
    "--image_size 224\n",
    "--image_size 330   --image_size 960 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 4760193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "script = f'''#!/bin/sh\n",
    "#\n",
    "#\n",
    "#SBATCH --account=nklab\n",
    "#SBATCH --job-name=clip  # The job name.\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --nodelist=ax17\n",
    "#SBATCH --cpus-per-task=12\n",
    "#SBATCH --mem-per-cpu=4G\n",
    "\n",
    "\n",
    "ml load anaconda3-2023.07\n",
    "\n",
    "cd /engram/nklab/hossein/recurrent_models/transformer_brain_encoder/\n",
    "\n",
    "conda activate py39\n",
    "\n",
    "python main.py --subj 1 --run 7 --image_size 224 --dataset \"nsd_algo\" --backbone_arch \"clip\" --epochs 20 --encoder_arch 'spatial_feature' --readout_res 'voxels' --save_model 0 --enc_output_layer 1 --batch_size 4 --lr 0.0005 --lr_drop 5\n",
    "\n",
    "'''\n",
    "\n",
    "bash_script_path = \"/engram/nklab/hossein/batch_scripts/imagenet_nb.sh\"\n",
    "os.chdir('/engram/nklab/hossein/batch_scripts/')\n",
    "\n",
    "with open(bash_script_path, \"w+\") as bash_script_file:\n",
    "    bash_script_file.write(script)\n",
    "\n",
    "subprocess.run(['sbatch', '/engram/nklab/hossein/batch_scripts/imagenet_nb.sh'\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env (py39)",
   "language": "python",
   "name": "py39"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
