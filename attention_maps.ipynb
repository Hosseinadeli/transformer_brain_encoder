{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0f03b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/engram/nklab/hossein/recurrent_models/transformer_brain_encoder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function datasets.nsd_utils.roi_maps(data_dir)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/')\n",
    "!pwd\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from models.activations import get_transformer_activations\n",
    "from collections import OrderedDict\n",
    "\n",
    "from datasets.nsd_utils import roi_maps\n",
    "from datasets.nsd import fetch_dataloaders\n",
    "\n",
    "roi_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e201638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.brain_encoder import brain_encoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model_path(model_path):\n",
    "\n",
    "    checkpoint = torch.load(model_path + 'checkpoint.pth', map_location='cpu')\n",
    "\n",
    "    pretrained_dict = checkpoint['model']\n",
    "    args = checkpoint['args']\n",
    "    model = brain_encoder(args)\n",
    "    model.load_state_dict(pretrained_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    try:\n",
    "        model = model.module\n",
    "    except:\n",
    "        model = model\n",
    "        \n",
    "    return model, args \n",
    "\n",
    "def extract_transformer_features(model, imgs, enc_layers=0, dec_layers=1):\n",
    "\n",
    "    try:\n",
    "        model = model.module\n",
    "    except:\n",
    "        model = model\n",
    "\n",
    "    outputs, enc_output, enc_attn_weights, dec_output, dec_attn_weights  = get_transformer_activations(model, imgs, enc_layers, dec_layers)\n",
    "\n",
    "    return outputs, enc_output, enc_attn_weights, dec_output, dec_attn_weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53183d23",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      2\u001b[0m preprocess \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      3\u001b[0m              T\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      4\u001b[0m              T\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m             )\n\u001b[1;32m      8\u001b[0m         ])\n\u001b[0;32m---> 11\u001b[0m train_img_dir  \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_split\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining_images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m image_path \u001b[38;5;241m=\u001b[39m train_img_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/train-9039_nsd-66847.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "preprocess = T.Compose([\n",
    "             T.ToTensor(),\n",
    "             T.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "image_path = train_img_dir + '/train-9039_nsd-66847.png'\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)\n",
    "img = preprocess(image)\n",
    "print(img.shape)\n",
    "patch_size = 14\n",
    "\n",
    "size_im = (\n",
    "    img.shape[0],\n",
    "    int(np.ceil(img.shape[1] / patch_size) * patch_size),\n",
    "    int(np.ceil(img.shape[2] / patch_size) * patch_size),\n",
    ")\n",
    "paded = torch.zeros(size_im)\n",
    "paded[:, : img.shape[1], : img.shape[2]] = img\n",
    "img = paded\n",
    "\n",
    "imgs = img[None, :,:,:]\n",
    "\n",
    "h = img.shape[-2] // patch_size\n",
    "w = img.shape[-1] // patch_size\n",
    "\n",
    "# train_loader, val_loader = fetch_nsd_dataloader(args, args.batch_size, train='train')\n",
    "# test_loader = fetch_nsd_dataloader(args, args.batch_size, train='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf1e219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test stimulus images: 159\n"
     ]
    }
   ],
   "source": [
    "test_loader = fetch_dataloaders(args, train='test')\n",
    "images = next(iter(test_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575594fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ha2366/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 27\u001b[0m\n\u001b[1;32m     19\u001b[0m     model, args \u001b[38;5;241m=\u001b[39m load_model_path(model_path)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m#     images\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# except:\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#     \u001b[39;00m\n\u001b[1;32m     26\u001b[0m     enc_output, enc_attn_weights, dec_output, dec_attn_weights \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 27\u001b[0m       \u001b[43mextract_transformer_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     dec_attn_weights_all\u001b[38;5;241m.\u001b[39mappend(dec_attn_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m50\u001b[39m,h, w)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     32\u001b[0m dec_attn_weights_all \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(dec_attn_weights_all)\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mextract_transformer_features\u001b[0;34m(model, imgs, enc_layers, dec_layers)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m---> 31\u001b[0m enc_output, enc_attn_weights, dec_output, dec_attn_weights  \u001b[38;5;241m=\u001b[39m \u001b[43mget_transformer_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m enc_output, enc_attn_weights, dec_output, dec_attn_weights\n",
      "File \u001b[0;32m/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/models/activations.py:38\u001b[0m, in \u001b[0;36mget_transformer_activations\u001b[0;34m(model, ims, enc_layers, dec_layers)\u001b[0m\n\u001b[1;32m     34\u001b[0m     hooks\u001b[38;5;241m.\u001b[39mappend(model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39mi]\u001b[38;5;241m.\u001b[39mmultihead_attn\u001b[38;5;241m.\u001b[39mregister_forward_hook(\n\u001b[1;32m     35\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, output: dec_attn_weights\u001b[38;5;241m.\u001b[39mappend(output[\u001b[38;5;241m1\u001b[39m])))\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# propagate through the model\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n\u001b[1;32m     41\u001b[0m     hook\u001b[38;5;241m.\u001b[39mremove()\n",
      "File \u001b[0;32m~/.conda/envs/py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/models/brain_encoder.py:60\u001b[0m, in \u001b[0;36mbrain_encoder.forward\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples: NestedTensor):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(samples, (\u001b[38;5;28mlist\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor)):\n\u001b[0;32m---> 60\u001b[0m         samples \u001b[38;5;241m=\u001b[39m \u001b[43mnested_tensor_from_tensor_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# if self.backbone_arch:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_backbone \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/engram/nklab/hossein/recurrent_models/transformer_brain_encoder/utils/utils.py:345\u001b[0m, in \u001b[0;36mnested_tensor_from_tensor_list\u001b[0;34m(tensor_list)\u001b[0m\n\u001b[1;32m    343\u001b[0m         m[: img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], :img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m NestedTensor(tensor, mask)\n",
      "\u001b[0;31mValueError\u001b[0m: not supported"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This model was trained using the following:\n",
    "\n",
    "python main.py --readout_res 'faces' --save_model 1 --enc_output_layer 2\n",
    "'''\n",
    "\n",
    "readout_res = 'rois_all'\n",
    "sub = '02'\n",
    "enc_output_layer = 1\n",
    "\n",
    "arch = 'dinov2_q_transformer'\n",
    "\n",
    "dec_attn_weights_all = []\n",
    "h, w = 31, 31\n",
    "\n",
    "for run in range(1,11):\n",
    "    print(f'Run {run}')\n",
    "    model_path = f'./results/nsd_test/{arch}/subj_{sub}/{readout_res}/enc_{enc_output_layer}/run_{run}/'\n",
    "    model, args = load_model_path(model_path)\n",
    "\n",
    "    # try:\n",
    "    #     images\n",
    "    # except:\n",
    "    #     \n",
    "\n",
    "    enc_output, enc_attn_weights, dec_output, dec_attn_weights = \\\n",
    "      extract_transformer_features(model, images.to(device), enc_layers=0, dec_layers=1)\n",
    "    \n",
    "\n",
    "    dec_attn_weights_all.append(dec_attn_weights[0].reshape(-1,50,h, w).detach().cpu().numpy())\n",
    "\n",
    "dec_attn_weights_all = np.array(dec_attn_weights_all)\n",
    "\n",
    "dec_attn_weights = dec_attn_weights_all.mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64188ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.rois_ind = [0, 1, 2, 3, 4]\n",
    "\n",
    "roi_name_maps, lh_challenge_rois, rh_challenge_rois = roi_maps(args.data_dir)\n",
    "\n",
    "lh_challenge_rois_s = []\n",
    "rh_challenge_rois_s = []\n",
    "\n",
    "lh_roi_names = []\n",
    "rh_roi_names = []\n",
    "\n",
    "for r in args.rois_ind:\n",
    "\n",
    "    #len(roi_name_maps[args.rois_ind])\n",
    "    #args.roi_nums = len(roi_name_maps[args.rois_ind])\n",
    "\n",
    "    lh_rois = torch.tensor(lh_challenge_rois[r]).to(args.device)  # -1\n",
    "    rh_rois = torch.tensor(rh_challenge_rois[r]).to(args.device)  # -1\n",
    "\n",
    "    \n",
    "    for i in range(1, len(roi_name_maps[r])):\n",
    "        lh_challenge_rois_s.append(torch.where(lh_rois == i, 1, 0))\n",
    "        rh_challenge_rois_s.append(torch.where(rh_rois == i, 1, 0))\n",
    "\n",
    "        lh_roi_names.append(roi_name_maps[r][i])\n",
    "        rh_roi_names.append(roi_name_maps[r][i])\n",
    "\n",
    "lh_challenge_rois_s = torch.vstack(lh_challenge_rois_s)\n",
    "rh_challenge_rois_s = torch.vstack(rh_challenge_rois_s)\n",
    "\n",
    "print('lh_challenge_rois_s.shape:', lh_challenge_rois_s.shape)\n",
    "print('rh_challenge_rois_s.shape:', rh_challenge_rois_s.shape)\n",
    "\n",
    "lh_challenge_rois_0 = torch.where(lh_challenge_rois_s.sum(0) == 0, 1, 0)\n",
    "rh_challenge_rois_0 = torch.where(rh_challenge_rois_s.sum(0) == 0, 1, 0)\n",
    "\n",
    "lh_challenge_rois_s = torch.cat((lh_challenge_rois_s, lh_challenge_rois_0[None,:]), dim=0)\n",
    "rh_challenge_rois_s = torch.cat((rh_challenge_rois_s, rh_challenge_rois_0[None,:]), dim=0)\n",
    "\n",
    "print('lh_challenge_rois_s.shape:', lh_challenge_rois_s.shape)\n",
    "print('rh_challenge_rois_s.shape:', rh_challenge_rois_s.shape)\n",
    "\n",
    "args.lh_vs = lh_challenge_rois_s.shape[1]\n",
    "args.rh_vs = rh_challenge_rois_s.shape[1]   \n",
    "\n",
    "if args.readout_res == 'voxels':\n",
    "    args.num_queries = args.lh_vs + args.rh_vs\n",
    "elif args.readout_res == 'rois_all':\n",
    "    args.num_queries = lh_challenge_rois_s.shape[0] + rh_challenge_rois_s.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce320c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(dec_attn_weights[0][1])\n",
    "im_ind = 22\n",
    "\n",
    "fig = plt.figure(figsize= (4.5, (2.29*(lh_challenge_rois_s.shape[0]-1))))\n",
    "# and we add one plot per reference point\n",
    "gs = fig.add_gridspec(lh_challenge_rois_s.shape[0], 2)\n",
    "fig.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(1,len(lh_roi_names)+1):\n",
    "        ax = plt.subplot(gs[j-1, i])\n",
    "        ax.imshow(dec_attn_weights[im_ind][(i*25) + j])\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            ax.set_title(lh_roi_names[j-1])\n",
    "        elif i == 1:\n",
    "            ax.set_title(rh_roi_names[j-1])\n",
    "#fig.savefig(\"../figures/faces.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b628ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]  # Mean used for normalization\n",
    "std = [0.229, 0.224, 0.225]  # Standard deviation used for normalization\n",
    "\n",
    "def denormalize(tensor, mean, std):\n",
    "    \"\"\"Denormalizes a tensor image.\"\"\"\n",
    "    # Create an inverse transform\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-m / s for m, s in zip(mean, std)],\n",
    "        std=[1 / s for s in std]\n",
    "    )\n",
    "\n",
    "    # Apply the inverse transform\n",
    "    return inv_normalize(tensor)\n",
    "\n",
    "\n",
    "# Denormalize the tensor\n",
    "denormalized_tensor = denormalize(images[im_ind], mean, std)\n",
    "# Convert the tensor to a PIL Image\n",
    "to_pil = transforms.ToPILImage()\n",
    "image = to_pil(denormalized_tensor)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e57ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7881c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a grid on the iamge to show different patches\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "train_img_dir  = os.path.join(args.data_dir, 'training_split', 'training_images')\n",
    "image_path = train_img_dir + '/train-9039_nsd-66847.png'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "#img = Image.fromarray(img_with_dots)\n",
    "# new_size = (608, 912)\n",
    "# image = img.resize(new_size)\n",
    "image = np.array(image)\n",
    "\n",
    "print(image.shape)\n",
    "init_image_size = image.shape\n",
    "patch_size = 14\n",
    "\n",
    "\n",
    "# Padding the image with zeros to fit multiple of patch-size\n",
    "size_im = (\n",
    "    int(np.ceil(image.shape[0] / patch_size) * patch_size),\n",
    "    int(np.ceil(image.shape[1] / patch_size) * patch_size),\n",
    "    image.shape[2]\n",
    ")\n",
    "print(size_im)\n",
    "paded = np.zeros(size_im)\n",
    "paded[: image.shape[0], : image.shape[1], :] = image\n",
    "print(np.max(paded))\n",
    "image = Image.fromarray(np.uint8(paded))\n",
    "    \n",
    "\n",
    "# Draw some lines\n",
    "draw = ImageDraw.Draw(image)\n",
    "y_start = 0\n",
    "y_end = image.size[1]\n",
    "\n",
    "#h_featmap,w_featmap  (57, 38)\n",
    "\n",
    "step_count_h, step_count_w = 31, 31\n",
    "\n",
    "step_size_w = int(image.size[0] / step_count_w)\n",
    "step_size_h = int(image.size[1] / step_count_h)\n",
    "\n",
    "for x in range(0, image.size[0], step_size_w):\n",
    "    line = ((x, y_start), (x, y_end))\n",
    "    draw.line(line, fill=(251, 250,245))\n",
    "\n",
    "x_start = 0\n",
    "x_end = image.size[0]\n",
    "\n",
    "for y in range(0, image.size[1], step_size_h):\n",
    "    line = ((x_start, y), (x_end, y))\n",
    "    draw.line(line, fill=(251, 250,245))\n",
    "\n",
    "\n",
    "image.save('./results/image_with_grid_orig.png', dpi=(300, 300))\n",
    "plt.imshow(np.array(image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c53b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "434/14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b24f20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b900ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot data on a flattened brain surface using pycortex.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cortex\n",
    "import cortex.polyutils\n",
    "\n",
    "# =============================================================================\n",
    "# Map the data to fsaverage space\n",
    "# =============================================================================\n",
    "# pycortex requires data in fsaverage space, so here you map the vertices from\n",
    "# Challenge space into fsaverage space. The voxels not used in the Challenge\n",
    "# are given NaN values, so that pycortex ignores them for the plotting.\n",
    "# \"ls_scores\" and \"rh_scores\" are lists with 8 elements, one for each subject.\n",
    "# These elements consist of vectors of length N, where N is the vertex amount\n",
    "# for each subject and hemisphere, and each vector component consists of the\n",
    "# prediction accuracy for that vertex.\n",
    "\n",
    "#challenge_data_dir = '../algonauts_2023_challenge_data'\n",
    "challenge_data_dir = '/engram/nklab/algonauts/algonauts_2023_challenge_data/'\n",
    "lh_fsaverage = []\n",
    "rh_fsaverage = []\n",
    "subjects = [1]\n",
    "for s, sub in enumerate(subjects):\n",
    "    lh_mask_dir = os.path.join(challenge_data_dir, 'subj'+format(sub, '02'),\n",
    "        'roi_masks', 'lh.all-vertices_fsaverage_space.npy')\n",
    "    rh_mask_dir = os.path.join(challenge_data_dir, 'subj'+format(sub, '02'),\n",
    "        'roi_masks', 'rh.all-vertices_fsaverage_space.npy')\n",
    "    lh_fsaverage_all_vertices = np.load(lh_mask_dir)\n",
    "    rh_fsaverage_all_vertices = np.load(rh_mask_dir)\n",
    "    lh_fsavg = np.empty((len(lh_fsaverage_all_vertices)))\n",
    "    lh_fsavg[:] = np.nan\n",
    "    lh_fsavg[np.where(lh_fsaverage_all_vertices)[0]] = lh_challenge_rois_0.cpu() #lh_scores[s]\n",
    "    lh_fsaverage.append(copy(lh_fsavg))\n",
    "    rh_fsavg = np.empty((len(rh_fsaverage_all_vertices)))\n",
    "    rh_fsavg[:] = np.nan\n",
    "    rh_fsavg[np.where(rh_fsaverage_all_vertices)[0]] = rh_challenge_rois_0.cpu() #rh_scores[s]\n",
    "    rh_fsaverage.append(copy(rh_fsavg))\n",
    "    \n",
    "    break\n",
    "\n",
    "# Average the scores across subjects\n",
    "lh_fsaverage = np.nanmean(lh_fsaverage, 0)\n",
    "rh_fsaverage = np.nanmean(rh_fsaverage, 0)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Plot parameters for colorbar\n",
    "# =============================================================================\n",
    "plt.rc('xtick', labelsize=19)\n",
    "plt.rc('ytick', labelsize=19)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Plot the results on brain surfaces\n",
    "# =============================================================================\n",
    "subject = 'fsaverage'\n",
    "data = np.append(lh_fsaverage, rh_fsaverage) * 100\n",
    "vertex_data = cortex.Vertex(data, subject, cmap='RdBu_r', vmin=0, vmax=100)\n",
    "cortex.quickshow(vertex_data, with_curvature=True)\n",
    "plt.savefig('my_plot.png', dpi=300) \n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py39]",
   "language": "python",
   "name": "conda-env-.conda-py39-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
